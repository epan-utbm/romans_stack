Script started on Sat 05 Nov 2016 05:14:34 GMT
]0;kevin@kevin-desktop: ~/catkin_ws/src/romans_stack/model_net/caffe_netkevin@kevin-desktop:~/catkin_ws/src/romans_stack/model_net/caffe_net$ ./solve.py
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Net<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Blob<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Solver<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1105 05:14:42.539572  9049 solver.cpp:48] Initializing solver from parameters: 
train_net: "train.prototxt"
test_net: "test.prototxt"
test_iter: 2356
test_interval: 9999999
base_lr: 0.01
display: 200
max_iter: 450000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 10000
snapshot: 100000
snapshot_prefix: "/home/kevin/snapshot"
solver_mode: GPU
I1105 05:14:42.539762  9049 solver.cpp:81] Creating training net from train_net file: train.prototxt
I1105 05:14:42.540446  9049 net.cpp:49] Initializing net from parameters: 
state {
  phase: TRAIN
}
layer {
  name: "img"
  type: "Python"
  top: "img"
  top: "label"
  python_param {
    module: "data_layers.model_net_layer"
    layer: "ModelNetDataLayer"
    param_str: "{\'img_size\': (250, 250), \'dtype\': \'frame\', \'batch_size\': 128, \'seed\': 1337, \'split\': \'train\', \'dataset_dir\': \'/home/kevin/dataset/processed_data\', \'mean\': 2}"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "img"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 0
    kernel_size: 11
    group: 1
    stride: 4
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    name: "conv3_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv3_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    name: "conv4_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv4_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv4"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 40
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1105 05:14:42.541759  9049 layer_factory.hpp:77] Creating layer img
I1105 05:14:42.547471  9049 net.cpp:91] Creating Layer img
I1105 05:14:42.547498  9049 net.cpp:399] img -> img
I1105 05:14:42.547521  9049 net.cpp:399] img -> label
{'img_size': (250, 250), 'dtype': 'frame', 'batch_size': 128, 'seed': 1337, 'split': 'train', 'dataset_dir': '/home/kevin/dataset/processed_data', 'mean': 2}
I1105 05:14:57.278877  9049 net.cpp:141] Setting up img
I1105 05:14:57.278918  9049 net.cpp:148] Top shape: 128 1 250 250 (8000000)
I1105 05:14:57.278924  9049 net.cpp:148] Top shape: 128 1 (128)
I1105 05:14:57.278928  9049 net.cpp:156] Memory required for data: 32000512
I1105 05:14:57.278947  9049 layer_factory.hpp:77] Creating layer label_img_1_split
I1105 05:14:57.278972  9049 net.cpp:91] Creating Layer label_img_1_split
I1105 05:14:57.278980  9049 net.cpp:425] label_img_1_split <- label
I1105 05:14:57.278997  9049 net.cpp:399] label_img_1_split -> label_img_1_split_0
I1105 05:14:57.279007  9049 net.cpp:399] label_img_1_split -> label_img_1_split_1
I1105 05:14:57.279043  9049 net.cpp:141] Setting up label_img_1_split
I1105 05:14:57.279049  9049 net.cpp:148] Top shape: 128 1 (128)
I1105 05:14:57.279063  9049 net.cpp:148] Top shape: 128 1 (128)
I1105 05:14:57.279068  9049 net.cpp:156] Memory required for data: 32001536
I1105 05:14:57.279072  9049 layer_factory.hpp:77] Creating layer conv1
I1105 05:14:57.279083  9049 net.cpp:91] Creating Layer conv1
I1105 05:14:57.279088  9049 net.cpp:425] conv1 <- img
I1105 05:14:57.279093  9049 net.cpp:399] conv1 -> conv1
I1105 05:14:57.280072  9049 net.cpp:141] Setting up conv1
I1105 05:14:57.280094  9049 net.cpp:148] Top shape: 128 96 60 60 (44236800)
I1105 05:14:57.280099  9049 net.cpp:156] Memory required for data: 208948736
I1105 05:14:57.280108  9049 layer_factory.hpp:77] Creating layer relu1
I1105 05:14:57.280125  9049 net.cpp:91] Creating Layer relu1
I1105 05:14:57.280130  9049 net.cpp:425] relu1 <- conv1
I1105 05:14:57.280135  9049 net.cpp:386] relu1 -> conv1 (in-place)
I1105 05:14:57.280143  9049 net.cpp:141] Setting up relu1
I1105 05:14:57.280148  9049 net.cpp:148] Top shape: 128 96 60 60 (44236800)
I1105 05:14:57.280153  9049 net.cpp:156] Memory required for data: 385895936
I1105 05:14:57.280156  9049 layer_factory.hpp:77] Creating layer pool1
I1105 05:14:57.280164  9049 net.cpp:91] Creating Layer pool1
I1105 05:14:57.280169  9049 net.cpp:425] pool1 <- conv1
I1105 05:14:57.280175  9049 net.cpp:399] pool1 -> pool1
I1105 05:14:57.280205  9049 net.cpp:141] Setting up pool1
I1105 05:14:57.280211  9049 net.cpp:148] Top shape: 128 96 30 30 (11059200)
I1105 05:14:57.280217  9049 net.cpp:156] Memory required for data: 430132736
I1105 05:14:57.280222  9049 layer_factory.hpp:77] Creating layer norm1
I1105 05:14:57.280230  9049 net.cpp:91] Creating Layer norm1
I1105 05:14:57.280235  9049 net.cpp:425] norm1 <- pool1
I1105 05:14:57.280239  9049 net.cpp:399] norm1 -> norm1
I1105 05:14:57.280262  9049 net.cpp:141] Setting up norm1
I1105 05:14:57.280268  9049 net.cpp:148] Top shape: 128 96 30 30 (11059200)
I1105 05:14:57.280273  9049 net.cpp:156] Memory required for data: 474369536
I1105 05:14:57.280277  9049 layer_factory.hpp:77] Creating layer conv2
I1105 05:14:57.280287  9049 net.cpp:91] Creating Layer conv2
I1105 05:14:57.280290  9049 net.cpp:425] conv2 <- norm1
I1105 05:14:57.280297  9049 net.cpp:399] conv2 -> conv2
I1105 05:14:57.282135  9049 net.cpp:141] Setting up conv2
I1105 05:14:57.282150  9049 net.cpp:148] Top shape: 128 256 30 30 (29491200)
I1105 05:14:57.282155  9049 net.cpp:156] Memory required for data: 592334336
I1105 05:14:57.282162  9049 layer_factory.hpp:77] Creating layer relu2
I1105 05:14:57.282171  9049 net.cpp:91] Creating Layer relu2
I1105 05:14:57.282176  9049 net.cpp:425] relu2 <- conv2
I1105 05:14:57.282181  9049 net.cpp:386] relu2 -> conv2 (in-place)
I1105 05:14:57.282186  9049 net.cpp:141] Setting up relu2
I1105 05:14:57.282192  9049 net.cpp:148] Top shape: 128 256 30 30 (29491200)
I1105 05:14:57.282197  9049 net.cpp:156] Memory required for data: 710299136
I1105 05:14:57.282202  9049 layer_factory.hpp:77] Creating layer pool2
I1105 05:14:57.282208  9049 net.cpp:91] Creating Layer pool2
I1105 05:14:57.282213  9049 net.cpp:425] pool2 <- conv2
I1105 05:14:57.282218  9049 net.cpp:399] pool2 -> pool2
I1105 05:14:57.282243  9049 net.cpp:141] Setting up pool2
I1105 05:14:57.282250  9049 net.cpp:148] Top shape: 128 256 15 15 (7372800)
I1105 05:14:57.282254  9049 net.cpp:156] Memory required for data: 739790336
I1105 05:14:57.282259  9049 layer_factory.hpp:77] Creating layer norm2
I1105 05:14:57.282266  9049 net.cpp:91] Creating Layer norm2
I1105 05:14:57.282270  9049 net.cpp:425] norm2 <- pool2
I1105 05:14:57.282276  9049 net.cpp:399] norm2 -> norm2
I1105 05:14:57.282296  9049 net.cpp:141] Setting up norm2
I1105 05:14:57.282302  9049 net.cpp:148] Top shape: 128 256 15 15 (7372800)
I1105 05:14:57.282306  9049 net.cpp:156] Memory required for data: 769281536
I1105 05:14:57.282311  9049 layer_factory.hpp:77] Creating layer conv3
I1105 05:14:57.282320  9049 net.cpp:91] Creating Layer conv3
I1105 05:14:57.282325  9049 net.cpp:425] conv3 <- norm2
I1105 05:14:57.282330  9049 net.cpp:399] conv3 -> conv3
I1105 05:14:57.284555  9049 net.cpp:141] Setting up conv3
I1105 05:14:57.284569  9049 net.cpp:148] Top shape: 128 384 15 15 (11059200)
I1105 05:14:57.284574  9049 net.cpp:156] Memory required for data: 813518336
I1105 05:14:57.284581  9049 layer_factory.hpp:77] Creating layer relu3
I1105 05:14:57.284587  9049 net.cpp:91] Creating Layer relu3
I1105 05:14:57.284592  9049 net.cpp:425] relu3 <- conv3
I1105 05:14:57.284598  9049 net.cpp:386] relu3 -> conv3 (in-place)
I1105 05:14:57.284605  9049 net.cpp:141] Setting up relu3
I1105 05:14:57.284610  9049 net.cpp:148] Top shape: 128 384 15 15 (11059200)
I1105 05:14:57.284615  9049 net.cpp:156] Memory required for data: 857755136
I1105 05:14:57.284620  9049 layer_factory.hpp:77] Creating layer conv4
I1105 05:14:57.284627  9049 net.cpp:91] Creating Layer conv4
I1105 05:14:57.284632  9049 net.cpp:425] conv4 <- conv3
I1105 05:14:57.284637  9049 net.cpp:399] conv4 -> conv4
I1105 05:14:57.287078  9049 net.cpp:141] Setting up conv4
I1105 05:14:57.287101  9049 net.cpp:148] Top shape: 128 256 15 15 (7372800)
I1105 05:14:57.287108  9049 net.cpp:156] Memory required for data: 887246336
I1105 05:14:57.287117  9049 layer_factory.hpp:77] Creating layer relu4
I1105 05:14:57.287125  9049 net.cpp:91] Creating Layer relu4
I1105 05:14:57.287132  9049 net.cpp:425] relu4 <- conv4
I1105 05:14:57.287138  9049 net.cpp:386] relu4 -> conv4 (in-place)
I1105 05:14:57.287145  9049 net.cpp:141] Setting up relu4
I1105 05:14:57.287150  9049 net.cpp:148] Top shape: 128 256 15 15 (7372800)
I1105 05:14:57.287155  9049 net.cpp:156] Memory required for data: 916737536
I1105 05:14:57.287160  9049 layer_factory.hpp:77] Creating layer pool5
I1105 05:14:57.287168  9049 net.cpp:91] Creating Layer pool5
I1105 05:14:57.287173  9049 net.cpp:425] pool5 <- conv4
I1105 05:14:57.287178  9049 net.cpp:399] pool5 -> pool5
I1105 05:14:57.287204  9049 net.cpp:141] Setting up pool5
I1105 05:14:57.287209  9049 net.cpp:148] Top shape: 128 256 7 7 (1605632)
I1105 05:14:57.287214  9049 net.cpp:156] Memory required for data: 923160064
I1105 05:14:57.287219  9049 layer_factory.hpp:77] Creating layer fc6
I1105 05:14:57.287226  9049 net.cpp:91] Creating Layer fc6
I1105 05:14:57.287231  9049 net.cpp:425] fc6 <- pool5
I1105 05:14:57.287237  9049 net.cpp:399] fc6 -> fc6
I1105 05:14:57.636991  9049 net.cpp:141] Setting up fc6
I1105 05:14:57.637032  9049 net.cpp:148] Top shape: 128 4096 (524288)
I1105 05:14:57.637043  9049 net.cpp:156] Memory required for data: 925257216
I1105 05:14:57.637064  9049 layer_factory.hpp:77] Creating layer relu6
I1105 05:14:57.637081  9049 net.cpp:91] Creating Layer relu6
I1105 05:14:57.637090  9049 net.cpp:425] relu6 <- fc6
I1105 05:14:57.637101  9049 net.cpp:386] relu6 -> fc6 (in-place)
I1105 05:14:57.637116  9049 net.cpp:141] Setting up relu6
I1105 05:14:57.637126  9049 net.cpp:148] Top shape: 128 4096 (524288)
I1105 05:14:57.637135  9049 net.cpp:156] Memory required for data: 927354368
I1105 05:14:57.637140  9049 layer_factory.hpp:77] Creating layer drop6
I1105 05:14:57.637161  9049 net.cpp:91] Creating Layer drop6
I1105 05:14:57.637168  9049 net.cpp:425] drop6 <- fc6
I1105 05:14:57.637178  9049 net.cpp:386] drop6 -> fc6 (in-place)
I1105 05:14:57.637204  9049 net.cpp:141] Setting up drop6
I1105 05:14:57.637215  9049 net.cpp:148] Top shape: 128 4096 (524288)
I1105 05:14:57.637223  9049 net.cpp:156] Memory required for data: 929451520
I1105 05:14:57.637231  9049 layer_factory.hpp:77] Creating layer fc7
I1105 05:14:57.637243  9049 net.cpp:91] Creating Layer fc7
I1105 05:14:57.637249  9049 net.cpp:425] fc7 <- fc6
I1105 05:14:57.637260  9049 net.cpp:399] fc7 -> fc7
I1105 05:14:57.731420  9049 net.cpp:141] Setting up fc7
I1105 05:14:57.731509  9049 net.cpp:148] Top shape: 128 4096 (524288)
I1105 05:14:57.731528  9049 net.cpp:156] Memory required for data: 931548672
I1105 05:14:57.731550  9049 layer_factory.hpp:77] Creating layer relu7
I1105 05:14:57.731570  9049 net.cpp:91] Creating Layer relu7
I1105 05:14:57.731582  9049 net.cpp:425] relu7 <- fc7
I1105 05:14:57.731595  9049 net.cpp:386] relu7 -> fc7 (in-place)
I1105 05:14:57.731608  9049 net.cpp:141] Setting up relu7
I1105 05:14:57.731617  9049 net.cpp:148] Top shape: 128 4096 (524288)
I1105 05:14:57.731624  9049 net.cpp:156] Memory required for data: 933645824
I1105 05:14:57.731632  9049 layer_factory.hpp:77] Creating layer drop7
I1105 05:14:57.731643  9049 net.cpp:91] Creating Layer drop7
I1105 05:14:57.731649  9049 net.cpp:425] drop7 <- fc7
I1105 05:14:57.731657  9049 net.cpp:386] drop7 -> fc7 (in-place)
I1105 05:14:57.731704  9049 net.cpp:141] Setting up drop7
I1105 05:14:57.731714  9049 net.cpp:148] Top shape: 128 4096 (524288)
I1105 05:14:57.731719  9049 net.cpp:156] Memory required for data: 935742976
I1105 05:14:57.731722  9049 layer_factory.hpp:77] Creating layer fc8
I1105 05:14:57.731734  9049 net.cpp:91] Creating Layer fc8
I1105 05:14:57.731739  9049 net.cpp:425] fc8 <- fc7
I1105 05:14:57.731753  9049 net.cpp:399] fc8 -> fc8
I1105 05:14:57.733044  9049 net.cpp:141] Setting up fc8
I1105 05:14:57.733084  9049 net.cpp:148] Top shape: 128 40 (5120)
I1105 05:14:57.733106  9049 net.cpp:156] Memory required for data: 935763456
I1105 05:14:57.733122  9049 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I1105 05:14:57.733137  9049 net.cpp:91] Creating Layer fc8_fc8_0_split
I1105 05:14:57.733146  9049 net.cpp:425] fc8_fc8_0_split <- fc8
I1105 05:14:57.733157  9049 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1105 05:14:57.733173  9049 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1105 05:14:57.733213  9049 net.cpp:141] Setting up fc8_fc8_0_split
I1105 05:14:57.733224  9049 net.cpp:148] Top shape: 128 40 (5120)
I1105 05:14:57.733230  9049 net.cpp:148] Top shape: 128 40 (5120)
I1105 05:14:57.733235  9049 net.cpp:156] Memory required for data: 935804416
I1105 05:14:57.733242  9049 layer_factory.hpp:77] Creating layer accuracy
I1105 05:14:57.733250  9049 net.cpp:91] Creating Layer accuracy
I1105 05:14:57.733256  9049 net.cpp:425] accuracy <- fc8_fc8_0_split_0
I1105 05:14:57.733263  9049 net.cpp:425] accuracy <- label_img_1_split_0
I1105 05:14:57.733270  9049 net.cpp:399] accuracy -> accuracy
I1105 05:14:57.733289  9049 net.cpp:141] Setting up accuracy
I1105 05:14:57.733295  9049 net.cpp:148] Top shape: (1)
I1105 05:14:57.733300  9049 net.cpp:156] Memory required for data: 935804420
I1105 05:14:57.733305  9049 layer_factory.hpp:77] Creating layer loss
I1105 05:14:57.733312  9049 net.cpp:91] Creating Layer loss
I1105 05:14:57.733319  9049 net.cpp:425] loss <- fc8_fc8_0_split_1
I1105 05:14:57.733325  9049 net.cpp:425] loss <- label_img_1_split_1
I1105 05:14:57.733331  9049 net.cpp:399] loss -> loss
I1105 05:14:57.733340  9049 layer_factory.hpp:77] Creating layer loss
I1105 05:14:57.733444  9049 net.cpp:141] Setting up loss
I1105 05:14:57.733453  9049 net.cpp:148] Top shape: (1)
I1105 05:14:57.733458  9049 net.cpp:151]     with loss weight 1
I1105 05:14:57.733472  9049 net.cpp:156] Memory required for data: 935804424
I1105 05:14:57.733477  9049 net.cpp:217] loss needs backward computation.
I1105 05:14:57.733484  9049 net.cpp:219] accuracy does not need backward computation.
I1105 05:14:57.733489  9049 net.cpp:217] fc8_fc8_0_split needs backward computation.
I1105 05:14:57.733494  9049 net.cpp:217] fc8 needs backward computation.
I1105 05:14:57.733499  9049 net.cpp:217] drop7 needs backward computation.
I1105 05:14:57.733502  9049 net.cpp:217] relu7 needs backward computation.
I1105 05:14:57.733508  9049 net.cpp:217] fc7 needs backward computation.
I1105 05:14:57.733513  9049 net.cpp:217] drop6 needs backward computation.
I1105 05:14:57.733518  9049 net.cpp:217] relu6 needs backward computation.
I1105 05:14:57.733523  9049 net.cpp:217] fc6 needs backward computation.
I1105 05:14:57.733528  9049 net.cpp:217] pool5 needs backward computation.
I1105 05:14:57.733533  9049 net.cpp:217] relu4 needs backward computation.
I1105 05:14:57.733538  9049 net.cpp:217] conv4 needs backward computation.
I1105 05:14:57.733543  9049 net.cpp:217] relu3 needs backward computation.
I1105 05:14:57.733548  9049 net.cpp:217] conv3 needs backward computation.
I1105 05:14:57.733554  9049 net.cpp:217] norm2 needs backward computation.
I1105 05:14:57.733559  9049 net.cpp:217] pool2 needs backward computation.
I1105 05:14:57.733564  9049 net.cpp:217] relu2 needs backward computation.
I1105 05:14:57.733568  9049 net.cpp:217] conv2 needs backward computation.
I1105 05:14:57.733573  9049 net.cpp:217] norm1 needs backward computation.
I1105 05:14:57.733579  9049 net.cpp:217] pool1 needs backward computation.
I1105 05:14:57.733584  9049 net.cpp:217] relu1 needs backward computation.
I1105 05:14:57.733588  9049 net.cpp:217] conv1 needs backward computation.
I1105 05:14:57.733594  9049 net.cpp:219] label_img_1_split does not need backward computation.
I1105 05:14:57.733600  9049 net.cpp:219] img does not need backward computation.
I1105 05:14:57.733605  9049 net.cpp:261] This network produces output accuracy
I1105 05:14:57.733609  9049 net.cpp:261] This network produces output loss
I1105 05:14:57.733624  9049 net.cpp:274] Network initialization done.
I1105 05:14:57.734076  9049 solver.cpp:181] Creating test net (#0) specified by test_net file: test.prototxt
I1105 05:14:57.734246  9049 net.cpp:49] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "img"
  type: "Python"
  top: "img"
  top: "label"
  python_param {
    module: "data_layers.model_net_layer"
    layer: "ModelNetDataLayer"
    param_str: "{\'img_size\': (250, 250), \'dtype\': \'object\', \'batch_size\': 128, \'seed\': 1337, \'split\': \'test\', \'dataset_dir\': \'/home/kevin/dataset/processed_data\', \'mean\': 2}"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "img"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 0
    kernel_size: 11
    group: 1
    stride: 4
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    name: "conv3_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv3_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    name: "conv4_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv4_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv4"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 40
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1105 05:14:57.734988  9049 layer_factory.hpp:77] Creating layer img
I1105 05:14:57.735051  9049 net.cpp:91] Creating Layer img
I1105 05:14:57.735062  9049 net.cpp:399] img -> img
I1105 05:14:57.735074  9049 net.cpp:399] img -> label
{'img_size': (250, 250), 'dtype': 'object', 'batch_size': 128, 'seed': 1337, 'split': 'test', 'dataset_dir': '/home/kevin/dataset/processed_data', 'mean': 2}
I1105 05:14:57.892788  9049 net.cpp:141] Setting up img
I1105 05:14:57.892827  9049 net.cpp:148] Top shape: 24 1 250 250 (1500000)
I1105 05:14:57.892839  9049 net.cpp:148] Top shape: 24 1 (24)
I1105 05:14:57.892846  9049 net.cpp:156] Memory required for data: 6000096
I1105 05:14:57.892858  9049 layer_factory.hpp:77] Creating layer label_img_1_split
I1105 05:14:57.892874  9049 net.cpp:91] Creating Layer label_img_1_split
I1105 05:14:57.892884  9049 net.cpp:425] label_img_1_split <- label
I1105 05:14:57.892894  9049 net.cpp:399] label_img_1_split -> label_img_1_split_0
I1105 05:14:57.892910  9049 net.cpp:399] label_img_1_split -> label_img_1_split_1
I1105 05:14:57.892947  9049 net.cpp:141] Setting up label_img_1_split
I1105 05:14:57.892959  9049 net.cpp:148] Top shape: 24 1 (24)
I1105 05:14:57.892967  9049 net.cpp:148] Top shape: 24 1 (24)
I1105 05:14:57.892974  9049 net.cpp:156] Memory required for data: 6000288
I1105 05:14:57.892982  9049 layer_factory.hpp:77] Creating layer conv1
I1105 05:14:57.892998  9049 net.cpp:91] Creating Layer conv1
I1105 05:14:57.893007  9049 net.cpp:425] conv1 <- img
I1105 05:14:57.893016  9049 net.cpp:399] conv1 -> conv1
I1105 05:14:57.893280  9049 net.cpp:141] Setting up conv1
I1105 05:14:57.893295  9049 net.cpp:148] Top shape: 24 96 60 60 (8294400)
I1105 05:14:57.893301  9049 net.cpp:156] Memory required for data: 39177888
I1105 05:14:57.893316  9049 layer_factory.hpp:77] Creating layer relu1
I1105 05:14:57.893326  9049 net.cpp:91] Creating Layer relu1
I1105 05:14:57.893333  9049 net.cpp:425] relu1 <- conv1
I1105 05:14:57.893342  9049 net.cpp:386] relu1 -> conv1 (in-place)
I1105 05:14:57.893352  9049 net.cpp:141] Setting up relu1
I1105 05:14:57.893360  9049 net.cpp:148] Top shape: 24 96 60 60 (8294400)
I1105 05:14:57.893368  9049 net.cpp:156] Memory required for data: 72355488
I1105 05:14:57.893375  9049 layer_factory.hpp:77] Creating layer pool1
I1105 05:14:57.893386  9049 net.cpp:91] Creating Layer pool1
I1105 05:14:57.893394  9049 net.cpp:425] pool1 <- conv1
I1105 05:14:57.893404  9049 net.cpp:399] pool1 -> pool1
I1105 05:14:57.893438  9049 net.cpp:141] Setting up pool1
I1105 05:14:57.893450  9049 net.cpp:148] Top shape: 24 96 30 30 (2073600)
I1105 05:14:57.893457  9049 net.cpp:156] Memory required for data: 80649888
I1105 05:14:57.893465  9049 layer_factory.hpp:77] Creating layer norm1
I1105 05:14:57.893476  9049 net.cpp:91] Creating Layer norm1
I1105 05:14:57.893482  9049 net.cpp:425] norm1 <- pool1
I1105 05:14:57.893491  9049 net.cpp:399] norm1 -> norm1
I1105 05:14:57.893523  9049 net.cpp:141] Setting up norm1
I1105 05:14:57.893533  9049 net.cpp:148] Top shape: 24 96 30 30 (2073600)
I1105 05:14:57.893540  9049 net.cpp:156] Memory required for data: 88944288
I1105 05:14:57.893548  9049 layer_factory.hpp:77] Creating layer conv2
I1105 05:14:57.893560  9049 net.cpp:91] Creating Layer conv2
I1105 05:14:57.893568  9049 net.cpp:425] conv2 <- norm1
I1105 05:14:57.893579  9049 net.cpp:399] conv2 -> conv2
I1105 05:14:57.895932  9049 net.cpp:141] Setting up conv2
I1105 05:14:57.895951  9049 net.cpp:148] Top shape: 24 256 30 30 (5529600)
I1105 05:14:57.895961  9049 net.cpp:156] Memory required for data: 111062688
I1105 05:14:57.895972  9049 layer_factory.hpp:77] Creating layer relu2
I1105 05:14:57.895982  9049 net.cpp:91] Creating Layer relu2
I1105 05:14:57.895990  9049 net.cpp:425] relu2 <- conv2
I1105 05:14:57.895999  9049 net.cpp:386] relu2 -> conv2 (in-place)
I1105 05:14:57.896008  9049 net.cpp:141] Setting up relu2
I1105 05:14:57.896018  9049 net.cpp:148] Top shape: 24 256 30 30 (5529600)
I1105 05:14:57.896025  9049 net.cpp:156] Memory required for data: 133181088
I1105 05:14:57.896034  9049 layer_factory.hpp:77] Creating layer pool2
I1105 05:14:57.896044  9049 net.cpp:91] Creating Layer pool2
I1105 05:14:57.896051  9049 net.cpp:425] pool2 <- conv2
I1105 05:14:57.896060  9049 net.cpp:399] pool2 -> pool2
I1105 05:14:57.896097  9049 net.cpp:141] Setting up pool2
I1105 05:14:57.896108  9049 net.cpp:148] Top shape: 24 256 15 15 (1382400)
I1105 05:14:57.896116  9049 net.cpp:156] Memory required for data: 138710688
I1105 05:14:57.896123  9049 layer_factory.hpp:77] Creating layer norm2
I1105 05:14:57.896133  9049 net.cpp:91] Creating Layer norm2
I1105 05:14:57.896142  9049 net.cpp:425] norm2 <- pool2
I1105 05:14:57.896150  9049 net.cpp:399] norm2 -> norm2
I1105 05:14:57.896183  9049 net.cpp:141] Setting up norm2
I1105 05:14:57.896193  9049 net.cpp:148] Top shape: 24 256 15 15 (1382400)
I1105 05:14:57.896200  9049 net.cpp:156] Memory required for data: 144240288
I1105 05:14:57.896208  9049 layer_factory.hpp:77] Creating layer conv3
I1105 05:14:57.896219  9049 net.cpp:91] Creating Layer conv3
I1105 05:14:57.896226  9049 net.cpp:425] conv3 <- norm2
I1105 05:14:57.896236  9049 net.cpp:399] conv3 -> conv3
I1105 05:14:57.899391  9049 net.cpp:141] Setting up conv3
I1105 05:14:57.899412  9049 net.cpp:148] Top shape: 24 384 15 15 (2073600)
I1105 05:14:57.899420  9049 net.cpp:156] Memory required for data: 152534688
I1105 05:14:57.899433  9049 layer_factory.hpp:77] Creating layer relu3
I1105 05:14:57.899443  9049 net.cpp:91] Creating Layer relu3
I1105 05:14:57.899451  9049 net.cpp:425] relu3 <- conv3
I1105 05:14:57.899461  9049 net.cpp:386] relu3 -> conv3 (in-place)
I1105 05:14:57.899471  9049 net.cpp:141] Setting up relu3
I1105 05:14:57.899479  9049 net.cpp:148] Top shape: 24 384 15 15 (2073600)
I1105 05:14:57.899492  9049 net.cpp:156] Memory required for data: 160829088
I1105 05:14:57.899499  9049 layer_factory.hpp:77] Creating layer conv4
I1105 05:14:57.899513  9049 net.cpp:91] Creating Layer conv4
I1105 05:14:57.899520  9049 net.cpp:425] conv4 <- conv3
I1105 05:14:57.899530  9049 net.cpp:399] conv4 -> conv4
I1105 05:14:57.902691  9049 net.cpp:141] Setting up conv4
I1105 05:14:57.902710  9049 net.cpp:148] Top shape: 24 256 15 15 (1382400)
I1105 05:14:57.902719  9049 net.cpp:156] Memory required for data: 166358688
I1105 05:14:57.902729  9049 layer_factory.hpp:77] Creating layer relu4
I1105 05:14:57.902740  9049 net.cpp:91] Creating Layer relu4
I1105 05:14:57.902747  9049 net.cpp:425] relu4 <- conv4
I1105 05:14:57.902756  9049 net.cpp:386] relu4 -> conv4 (in-place)
I1105 05:14:57.902766  9049 net.cpp:141] Setting up relu4
I1105 05:14:57.902776  9049 net.cpp:148] Top shape: 24 256 15 15 (1382400)
I1105 05:14:57.902783  9049 net.cpp:156] Memory required for data: 171888288
I1105 05:14:57.902791  9049 layer_factory.hpp:77] Creating layer pool5
I1105 05:14:57.902801  9049 net.cpp:91] Creating Layer pool5
I1105 05:14:57.902808  9049 net.cpp:425] pool5 <- conv4
I1105 05:14:57.902817  9049 net.cpp:399] pool5 -> pool5
I1105 05:14:57.902855  9049 net.cpp:141] Setting up pool5
I1105 05:14:57.902866  9049 net.cpp:148] Top shape: 24 256 7 7 (301056)
I1105 05:14:57.902873  9049 net.cpp:156] Memory required for data: 173092512
I1105 05:14:57.902880  9049 layer_factory.hpp:77] Creating layer fc6
I1105 05:14:57.902892  9049 net.cpp:91] Creating Layer fc6
I1105 05:14:57.902899  9049 net.cpp:425] fc6 <- pool5
I1105 05:14:57.902909  9049 net.cpp:399] fc6 -> fc6
I1105 05:14:58.256240  9049 net.cpp:141] Setting up fc6
I1105 05:14:58.256280  9049 net.cpp:148] Top shape: 24 4096 (98304)
I1105 05:14:58.256301  9049 net.cpp:156] Memory required for data: 173485728
I1105 05:14:58.256321  9049 layer_factory.hpp:77] Creating layer relu6
I1105 05:14:58.256340  9049 net.cpp:91] Creating Layer relu6
I1105 05:14:58.256350  9049 net.cpp:425] relu6 <- fc6
I1105 05:14:58.256361  9049 net.cpp:386] relu6 -> fc6 (in-place)
I1105 05:14:58.256374  9049 net.cpp:141] Setting up relu6
I1105 05:14:58.256383  9049 net.cpp:148] Top shape: 24 4096 (98304)
I1105 05:14:58.256392  9049 net.cpp:156] Memory required for data: 173878944
I1105 05:14:58.256399  9049 layer_factory.hpp:77] Creating layer drop6
I1105 05:14:58.256412  9049 net.cpp:91] Creating Layer drop6
I1105 05:14:58.256418  9049 net.cpp:425] drop6 <- fc6
I1105 05:14:58.256428  9049 net.cpp:386] drop6 -> fc6 (in-place)
I1105 05:14:58.256458  9049 net.cpp:141] Setting up drop6
I1105 05:14:58.256467  9049 net.cpp:148] Top shape: 24 4096 (98304)
I1105 05:14:58.256474  9049 net.cpp:156] Memory required for data: 174272160
I1105 05:14:58.256482  9049 layer_factory.hpp:77] Creating layer fc7
I1105 05:14:58.256494  9049 net.cpp:91] Creating Layer fc7
I1105 05:14:58.256501  9049 net.cpp:425] fc7 <- fc6
I1105 05:14:58.256511  9049 net.cpp:399] fc7 -> fc7
I1105 05:14:58.360131  9049 net.cpp:141] Setting up fc7
I1105 05:14:58.360174  9049 net.cpp:148] Top shape: 24 4096 (98304)
I1105 05:14:58.360182  9049 net.cpp:156] Memory required for data: 174665376
I1105 05:14:58.360204  9049 layer_factory.hpp:77] Creating layer relu7
I1105 05:14:58.360215  9049 net.cpp:91] Creating Layer relu7
I1105 05:14:58.360224  9049 net.cpp:425] relu7 <- fc7
I1105 05:14:58.360230  9049 net.cpp:386] relu7 -> fc7 (in-place)
I1105 05:14:58.360241  9049 net.cpp:141] Setting up relu7
I1105 05:14:58.360247  9049 net.cpp:148] Top shape: 24 4096 (98304)
I1105 05:14:58.360251  9049 net.cpp:156] Memory required for data: 175058592
I1105 05:14:58.360257  9049 layer_factory.hpp:77] Creating layer drop7
I1105 05:14:58.360265  9049 net.cpp:91] Creating Layer drop7
I1105 05:14:58.360271  9049 net.cpp:425] drop7 <- fc7
I1105 05:14:58.360277  9049 net.cpp:386] drop7 -> fc7 (in-place)
I1105 05:14:58.360313  9049 net.cpp:141] Setting up drop7
I1105 05:14:58.360319  9049 net.cpp:148] Top shape: 24 4096 (98304)
I1105 05:14:58.360332  9049 net.cpp:156] Memory required for data: 175451808
I1105 05:14:58.360337  9049 layer_factory.hpp:77] Creating layer fc8
I1105 05:14:58.360353  9049 net.cpp:91] Creating Layer fc8
I1105 05:14:58.360357  9049 net.cpp:425] fc8 <- fc7
I1105 05:14:58.360365  9049 net.cpp:399] fc8 -> fc8
I1105 05:14:58.361484  9049 net.cpp:141] Setting up fc8
I1105 05:14:58.361505  9049 net.cpp:148] Top shape: 24 40 (960)
I1105 05:14:58.361510  9049 net.cpp:156] Memory required for data: 175455648
I1105 05:14:58.361516  9049 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I1105 05:14:58.361536  9049 net.cpp:91] Creating Layer fc8_fc8_0_split
I1105 05:14:58.361541  9049 net.cpp:425] fc8_fc8_0_split <- fc8
I1105 05:14:58.361548  9049 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1105 05:14:58.361557  9049 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1105 05:14:58.361593  9049 net.cpp:141] Setting up fc8_fc8_0_split
I1105 05:14:58.361598  9049 net.cpp:148] Top shape: 24 40 (960)
I1105 05:14:58.361613  9049 net.cpp:148] Top shape: 24 40 (960)
I1105 05:14:58.361616  9049 net.cpp:156] Memory required for data: 175463328
I1105 05:14:58.361620  9049 layer_factory.hpp:77] Creating layer accuracy
I1105 05:14:58.361626  9049 net.cpp:91] Creating Layer accuracy
I1105 05:14:58.361631  9049 net.cpp:425] accuracy <- fc8_fc8_0_split_0
I1105 05:14:58.361637  9049 net.cpp:425] accuracy <- label_img_1_split_0
I1105 05:14:58.361642  9049 net.cpp:399] accuracy -> accuracy
I1105 05:14:58.361649  9049 net.cpp:141] Setting up accuracy
I1105 05:14:58.361654  9049 net.cpp:148] Top shape: (1)
I1105 05:14:58.361660  9049 net.cpp:156] Memory required for data: 175463332
I1105 05:14:58.361663  9049 layer_factory.hpp:77] Creating layer loss
I1105 05:14:58.361670  9049 net.cpp:91] Creating Layer loss
I1105 05:14:58.361675  9049 net.cpp:425] loss <- fc8_fc8_0_split_1
I1105 05:14:58.361680  9049 net.cpp:425] loss <- label_img_1_split_1
I1105 05:14:58.361685  9049 net.cpp:399] loss -> loss
I1105 05:14:58.361693  9049 layer_factory.hpp:77] Creating layer loss
I1105 05:14:58.361752  9049 net.cpp:141] Setting up loss
I1105 05:14:58.361758  9049 net.cpp:148] Top shape: (1)
I1105 05:14:58.361763  9049 net.cpp:151]     with loss weight 1
I1105 05:14:58.361774  9049 net.cpp:156] Memory required for data: 175463336
I1105 05:14:58.361779  9049 net.cpp:217] loss needs backward computation.
I1105 05:14:58.361784  9049 net.cpp:219] accuracy does not need backward computation.
I1105 05:14:58.361789  9049 net.cpp:217] fc8_fc8_0_split needs backward computation.
I1105 05:14:58.361794  9049 net.cpp:217] fc8 needs backward computation.
I1105 05:14:58.361799  9049 net.cpp:217] drop7 needs backward computation.
I1105 05:14:58.361804  9049 net.cpp:217] relu7 needs backward computation.
I1105 05:14:58.361809  9049 net.cpp:217] fc7 needs backward computation.
I1105 05:14:58.361812  9049 net.cpp:217] drop6 needs backward computation.
I1105 05:14:58.361817  9049 net.cpp:217] relu6 needs backward computation.
I1105 05:14:58.361821  9049 net.cpp:217] fc6 needs backward computation.
I1105 05:14:58.361826  9049 net.cpp:217] pool5 needs backward computation.
I1105 05:14:58.361832  9049 net.cpp:217] relu4 needs backward computation.
I1105 05:14:58.361837  9049 net.cpp:217] conv4 needs backward computation.
I1105 05:14:58.361842  9049 net.cpp:217] relu3 needs backward computation.
I1105 05:14:58.361846  9049 net.cpp:217] conv3 needs backward computation.
I1105 05:14:58.361851  9049 net.cpp:217] norm2 needs backward computation.
I1105 05:14:58.361856  9049 net.cpp:217] pool2 needs backward computation.
I1105 05:14:58.361861  9049 net.cpp:217] relu2 needs backward computation.
I1105 05:14:58.361866  9049 net.cpp:217] conv2 needs backward computation.
I1105 05:14:58.361871  9049 net.cpp:217] norm1 needs backward computation.
I1105 05:14:58.361876  9049 net.cpp:217] pool1 needs backward computation.
I1105 05:14:58.361881  9049 net.cpp:217] relu1 needs backward computation.
I1105 05:14:58.361884  9049 net.cpp:217] conv1 needs backward computation.
I1105 05:14:58.361891  9049 net.cpp:219] label_img_1_split does not need backward computation.
I1105 05:14:58.361896  9049 net.cpp:219] img does not need backward computation.
I1105 05:14:58.361899  9049 net.cpp:261] This network produces output accuracy
I1105 05:14:58.361904  9049 net.cpp:261] This network produces output loss
I1105 05:14:58.361917  9049 net.cpp:274] Network initialization done.
I1105 05:14:58.361979  9049 solver.cpp:60] Solver scaffolding done.
I1105 05:14:58.370252  9049 solver.cpp:337] Iteration 0, Testing net (#0)
I1105 05:26:00.066905  9049 solver.cpp:404]     Test net output #0: accuracy = 0.0439835
I1105 05:26:00.066957  9049 solver.cpp:404]     Test net output #1: loss = 3.7017 (* 1 = 3.7017 loss)
I1105 05:26:01.434597  9049 solver.cpp:228] Iteration 0, loss = 3.73038
I1105 05:26:01.434643  9049 solver.cpp:244]     Train net output #0: accuracy = 0
I1105 05:26:01.434651  9049 solver.cpp:244]     Train net output #1: loss = 3.73038 (* 1 = 3.73038 loss)
I1105 05:26:01.434661  9049 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1105 05:40:31.173943  9049 solver.cpp:228] Iteration 200, loss = 2.05132
I1105 05:40:31.174010  9049 solver.cpp:244]     Train net output #0: accuracy = 0.390625
I1105 05:40:31.174111  9049 solver.cpp:244]     Train net output #1: loss = 2.05132 (* 1 = 2.05132 loss)
I1105 05:40:31.174190  9049 sgd_solver.cpp:106] Iteration 200, lr = 0.01
I1105 05:54:19.409898  9049 solver.cpp:228] Iteration 400, loss = 1.51189
I1105 05:54:19.409947  9049 solver.cpp:244]     Train net output #0: accuracy = 0.59375
I1105 05:54:19.409970  9049 solver.cpp:244]     Train net output #1: loss = 1.51189 (* 1 = 1.51189 loss)
I1105 05:54:19.409986  9049 sgd_solver.cpp:106] Iteration 400, lr = 0.01
>>> 2016-11-05 06:01:37.440080 Begin model classification tests
>>> 2016-11-05 06:15:17.489277 Iteration 500 mean classification accuracy  0.0
>>> 2016-11-05 06:15:17.489313 Iteration 500 mean testing loss 1.55997791335
>>> 2016-11-05 06:15:17.489329 Iteration 500 mean confusion matrix [[ 0.04244482  0.          0.         ...,  0.          0.          0.        ]
 [ 0.          0.0008489   0.00127334 ...,  0.0008489   0.          0.        ]
 [ 0.          0.          0.02504244 ...,  0.          0.          0.        ]
 ..., 
 [ 0.          0.          0.         ...,  0.03310696  0.          0.        ]
 [ 0.          0.          0.         ...,  0.          0.          0.        ]
 [ 0.          0.          0.         ...,  0.          0.          0.        ]]
I1105 06:21:39.169642  9049 solver.cpp:228] Iteration 600, loss = 0.980154
I1105 06:21:39.169688  9049 solver.cpp:244]     Train net output #0: accuracy = 0.71875
I1105 06:21:39.169699  9049 solver.cpp:244]     Train net output #1: loss = 0.980154 (* 1 = 0.980154 loss)
I1105 06:21:39.169720  9049 sgd_solver.cpp:106] Iteration 600, lr = 0.01
I1105 06:34:25.889320  9049 solver.cpp:228] Iteration 800, loss = 1.11533
I1105 06:34:25.889366  9049 solver.cpp:244]     Train net output #0: accuracy = 0.679688
I1105 06:34:25.889389  9049 solver.cpp:244]     Train net output #1: loss = 1.11533 (* 1 = 1.11533 loss)
I1105 06:34:25.889402  9049 sgd_solver.cpp:106] Iteration 800, lr = 0.01
>>> 2016-11-05 06:46:32.434149 Begin model classification tests
>>> 2016-11-05 06:59:51.251858 Iteration 1000 mean classification accuracy  0.0
>>> 2016-11-05 06:59:51.251903 Iteration 1000 mean testing loss 1.19247565744
>>> 2016-11-05 06:59:51.251925 Iteration 1000 mean confusion matrix [[ 0.04244482  0.          0.         ...,  0.          0.          0.        ]
 [ 0.          0.01485569  0.00212224 ...,  0.          0.          0.        ]
 [ 0.          0.          0.0352292  ...,  0.          0.          0.        ]
 ..., 
 [ 0.          0.          0.         ...,  0.03098472  0.          0.        ]
 [ 0.          0.          0.         ...,  0.          0.          0.        ]
 [ 0.          0.          0.         ...,  0.          0.          0.        ]]
I1105 06:59:55.110112  9049 solver.cpp:228] Iteration 1000, loss = 1.06034
I1105 06:59:55.110157  9049 solver.cpp:244]     Train net output #0: accuracy = 0.6875
I1105 06:59:55.110198  9049 solver.cpp:244]     Train net output #1: loss = 1.06034 (* 1 = 1.06034 loss)
I1105 06:59:55.110218  9049 sgd_solver.cpp:106] Iteration 1000, lr = 0.01
I1105 07:11:58.959385  9049 solver.cpp:228] Iteration 1200, loss = 1.00495
I1105 07:11:58.959439  9049 solver.cpp:244]     Train net output #0: accuracy = 0.734375
I1105 07:11:58.959460  9049 solver.cpp:244]     Train net output #1: loss = 1.00495 (* 1 = 1.00495 loss)
I1105 07:11:58.959476  9049 sgd_solver.cpp:106] Iteration 1200, lr = 0.01
I1105 07:24:07.621906  9049 solver.cpp:228] Iteration 1400, loss = 0.712586
I1105 07:24:07.621954  9049 solver.cpp:244]     Train net output #0: accuracy = 0.765625
I1105 07:24:07.621976  9049 solver.cpp:244]     Train net output #1: loss = 0.712586 (* 1 = 0.712586 loss)
I1105 07:24:07.621991  9049 sgd_solver.cpp:106] Iteration 1400, lr = 0.01
>>> 2016-11-05 07:30:04.720494 Begin model classification tests
>>> 2016-11-05 07:46:25.360828 Iteration 1500 mean classification accuracy  0.0
>>> 2016-11-05 07:46:25.360905 Iteration 1500 mean testing loss 1.01545036848
>>> 2016-11-05 07:46:25.360938 Iteration 1500 mean confusion matrix [[ 0.04244482  0.          0.         ...,  0.          0.          0.        ]
 [ 0.          0.01740238  0.00127334 ...,  0.          0.          0.        ]
 [ 0.          0.          0.03438031 ...,  0.          0.          0.        ]
 ..., 
 [ 0.          0.          0.         ...,  0.03013582  0.          0.        ]
 [ 0.          0.          0.         ...,  0.          0.          0.        ]
 [ 0.          0.          0.         ...,  0.          0.          0.        ]]
I1105 07:52:28.496239  9049 solver.cpp:228] Iteration 1600, loss = 0.772745
I1105 07:52:28.496284  9049 solver.cpp:244]     Train net output #0: accuracy = 0.765625
I1105 07:52:28.496299  9049 solver.cpp:244]     Train net output #1: loss = 0.772745 (* 1 = 0.772745 loss)
I1105 07:52:28.496311  9049 sgd_solver.cpp:106] Iteration 1600, lr = 0.01
I1105 08:04:49.560022  9049 solver.cpp:228] Iteration 1800, loss = 0.568573
I1105 08:04:49.560058  9049 solver.cpp:244]     Train net output #0: accuracy = 0.859375
I1105 08:04:49.560068  9049 solver.cpp:244]     Train net output #1: loss = 0.568573 (* 1 = 0.568573 loss)
I1105 08:04:49.560087  9049 sgd_solver.cpp:106] Iteration 1800, lr = 0.01
>>> 2016-11-05 08:17:49.140222 Begin model classification tests
>>> 2016-11-05 08:31:31.155478 Iteration 2000 mean classification accuracy  0.0
>>> 2016-11-05 08:31:31.155591 Iteration 2000 mean testing loss 0.837623334172
>>> 2016-11-05 08:31:31.155632 Iteration 2000 mean confusion matrix [[ 0.04244482  0.          0.         ...,  0.          0.          0.        ]
 [ 0.          0.01867572  0.0008489  ...,  0.          0.          0.        ]
 [ 0.          0.          0.03820034 ...,  0.          0.          0.        ]
 ..., 
 [ 0.          0.          0.         ...,  0.03692699  0.          0.        ]
 [ 0.          0.          0.         ...,  0.          0.          0.        ]
 [ 0.          0.          0.         ...,  0.          0.          0.        ]]
I1105 08:31:34.533776  9049 solver.cpp:228] Iteration 2000, loss = 0.853066
I1105 08:31:34.533854  9049 solver.cpp:244]     Train net output #0: accuracy = 0.742188
I1105 08:31:34.533877  9049 solver.cpp:244]     Train net output #1: loss = 0.853066 (* 1 = 0.853066 loss)
I1105 08:31:34.533893  9049 sgd_solver.cpp:106] Iteration 2000, lr = 0.01
I1105 08:44:02.113123  9049 solver.cpp:228] Iteration 2200, loss = 0.670896
I1105 08:44:02.113174  9049 solver.cpp:244]     Train net output #0: accuracy = 0.796875
I1105 08:44:02.113198  9049 solver.cpp:244]     Train net output #1: loss = 0.670896 (* 1 = 0.670896 loss)
I1105 08:44:02.113212  9049 sgd_solver.cpp:106] Iteration 2200, lr = 0.01
I1105 08:58:38.175336  9049 solver.cpp:228] Iteration 2400, loss = 0.55733
I1105 08:58:38.175381  9049 solver.cpp:244]     Train net output #0: accuracy = 0.789062
I1105 08:58:38.175391  9049 solver.cpp:244]     Train net output #1: loss = 0.55733 (* 1 = 0.55733 loss)
I1105 08:58:38.175401  9049 sgd_solver.cpp:106] Iteration 2400, lr = 0.01
>>> 2016-11-05 09:05:57.387238 Begin model classification tests
>>> 2016-11-05 09:20:36.722044 Iteration 2500 mean classification accuracy  0.0
>>> 2016-11-05 09:20:36.722090 Iteration 2500 mean testing loss 0.799167280415
>>> 2016-11-05 09:20:36.722111 Iteration 2500 mean confusion matrix [[ 0.04244482  0.          0.         ...,  0.          0.          0.        ]
 [ 0.          0.01655348  0.00169779 ...,  0.          0.          0.        ]
 [ 0.          0.          0.03480475 ...,  0.          0.          0.        ]
 ..., 
 [ 0.          0.          0.         ...,  0.03565365  0.          0.        ]
 [ 0.          0.          0.         ...,  0.          0.          0.        ]
 [ 0.          0.          0.         ...,  0.          0.          0.        ]]
I1105 09:26:51.476567  9049 solver.cpp:228] Iteration 2600, loss = 0.625538
I1105 09:26:51.476613  9049 solver.cpp:244]     Train net output #0: accuracy = 0.796875
I1105 09:26:51.476622  9049 solver.cpp:244]     Train net output #1: loss = 0.625538 (* 1 = 0.625538 loss)
I1105 09:26:51.476642  9049 sgd_solver.cpp:106] Iteration 2600, lr = 0.01
I1105 09:39:48.936162  9049 solver.cpp:228] Iteration 2800, loss = 0.604024
I1105 09:39:48.936209  9049 solver.cpp:244]     Train net output #0: accuracy = 0.789062
I1105 09:39:48.936231  9049 solver.cpp:244]     Train net output #1: loss = 0.604024 (* 1 = 0.604024 loss)
I1105 09:39:48.936246  9049 sgd_solver.cpp:106] Iteration 2800, lr = 0.01
>>> 2016-11-05 09:51:36.874259 Begin model classification tests
>>> 2016-11-05 10:05:29.032285 Iteration 3000 mean classification accuracy  0.0
>>> 2016-11-05 10:05:29.032370 Iteration 3000 mean testing loss 0.741899678945
>>> 2016-11-05 10:05:29.032424 Iteration 3000 mean confusion matrix [[ 0.04244482  0.          0.         ...,  0.          0.          0.        ]
 [ 0.          0.02037351  0.00042445 ...,  0.          0.          0.        ]
 [ 0.          0.          0.03565365 ...,  0.          0.          0.        ]
 ..., 
 [ 0.          0.          0.         ...,  0.03438031  0.          0.        ]
 [ 0.          0.          0.         ...,  0.          0.          0.        ]
 [ 0.          0.          0.         ...,  0.          0.          0.        ]]
I1105 10:05:32.496476  9049 solver.cpp:228] Iteration 3000, loss = 0.592465
I1105 10:05:32.496526  9049 solver.cpp:244]     Train net output #0: accuracy = 0.796875
I1105 10:05:32.496546  9049 solver.cpp:244]     Train net output #1: loss = 0.592465 (* 1 = 0.592465 loss)
I1105 10:05:32.496562  9049 sgd_solver.cpp:106] Iteration 3000, lr = 0.01
I1105 10:17:25.850244  9049 solver.cpp:228] Iteration 3200, loss = 0.555988
I1105 10:17:25.850312  9049 solver.cpp:244]     Train net output #0: accuracy = 0.8125
I1105 10:17:25.850350  9049 solver.cpp:244]     Train net output #1: loss = 0.555988 (* 1 = 0.555988 loss)
I1105 10:17:25.850371  9049 sgd_solver.cpp:106] Iteration 3200, lr = 0.01
I1105 10:29:18.105324  9049 solver.cpp:228] Iteration 3400, loss = 0.466465
I1105 10:29:18.105370  9049 solver.cpp:244]     Train net output #0: accuracy = 0.859375
I1105 10:29:18.105386  9049 solver.cpp:244]     Train net output #1: loss = 0.466465 (* 1 = 0.466465 loss)
I1105 10:29:18.105401  9049 sgd_solver.cpp:106] Iteration 3400, lr = 0.01
>>> 2016-11-05 10:35:07.893175 Begin model classification tests
>>> 2016-11-05 10:49:04.667426 Iteration 3500 mean classification accuracy  0.0
>>> 2016-11-05 10:49:04.667469 Iteration 3500 mean testing loss 0.712610665221
>>> 2016-11-05 10:49:04.667547 Iteration 3500 mean confusion matrix [[ 0.04244482  0.          0.         ...,  0.          0.          0.        ]
 [ 0.          0.01910017  0.00127334 ...,  0.          0.          0.        ]
 [ 0.          0.          0.03904924 ...,  0.          0.          0.        ]
 ..., 
 [ 0.          0.          0.         ...,  0.03735144  0.          0.        ]
 [ 0.          0.          0.         ...,  0.          0.          0.        ]
 [ 0.          0.          0.         ...,  0.          0.          0.        ]]
I1105 10:56:47.770943  9049 solver.cpp:228] Iteration 3600, loss = 0.650025
I1105 10:56:47.770995  9049 solver.cpp:244]     Train net output #0: accuracy = 0.804688
I1105 10:56:47.771040  9049 solver.cpp:244]     Train net output #1: loss = 0.650025 (* 1 = 0.650025 loss)
I1105 10:56:47.771065  9049 sgd_solver.cpp:106] Iteration 3600, lr = 0.01
I1105 11:12:02.319398  9049 solver.cpp:228] Iteration 3800, loss = 0.608786
I1105 11:12:02.319442  9049 solver.cpp:244]     Train net output #0: accuracy = 0.8125
I1105 11:12:02.319458  9049 solver.cpp:244]     Train net output #1: loss = 0.608786 (* 1 = 0.608786 loss)
I1105 11:12:02.319470  9049 sgd_solver.cpp:106] Iteration 3800, lr = 0.01
>>> 2016-11-05 11:27:41.007263 Begin model classification tests
>>> 2016-11-05 11:42:17.924122 Iteration 4000 mean classification accuracy  0.0
>>> 2016-11-05 11:42:17.924204 Iteration 4000 mean testing loss 0.706407340184
>>> 2016-11-05 11:42:17.924252 Iteration 4000 mean confusion matrix [[ 0.04244482  0.          0.         ...,  0.          0.          0.        ]
 [ 0.          0.01910017  0.0008489  ...,  0.          0.          0.        ]
 [ 0.          0.          0.03862479 ...,  0.          0.          0.        ]
 ..., 
 [ 0.          0.          0.         ...,  0.03904924  0.          0.        ]
 [ 0.          0.          0.         ...,  0.          0.          0.        ]
 [ 0.          0.          0.         ...,  0.          0.          0.        ]]
I1105 11:42:21.969712  9049 solver.cpp:228] Iteration 4000, loss = 0.433
I1105 11:42:21.969765  9049 solver.cpp:244]     Train net output #0: accuracy = 0.84375
I1105 11:42:21.969784  9049 solver.cpp:244]     Train net output #1: loss = 0.433 (* 1 = 0.433 loss)
I1105 11:42:21.969796  9049 sgd_solver.cpp:106] Iteration 4000, lr = 0.01
I1105 11:57:42.881817  9049 solver.cpp:228] Iteration 4200, loss = 0.493702
I1105 11:57:42.881870  9049 solver.cpp:244]     Train net output #0: accuracy = 0.84375
I1105 11:57:42.881888  9049 solver.cpp:244]     Train net output #1: loss = 0.493702 (* 1 = 0.493702 loss)
I1105 11:57:42.881903  9049 sgd_solver.cpp:106] Iteration 4200, lr = 0.01
I1105 12:10:15.873891  9049 solver.cpp:228] Iteration 4400, loss = 0.475246
I1105 12:10:15.873931  9049 solver.cpp:244]     Train net output #0: accuracy = 0.835938
I1105 12:10:15.873940  9049 solver.cpp:244]     Train net output #1: loss = 0.475246 (* 1 = 0.475246 loss)
I1105 12:10:15.873950  9049 sgd_solver.cpp:106] Iteration 4400, lr = 0.01
>>> 2016-11-05 12:16:24.343015 Begin model classification tests
^CTraceback (most recent call last):
  File "./solve.py", line 27, in <module>
    score.model_classification_test(solver, './tmp/'.format(i), test_list, layer='fc8', gt='label')
  File "/home/kevin/catkin_ws/src/romans_stack/model_net/caffe_net/score.py", line 104, in model_classification_test
    do_tests(solver.test_nets[0], solver.iter, save_dir, test_indices, layer, gt)
  File "/home/kevin/catkin_ws/src/romans_stack/model_net/caffe_net/score.py", line 86, in do_tests
    accuracy, loss, confusion_mat = compute_classification_error(net, save_dir, test_indices, layer, gt)
  File "/home/kevin/catkin_ws/src/romans_stack/model_net/caffe_net/score.py", line 67, in compute_classification_error
    net.forward()
  File "/home/kevin/caffeplus/python/caffe/pycaffe.py", line 121, in _Net_forward
    self._forward(start_ind, end_ind)
  File "/home/kevin/caffeplus/python_layer/data_layers/model_net_layer.py", line 69, in reshape
    self.data, self.label = self.load_data(self.indices, self.idx)
  File "/home/kevin/caffeplus/python_layer/data_layers/model_net_layer.py", line 144, in load_data
    mat = scipy.io.loadmat('{}/{}_{}.mat'.format(self.dataset_dir, file, i+1))
  File "/usr/lib/python2.7/dist-packages/scipy/io/matlab/mio.py", line 126, in loadmat
    matfile_dict = MR.get_variables(variable_names)
  File "/usr/lib/python2.7/dist-packages/scipy/io/matlab/mio5.py", line 268, in get_variables
    hdr, next_position = self.read_var_header()
  File "/usr/lib/python2.7/dist-packages/scipy/io/matlab/mio5.py", line 223, in read_var_header
    mdtype, byte_count = self._matrix_reader.read_full_tag()
KeyboardInterrupt
]0;kevin@kevin-desktop: ~/catkin_ws/src/romans_stack/model_net/caffe_netkevin@kevin-desktop:~/catkin_ws/src/romans_stack/model_net/caffe_net$ ./solve.py
  File "./solve.py", line 24
    test_list = test_list(:10,:)
                          ^
SyntaxError: invalid syntax
]0;kevin@kevin-desktop: ~/catkin_ws/src/romans_stack/model_net/caffe_netkevin@kevin-desktop:~/catkin_ws/src/romans_stack/model_net/caffe_net$ ./solve.py
  File "./solve.py", line 24
    test_list = test_list(:：10,:)
                          ^
SyntaxError: invalid syntax
]0;kevin@kevin-desktop: ~/catkin_ws/src/romans_stack/model_net/caffe_netkevin@kevin-desktop:~/catkin_ws/src/romans_stack/model_net/caffe_net$ ./solve.py
  File "./solve.py", line 24
    test_list = test_list(::10,:)
                          ^
SyntaxError: invalid syntax
]0;kevin@kevin-desktop: ~/catkin_ws/src/romans_stack/model_net/caffe_netkevin@kevin-desktop:~/catkin_ws/src/romans_stack/model_net/caffe_net$ ./solve.py
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Net<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Blob<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Solver<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1105 12:33:43.146371 22398 solver.cpp:48] Initializing solver from parameters: 
train_net: "train.prototxt"
test_net: "test.prototxt"
test_iter: 0
test_interval: 9999999
base_lr: 0.01
display: 50
max_iter: 450000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 10000
snapshot: 100000
snapshot_prefix: "/home/kevin/snapshot"
solver_mode: GPU
I1105 12:33:43.146540 22398 solver.cpp:81] Creating training net from train_net file: train.prototxt
I1105 12:33:43.151770 22398 net.cpp:49] Initializing net from parameters: 
state {
  phase: TRAIN
}
layer {
  name: "img"
  type: "Python"
  top: "img"
  top: "label"
  python_param {
    module: "data_layers.model_net_layer"
    layer: "ModelNetDataLayer"
    param_str: "{\'img_size\': (250, 250), \'dtype\': \'frame\', \'batch_size\': 128, \'seed\': 1337, \'split\': \'train\', \'dataset_dir\': \'/home/kevin/dataset/processed_data\', \'mean\': 2}"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "img"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 0
    kernel_size: 11
    group: 1
    stride: 4
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    name: "conv3_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv3_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    name: "conv4_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv4_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv4"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 40
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1105 12:33:43.187597 22398 layer_factory.hpp:77] Creating layer img
I1105 12:33:43.642381 22398 net.cpp:91] Creating Layer img
I1105 12:33:43.642431 22398 net.cpp:399] img -> img
I1105 12:33:43.642444 22398 net.cpp:399] img -> label
{'img_size': (250, 250), 'dtype': 'frame', 'batch_size': 128, 'seed': 1337, 'split': 'train', 'dataset_dir': '/home/kevin/dataset/processed_data', 'mean': 2}
I1105 12:34:10.321180 22398 net.cpp:141] Setting up img
I1105 12:34:10.321209 22398 net.cpp:148] Top shape: 128 1 250 250 (8000000)
I1105 12:34:10.321215 22398 net.cpp:148] Top shape: 128 1 (128)
I1105 12:34:10.321218 22398 net.cpp:156] Memory required for data: 32000512
I1105 12:34:10.321225 22398 layer_factory.hpp:77] Creating layer label_img_1_split
I1105 12:34:10.321243 22398 net.cpp:91] Creating Layer label_img_1_split
I1105 12:34:10.321246 22398 net.cpp:425] label_img_1_split <- label
I1105 12:34:10.321254 22398 net.cpp:399] label_img_1_split -> label_img_1_split_0
I1105 12:34:10.321264 22398 net.cpp:399] label_img_1_split -> label_img_1_split_1
I1105 12:34:10.321297 22398 net.cpp:141] Setting up label_img_1_split
I1105 12:34:10.321303 22398 net.cpp:148] Top shape: 128 1 (128)
I1105 12:34:10.321308 22398 net.cpp:148] Top shape: 128 1 (128)
I1105 12:34:10.321311 22398 net.cpp:156] Memory required for data: 32001536
I1105 12:34:10.321316 22398 layer_factory.hpp:77] Creating layer conv1
I1105 12:34:10.321327 22398 net.cpp:91] Creating Layer conv1
I1105 12:34:10.321331 22398 net.cpp:425] conv1 <- img
I1105 12:34:10.321336 22398 net.cpp:399] conv1 -> conv1
I1105 12:34:10.333856 22398 net.cpp:141] Setting up conv1
I1105 12:34:10.333881 22398 net.cpp:148] Top shape: 128 96 60 60 (44236800)
I1105 12:34:10.333886 22398 net.cpp:156] Memory required for data: 208948736
I1105 12:34:10.333900 22398 layer_factory.hpp:77] Creating layer relu1
I1105 12:34:10.333909 22398 net.cpp:91] Creating Layer relu1
I1105 12:34:10.333914 22398 net.cpp:425] relu1 <- conv1
I1105 12:34:10.333920 22398 net.cpp:386] relu1 -> conv1 (in-place)
I1105 12:34:10.333926 22398 net.cpp:141] Setting up relu1
I1105 12:34:10.333931 22398 net.cpp:148] Top shape: 128 96 60 60 (44236800)
I1105 12:34:10.333935 22398 net.cpp:156] Memory required for data: 385895936
I1105 12:34:10.333938 22398 layer_factory.hpp:77] Creating layer pool1
I1105 12:34:10.333945 22398 net.cpp:91] Creating Layer pool1
I1105 12:34:10.333948 22398 net.cpp:425] pool1 <- conv1
I1105 12:34:10.333953 22398 net.cpp:399] pool1 -> pool1
I1105 12:34:10.333992 22398 net.cpp:141] Setting up pool1
I1105 12:34:10.333997 22398 net.cpp:148] Top shape: 128 96 30 30 (11059200)
I1105 12:34:10.334002 22398 net.cpp:156] Memory required for data: 430132736
I1105 12:34:10.334004 22398 layer_factory.hpp:77] Creating layer norm1
I1105 12:34:10.334012 22398 net.cpp:91] Creating Layer norm1
I1105 12:34:10.334014 22398 net.cpp:425] norm1 <- pool1
I1105 12:34:10.334019 22398 net.cpp:399] norm1 -> norm1
I1105 12:34:10.334050 22398 net.cpp:141] Setting up norm1
I1105 12:34:10.334055 22398 net.cpp:148] Top shape: 128 96 30 30 (11059200)
I1105 12:34:10.334059 22398 net.cpp:156] Memory required for data: 474369536
I1105 12:34:10.334062 22398 layer_factory.hpp:77] Creating layer conv2
I1105 12:34:10.334071 22398 net.cpp:91] Creating Layer conv2
I1105 12:34:10.334075 22398 net.cpp:425] conv2 <- norm1
I1105 12:34:10.334080 22398 net.cpp:399] conv2 -> conv2
I1105 12:34:10.336738 22398 net.cpp:141] Setting up conv2
I1105 12:34:10.336755 22398 net.cpp:148] Top shape: 128 256 30 30 (29491200)
I1105 12:34:10.336760 22398 net.cpp:156] Memory required for data: 592334336
I1105 12:34:10.336771 22398 layer_factory.hpp:77] Creating layer relu2
I1105 12:34:10.336777 22398 net.cpp:91] Creating Layer relu2
I1105 12:34:10.336781 22398 net.cpp:425] relu2 <- conv2
I1105 12:34:10.336787 22398 net.cpp:386] relu2 -> conv2 (in-place)
I1105 12:34:10.336793 22398 net.cpp:141] Setting up relu2
I1105 12:34:10.336798 22398 net.cpp:148] Top shape: 128 256 30 30 (29491200)
I1105 12:34:10.336802 22398 net.cpp:156] Memory required for data: 710299136
I1105 12:34:10.336805 22398 layer_factory.hpp:77] Creating layer pool2
I1105 12:34:10.336812 22398 net.cpp:91] Creating Layer pool2
I1105 12:34:10.336815 22398 net.cpp:425] pool2 <- conv2
I1105 12:34:10.336820 22398 net.cpp:399] pool2 -> pool2
I1105 12:34:10.336854 22398 net.cpp:141] Setting up pool2
I1105 12:34:10.336860 22398 net.cpp:148] Top shape: 128 256 15 15 (7372800)
I1105 12:34:10.336864 22398 net.cpp:156] Memory required for data: 739790336
I1105 12:34:10.336868 22398 layer_factory.hpp:77] Creating layer norm2
I1105 12:34:10.336874 22398 net.cpp:91] Creating Layer norm2
I1105 12:34:10.336877 22398 net.cpp:425] norm2 <- pool2
I1105 12:34:10.336882 22398 net.cpp:399] norm2 -> norm2
I1105 12:34:10.336908 22398 net.cpp:141] Setting up norm2
I1105 12:34:10.336913 22398 net.cpp:148] Top shape: 128 256 15 15 (7372800)
I1105 12:34:10.336917 22398 net.cpp:156] Memory required for data: 769281536
I1105 12:34:10.336920 22398 layer_factory.hpp:77] Creating layer conv3
I1105 12:34:10.336930 22398 net.cpp:91] Creating Layer conv3
I1105 12:34:10.336932 22398 net.cpp:425] conv3 <- norm2
I1105 12:34:10.336937 22398 net.cpp:399] conv3 -> conv3
I1105 12:34:10.340133 22398 net.cpp:141] Setting up conv3
I1105 12:34:10.340149 22398 net.cpp:148] Top shape: 128 384 15 15 (11059200)
I1105 12:34:10.340153 22398 net.cpp:156] Memory required for data: 813518336
I1105 12:34:10.340163 22398 layer_factory.hpp:77] Creating layer relu3
I1105 12:34:10.340170 22398 net.cpp:91] Creating Layer relu3
I1105 12:34:10.340174 22398 net.cpp:425] relu3 <- conv3
I1105 12:34:10.340179 22398 net.cpp:386] relu3 -> conv3 (in-place)
I1105 12:34:10.340186 22398 net.cpp:141] Setting up relu3
I1105 12:34:10.340190 22398 net.cpp:148] Top shape: 128 384 15 15 (11059200)
I1105 12:34:10.340194 22398 net.cpp:156] Memory required for data: 857755136
I1105 12:34:10.340198 22398 layer_factory.hpp:77] Creating layer conv4
I1105 12:34:10.340206 22398 net.cpp:91] Creating Layer conv4
I1105 12:34:10.340210 22398 net.cpp:425] conv4 <- conv3
I1105 12:34:10.340215 22398 net.cpp:399] conv4 -> conv4
I1105 12:34:10.343405 22398 net.cpp:141] Setting up conv4
I1105 12:34:10.343418 22398 net.cpp:148] Top shape: 128 256 15 15 (7372800)
I1105 12:34:10.343422 22398 net.cpp:156] Memory required for data: 887246336
I1105 12:34:10.343428 22398 layer_factory.hpp:77] Creating layer relu4
I1105 12:34:10.343436 22398 net.cpp:91] Creating Layer relu4
I1105 12:34:10.343438 22398 net.cpp:425] relu4 <- conv4
I1105 12:34:10.343444 22398 net.cpp:386] relu4 -> conv4 (in-place)
I1105 12:34:10.343451 22398 net.cpp:141] Setting up relu4
I1105 12:34:10.343454 22398 net.cpp:148] Top shape: 128 256 15 15 (7372800)
I1105 12:34:10.343458 22398 net.cpp:156] Memory required for data: 916737536
I1105 12:34:10.343461 22398 layer_factory.hpp:77] Creating layer pool5
I1105 12:34:10.343467 22398 net.cpp:91] Creating Layer pool5
I1105 12:34:10.343471 22398 net.cpp:425] pool5 <- conv4
I1105 12:34:10.343475 22398 net.cpp:399] pool5 -> pool5
I1105 12:34:10.343513 22398 net.cpp:141] Setting up pool5
I1105 12:34:10.343521 22398 net.cpp:148] Top shape: 128 256 7 7 (1605632)
I1105 12:34:10.343525 22398 net.cpp:156] Memory required for data: 923160064
I1105 12:34:10.343528 22398 layer_factory.hpp:77] Creating layer fc6
I1105 12:34:10.343535 22398 net.cpp:91] Creating Layer fc6
I1105 12:34:10.343539 22398 net.cpp:425] fc6 <- pool5
I1105 12:34:10.343544 22398 net.cpp:399] fc6 -> fc6
I1105 12:34:10.701658 22398 net.cpp:141] Setting up fc6
I1105 12:34:10.701699 22398 net.cpp:148] Top shape: 128 4096 (524288)
I1105 12:34:10.701709 22398 net.cpp:156] Memory required for data: 925257216
I1105 12:34:10.701730 22398 layer_factory.hpp:77] Creating layer relu6
I1105 12:34:10.701748 22398 net.cpp:91] Creating Layer relu6
I1105 12:34:10.701757 22398 net.cpp:425] relu6 <- fc6
I1105 12:34:10.701768 22398 net.cpp:386] relu6 -> fc6 (in-place)
I1105 12:34:10.701781 22398 net.cpp:141] Setting up relu6
I1105 12:34:10.701791 22398 net.cpp:148] Top shape: 128 4096 (524288)
I1105 12:34:10.701798 22398 net.cpp:156] Memory required for data: 927354368
I1105 12:34:10.701807 22398 layer_factory.hpp:77] Creating layer drop6
I1105 12:34:10.701825 22398 net.cpp:91] Creating Layer drop6
I1105 12:34:10.701833 22398 net.cpp:425] drop6 <- fc6
I1105 12:34:10.701843 22398 net.cpp:386] drop6 -> fc6 (in-place)
I1105 12:34:10.701864 22398 net.cpp:141] Setting up drop6
I1105 12:34:10.701874 22398 net.cpp:148] Top shape: 128 4096 (524288)
I1105 12:34:10.701882 22398 net.cpp:156] Memory required for data: 929451520
I1105 12:34:10.701890 22398 layer_factory.hpp:77] Creating layer fc7
I1105 12:34:10.701902 22398 net.cpp:91] Creating Layer fc7
I1105 12:34:10.701910 22398 net.cpp:425] fc7 <- fc6
I1105 12:34:10.701920 22398 net.cpp:399] fc7 -> fc7
I1105 12:34:10.812315 22398 net.cpp:141] Setting up fc7
I1105 12:34:10.812352 22398 net.cpp:148] Top shape: 128 4096 (524288)
I1105 12:34:10.812361 22398 net.cpp:156] Memory required for data: 931548672
I1105 12:34:10.812386 22398 layer_factory.hpp:77] Creating layer relu7
I1105 12:34:10.812420 22398 net.cpp:91] Creating Layer relu7
I1105 12:34:10.812439 22398 net.cpp:425] relu7 <- fc7
I1105 12:34:10.812458 22398 net.cpp:386] relu7 -> fc7 (in-place)
I1105 12:34:10.812480 22398 net.cpp:141] Setting up relu7
I1105 12:34:10.812489 22398 net.cpp:148] Top shape: 128 4096 (524288)
I1105 12:34:10.812507 22398 net.cpp:156] Memory required for data: 933645824
I1105 12:34:10.812525 22398 layer_factory.hpp:77] Creating layer drop7
I1105 12:34:10.812546 22398 net.cpp:91] Creating Layer drop7
I1105 12:34:10.812551 22398 net.cpp:425] drop7 <- fc7
I1105 12:34:10.812577 22398 net.cpp:386] drop7 -> fc7 (in-place)
I1105 12:34:10.812628 22398 net.cpp:141] Setting up drop7
I1105 12:34:10.812647 22398 net.cpp:148] Top shape: 128 4096 (524288)
I1105 12:34:10.812655 22398 net.cpp:156] Memory required for data: 935742976
I1105 12:34:10.812674 22398 layer_factory.hpp:77] Creating layer fc8
I1105 12:34:10.812685 22398 net.cpp:91] Creating Layer fc8
I1105 12:34:10.812693 22398 net.cpp:425] fc8 <- fc7
I1105 12:34:10.812703 22398 net.cpp:399] fc8 -> fc8
I1105 12:34:10.814142 22398 net.cpp:141] Setting up fc8
I1105 12:34:10.814157 22398 net.cpp:148] Top shape: 128 40 (5120)
I1105 12:34:10.814177 22398 net.cpp:156] Memory required for data: 935763456
I1105 12:34:10.814196 22398 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I1105 12:34:10.814205 22398 net.cpp:91] Creating Layer fc8_fc8_0_split
I1105 12:34:10.814213 22398 net.cpp:425] fc8_fc8_0_split <- fc8
I1105 12:34:10.814232 22398 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1105 12:34:10.814259 22398 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1105 12:34:10.814317 22398 net.cpp:141] Setting up fc8_fc8_0_split
I1105 12:34:10.814337 22398 net.cpp:148] Top shape: 128 40 (5120)
I1105 12:34:10.814353 22398 net.cpp:148] Top shape: 128 40 (5120)
I1105 12:34:10.814360 22398 net.cpp:156] Memory required for data: 935804416
I1105 12:34:10.814368 22398 layer_factory.hpp:77] Creating layer accuracy
I1105 12:34:10.814393 22398 net.cpp:91] Creating Layer accuracy
I1105 12:34:10.814409 22398 net.cpp:425] accuracy <- fc8_fc8_0_split_0
I1105 12:34:10.814425 22398 net.cpp:425] accuracy <- label_img_1_split_0
I1105 12:34:10.814435 22398 net.cpp:399] accuracy -> accuracy
I1105 12:34:10.814446 22398 net.cpp:141] Setting up accuracy
I1105 12:34:10.814455 22398 net.cpp:148] Top shape: (1)
I1105 12:34:10.814462 22398 net.cpp:156] Memory required for data: 935804420
I1105 12:34:10.814471 22398 layer_factory.hpp:77] Creating layer loss
I1105 12:34:10.814483 22398 net.cpp:91] Creating Layer loss
I1105 12:34:10.814491 22398 net.cpp:425] loss <- fc8_fc8_0_split_1
I1105 12:34:10.814499 22398 net.cpp:425] loss <- label_img_1_split_1
I1105 12:34:10.814508 22398 net.cpp:399] loss -> loss
I1105 12:34:10.814519 22398 layer_factory.hpp:77] Creating layer loss
I1105 12:34:10.814607 22398 net.cpp:141] Setting up loss
I1105 12:34:10.814620 22398 net.cpp:148] Top shape: (1)
I1105 12:34:10.814627 22398 net.cpp:151]     with loss weight 1
I1105 12:34:10.814642 22398 net.cpp:156] Memory required for data: 935804424
I1105 12:34:10.814651 22398 net.cpp:217] loss needs backward computation.
I1105 12:34:10.814658 22398 net.cpp:219] accuracy does not need backward computation.
I1105 12:34:10.814667 22398 net.cpp:217] fc8_fc8_0_split needs backward computation.
I1105 12:34:10.814674 22398 net.cpp:217] fc8 needs backward computation.
I1105 12:34:10.814682 22398 net.cpp:217] drop7 needs backward computation.
I1105 12:34:10.814688 22398 net.cpp:217] relu7 needs backward computation.
I1105 12:34:10.814695 22398 net.cpp:217] fc7 needs backward computation.
I1105 12:34:10.814703 22398 net.cpp:217] drop6 needs backward computation.
I1105 12:34:10.814710 22398 net.cpp:217] relu6 needs backward computation.
I1105 12:34:10.814718 22398 net.cpp:217] fc6 needs backward computation.
I1105 12:34:10.814725 22398 net.cpp:217] pool5 needs backward computation.
I1105 12:34:10.814733 22398 net.cpp:217] relu4 needs backward computation.
I1105 12:34:10.814741 22398 net.cpp:217] conv4 needs backward computation.
I1105 12:34:10.814757 22398 net.cpp:217] relu3 needs backward computation.
I1105 12:34:10.814764 22398 net.cpp:217] conv3 needs backward computation.
I1105 12:34:10.814772 22398 net.cpp:217] norm2 needs backward computation.
I1105 12:34:10.814779 22398 net.cpp:217] pool2 needs backward computation.
I1105 12:34:10.814786 22398 net.cpp:217] relu2 needs backward computation.
I1105 12:34:10.814795 22398 net.cpp:217] conv2 needs backward computation.
I1105 12:34:10.814801 22398 net.cpp:217] norm1 needs backward computation.
I1105 12:34:10.814808 22398 net.cpp:217] pool1 needs backward computation.
I1105 12:34:10.814816 22398 net.cpp:217] relu1 needs backward computation.
I1105 12:34:10.814823 22398 net.cpp:217] conv1 needs backward computation.
I1105 12:34:10.814831 22398 net.cpp:219] label_img_1_split does not need backward computation.
I1105 12:34:10.814839 22398 net.cpp:219] img does not need backward computation.
I1105 12:34:10.814846 22398 net.cpp:261] This network produces output accuracy
I1105 12:34:10.814853 22398 net.cpp:261] This network produces output loss
I1105 12:34:10.814872 22398 net.cpp:274] Network initialization done.
I1105 12:34:10.815361 22398 solver.cpp:181] Creating test net (#0) specified by test_net file: test.prototxt
I1105 12:34:10.815556 22398 net.cpp:49] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "img"
  type: "Python"
  top: "img"
  top: "label"
  python_param {
    module: "data_layers.model_net_layer"
    layer: "ModelNetDataLayer"
    param_str: "{\'img_size\': (250, 250), \'dtype\': \'object\', \'batch_size\': 128, \'seed\': 1337, \'split\': \'test\', \'dataset_dir\': \'/home/kevin/dataset/processed_data\', \'mean\': 2}"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "img"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 0
    kernel_size: 11
    group: 1
    stride: 4
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    name: "conv3_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv3_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    name: "conv4_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv4_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv4"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 40
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1105 12:34:10.816694 22398 layer_factory.hpp:77] Creating layer img
I1105 12:34:10.816745 22398 net.cpp:91] Creating Layer img
I1105 12:34:10.816756 22398 net.cpp:399] img -> img
I1105 12:34:10.816767 22398 net.cpp:399] img -> label
{'img_size': (250, 250), 'dtype': 'object', 'batch_size': 128, 'seed': 1337, 'split': 'test', 'dataset_dir': '/home/kevin/dataset/processed_data', 'mean': 2}
I1105 12:34:11.027572 22398 net.cpp:141] Setting up img
I1105 12:34:11.027637 22398 net.cpp:148] Top shape: 24 1 250 250 (1500000)
I1105 12:34:11.027652 22398 net.cpp:148] Top shape: 24 1 (24)
I1105 12:34:11.027671 22398 net.cpp:156] Memory required for data: 6000096
I1105 12:34:11.027688 22398 layer_factory.hpp:77] Creating layer label_img_1_split
I1105 12:34:11.027721 22398 net.cpp:91] Creating Layer label_img_1_split
I1105 12:34:11.027743 22398 net.cpp:425] label_img_1_split <- label
I1105 12:34:11.027770 22398 net.cpp:399] label_img_1_split -> label_img_1_split_0
I1105 12:34:11.027796 22398 net.cpp:399] label_img_1_split -> label_img_1_split_1
I1105 12:34:11.027876 22398 net.cpp:141] Setting up label_img_1_split
I1105 12:34:11.027901 22398 net.cpp:148] Top shape: 24 1 (24)
I1105 12:34:11.027923 22398 net.cpp:148] Top shape: 24 1 (24)
I1105 12:34:11.027935 22398 net.cpp:156] Memory required for data: 6000288
I1105 12:34:11.027947 22398 layer_factory.hpp:77] Creating layer conv1
I1105 12:34:11.027973 22398 net.cpp:91] Creating Layer conv1
I1105 12:34:11.028007 22398 net.cpp:425] conv1 <- img
I1105 12:34:11.028029 22398 net.cpp:399] conv1 -> conv1
I1105 12:34:11.028501 22398 net.cpp:141] Setting up conv1
I1105 12:34:11.028530 22398 net.cpp:148] Top shape: 24 96 60 60 (8294400)
I1105 12:34:11.028553 22398 net.cpp:156] Memory required for data: 39177888
I1105 12:34:11.028583 22398 layer_factory.hpp:77] Creating layer relu1
I1105 12:34:11.028604 22398 net.cpp:91] Creating Layer relu1
I1105 12:34:11.028625 22398 net.cpp:425] relu1 <- conv1
I1105 12:34:11.028648 22398 net.cpp:386] relu1 -> conv1 (in-place)
I1105 12:34:11.028674 22398 net.cpp:141] Setting up relu1
I1105 12:34:11.028697 22398 net.cpp:148] Top shape: 24 96 60 60 (8294400)
I1105 12:34:11.028707 22398 net.cpp:156] Memory required for data: 72355488
I1105 12:34:11.028718 22398 layer_factory.hpp:77] Creating layer pool1
I1105 12:34:11.028736 22398 net.cpp:91] Creating Layer pool1
I1105 12:34:11.028745 22398 net.cpp:425] pool1 <- conv1
I1105 12:34:11.028756 22398 net.cpp:399] pool1 -> pool1
I1105 12:34:11.028820 22398 net.cpp:141] Setting up pool1
I1105 12:34:11.028837 22398 net.cpp:148] Top shape: 24 96 30 30 (2073600)
I1105 12:34:11.028849 22398 net.cpp:156] Memory required for data: 80649888
I1105 12:34:11.028859 22398 layer_factory.hpp:77] Creating layer norm1
I1105 12:34:11.028877 22398 net.cpp:91] Creating Layer norm1
I1105 12:34:11.028889 22398 net.cpp:425] norm1 <- pool1
I1105 12:34:11.028903 22398 net.cpp:399] norm1 -> norm1
I1105 12:34:11.028959 22398 net.cpp:141] Setting up norm1
I1105 12:34:11.028970 22398 net.cpp:148] Top shape: 24 96 30 30 (2073600)
I1105 12:34:11.028980 22398 net.cpp:156] Memory required for data: 88944288
I1105 12:34:11.028992 22398 layer_factory.hpp:77] Creating layer conv2
I1105 12:34:11.029012 22398 net.cpp:91] Creating Layer conv2
I1105 12:34:11.029023 22398 net.cpp:425] conv2 <- norm1
I1105 12:34:11.029038 22398 net.cpp:399] conv2 -> conv2
I1105 12:34:11.032819 22398 net.cpp:141] Setting up conv2
I1105 12:34:11.032841 22398 net.cpp:148] Top shape: 24 256 30 30 (5529600)
I1105 12:34:11.032852 22398 net.cpp:156] Memory required for data: 111062688
I1105 12:34:11.032871 22398 layer_factory.hpp:77] Creating layer relu2
I1105 12:34:11.032886 22398 net.cpp:91] Creating Layer relu2
I1105 12:34:11.032897 22398 net.cpp:425] relu2 <- conv2
I1105 12:34:11.032910 22398 net.cpp:386] relu2 -> conv2 (in-place)
I1105 12:34:11.032928 22398 net.cpp:141] Setting up relu2
I1105 12:34:11.032941 22398 net.cpp:148] Top shape: 24 256 30 30 (5529600)
I1105 12:34:11.032954 22398 net.cpp:156] Memory required for data: 133181088
I1105 12:34:11.032965 22398 layer_factory.hpp:77] Creating layer pool2
I1105 12:34:11.032984 22398 net.cpp:91] Creating Layer pool2
I1105 12:34:11.032995 22398 net.cpp:425] pool2 <- conv2
I1105 12:34:11.033010 22398 net.cpp:399] pool2 -> pool2
I1105 12:34:11.033068 22398 net.cpp:141] Setting up pool2
I1105 12:34:11.033087 22398 net.cpp:148] Top shape: 24 256 15 15 (1382400)
I1105 12:34:11.033097 22398 net.cpp:156] Memory required for data: 138710688
I1105 12:34:11.033108 22398 layer_factory.hpp:77] Creating layer norm2
I1105 12:34:11.033124 22398 net.cpp:91] Creating Layer norm2
I1105 12:34:11.033135 22398 net.cpp:425] norm2 <- pool2
I1105 12:34:11.033149 22398 net.cpp:399] norm2 -> norm2
I1105 12:34:11.033207 22398 net.cpp:141] Setting up norm2
I1105 12:34:11.033223 22398 net.cpp:148] Top shape: 24 256 15 15 (1382400)
I1105 12:34:11.033236 22398 net.cpp:156] Memory required for data: 144240288
I1105 12:34:11.033243 22398 layer_factory.hpp:77] Creating layer conv3
I1105 12:34:11.033262 22398 net.cpp:91] Creating Layer conv3
I1105 12:34:11.033275 22398 net.cpp:425] conv3 <- norm2
I1105 12:34:11.033290 22398 net.cpp:399] conv3 -> conv3
I1105 12:34:11.036557 22398 net.cpp:141] Setting up conv3
I1105 12:34:11.036593 22398 net.cpp:148] Top shape: 24 384 15 15 (2073600)
I1105 12:34:11.036602 22398 net.cpp:156] Memory required for data: 152534688
I1105 12:34:11.036622 22398 layer_factory.hpp:77] Creating layer relu3
I1105 12:34:11.036636 22398 net.cpp:91] Creating Layer relu3
I1105 12:34:11.036645 22398 net.cpp:425] relu3 <- conv3
I1105 12:34:11.036654 22398 net.cpp:386] relu3 -> conv3 (in-place)
I1105 12:34:11.036667 22398 net.cpp:141] Setting up relu3
I1105 12:34:11.036677 22398 net.cpp:148] Top shape: 24 384 15 15 (2073600)
I1105 12:34:11.036686 22398 net.cpp:156] Memory required for data: 160829088
I1105 12:34:11.036695 22398 layer_factory.hpp:77] Creating layer conv4
I1105 12:34:11.036710 22398 net.cpp:91] Creating Layer conv4
I1105 12:34:11.036716 22398 net.cpp:425] conv4 <- conv3
I1105 12:34:11.036726 22398 net.cpp:399] conv4 -> conv4
I1105 12:34:11.040943 22398 net.cpp:141] Setting up conv4
I1105 12:34:11.041025 22398 net.cpp:148] Top shape: 24 256 15 15 (1382400)
I1105 12:34:11.041039 22398 net.cpp:156] Memory required for data: 166358688
I1105 12:34:11.041064 22398 layer_factory.hpp:77] Creating layer relu4
I1105 12:34:11.041086 22398 net.cpp:91] Creating Layer relu4
I1105 12:34:11.041098 22398 net.cpp:425] relu4 <- conv4
I1105 12:34:11.041113 22398 net.cpp:386] relu4 -> conv4 (in-place)
I1105 12:34:11.041129 22398 net.cpp:141] Setting up relu4
I1105 12:34:11.041139 22398 net.cpp:148] Top shape: 24 256 15 15 (1382400)
I1105 12:34:11.041147 22398 net.cpp:156] Memory required for data: 171888288
I1105 12:34:11.041155 22398 layer_factory.hpp:77] Creating layer pool5
I1105 12:34:11.041169 22398 net.cpp:91] Creating Layer pool5
I1105 12:34:11.041178 22398 net.cpp:425] pool5 <- conv4
I1105 12:34:11.041188 22398 net.cpp:399] pool5 -> pool5
I1105 12:34:11.041245 22398 net.cpp:141] Setting up pool5
I1105 12:34:11.041260 22398 net.cpp:148] Top shape: 24 256 7 7 (301056)
I1105 12:34:11.041268 22398 net.cpp:156] Memory required for data: 173092512
I1105 12:34:11.041278 22398 layer_factory.hpp:77] Creating layer fc6
I1105 12:34:11.041293 22398 net.cpp:91] Creating Layer fc6
I1105 12:34:11.041301 22398 net.cpp:425] fc6 <- pool5
I1105 12:34:11.041311 22398 net.cpp:399] fc6 -> fc6
I1105 12:34:11.370276 22398 net.cpp:141] Setting up fc6
I1105 12:34:11.370343 22398 net.cpp:148] Top shape: 24 4096 (98304)
I1105 12:34:11.370378 22398 net.cpp:156] Memory required for data: 173485728
I1105 12:34:11.370432 22398 layer_factory.hpp:77] Creating layer relu6
I1105 12:34:11.370471 22398 net.cpp:91] Creating Layer relu6
I1105 12:34:11.370486 22398 net.cpp:425] relu6 <- fc6
I1105 12:34:11.370503 22398 net.cpp:386] relu6 -> fc6 (in-place)
I1105 12:34:11.370522 22398 net.cpp:141] Setting up relu6
I1105 12:34:11.370535 22398 net.cpp:148] Top shape: 24 4096 (98304)
I1105 12:34:11.370548 22398 net.cpp:156] Memory required for data: 173878944
I1105 12:34:11.370558 22398 layer_factory.hpp:77] Creating layer drop6
I1105 12:34:11.370576 22398 net.cpp:91] Creating Layer drop6
I1105 12:34:11.370587 22398 net.cpp:425] drop6 <- fc6
I1105 12:34:11.370601 22398 net.cpp:386] drop6 -> fc6 (in-place)
I1105 12:34:11.370678 22398 net.cpp:141] Setting up drop6
I1105 12:34:11.370704 22398 net.cpp:148] Top shape: 24 4096 (98304)
I1105 12:34:11.370718 22398 net.cpp:156] Memory required for data: 174272160
I1105 12:34:11.370729 22398 layer_factory.hpp:77] Creating layer fc7
I1105 12:34:11.370750 22398 net.cpp:91] Creating Layer fc7
I1105 12:34:11.370759 22398 net.cpp:425] fc7 <- fc6
I1105 12:34:11.370774 22398 net.cpp:399] fc7 -> fc7
I1105 12:34:11.477121 22398 net.cpp:141] Setting up fc7
I1105 12:34:11.477160 22398 net.cpp:148] Top shape: 24 4096 (98304)
I1105 12:34:11.477172 22398 net.cpp:156] Memory required for data: 174665376
I1105 12:34:11.477188 22398 layer_factory.hpp:77] Creating layer relu7
I1105 12:34:11.477202 22398 net.cpp:91] Creating Layer relu7
I1105 12:34:11.477211 22398 net.cpp:425] relu7 <- fc7
I1105 12:34:11.477221 22398 net.cpp:386] relu7 -> fc7 (in-place)
I1105 12:34:11.477233 22398 net.cpp:141] Setting up relu7
I1105 12:34:11.477241 22398 net.cpp:148] Top shape: 24 4096 (98304)
I1105 12:34:11.477247 22398 net.cpp:156] Memory required for data: 175058592
I1105 12:34:11.477253 22398 layer_factory.hpp:77] Creating layer drop7
I1105 12:34:11.477263 22398 net.cpp:91] Creating Layer drop7
I1105 12:34:11.477272 22398 net.cpp:425] drop7 <- fc7
I1105 12:34:11.477282 22398 net.cpp:386] drop7 -> fc7 (in-place)
I1105 12:34:11.477308 22398 net.cpp:141] Setting up drop7
I1105 12:34:11.477319 22398 net.cpp:148] Top shape: 24 4096 (98304)
I1105 12:34:11.477327 22398 net.cpp:156] Memory required for data: 175451808
I1105 12:34:11.477335 22398 layer_factory.hpp:77] Creating layer fc8
I1105 12:34:11.477347 22398 net.cpp:91] Creating Layer fc8
I1105 12:34:11.477355 22398 net.cpp:425] fc8 <- fc7
I1105 12:34:11.477366 22398 net.cpp:399] fc8 -> fc8
I1105 12:34:11.478827 22398 net.cpp:141] Setting up fc8
I1105 12:34:11.478850 22398 net.cpp:148] Top shape: 24 40 (960)
I1105 12:34:11.478859 22398 net.cpp:156] Memory required for data: 175455648
I1105 12:34:11.478871 22398 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I1105 12:34:11.478883 22398 net.cpp:91] Creating Layer fc8_fc8_0_split
I1105 12:34:11.478891 22398 net.cpp:425] fc8_fc8_0_split <- fc8
I1105 12:34:11.478902 22398 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1105 12:34:11.478916 22398 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1105 12:34:11.478952 22398 net.cpp:141] Setting up fc8_fc8_0_split
I1105 12:34:11.478965 22398 net.cpp:148] Top shape: 24 40 (960)
I1105 12:34:11.478973 22398 net.cpp:148] Top shape: 24 40 (960)
I1105 12:34:11.478981 22398 net.cpp:156] Memory required for data: 175463328
I1105 12:34:11.478989 22398 layer_factory.hpp:77] Creating layer accuracy
I1105 12:34:11.478999 22398 net.cpp:91] Creating Layer accuracy
I1105 12:34:11.479008 22398 net.cpp:425] accuracy <- fc8_fc8_0_split_0
I1105 12:34:11.479017 22398 net.cpp:425] accuracy <- label_img_1_split_0
I1105 12:34:11.479027 22398 net.cpp:399] accuracy -> accuracy
I1105 12:34:11.479038 22398 net.cpp:141] Setting up accuracy
I1105 12:34:11.479048 22398 net.cpp:148] Top shape: (1)
I1105 12:34:11.479055 22398 net.cpp:156] Memory required for data: 175463332
I1105 12:34:11.479061 22398 layer_factory.hpp:77] Creating layer loss
I1105 12:34:11.479069 22398 net.cpp:91] Creating Layer loss
I1105 12:34:11.479076 22398 net.cpp:425] loss <- fc8_fc8_0_split_1
I1105 12:34:11.479086 22398 net.cpp:425] loss <- label_img_1_split_1
I1105 12:34:11.479094 22398 net.cpp:399] loss -> loss
I1105 12:34:11.479105 22398 layer_factory.hpp:77] Creating layer loss
I1105 12:34:11.479188 22398 net.cpp:141] Setting up loss
I1105 12:34:11.479200 22398 net.cpp:148] Top shape: (1)
I1105 12:34:11.479207 22398 net.cpp:151]     with loss weight 1
I1105 12:34:11.479223 22398 net.cpp:156] Memory required for data: 175463336
I1105 12:34:11.479229 22398 net.cpp:217] loss needs backward computation.
I1105 12:34:11.479236 22398 net.cpp:219] accuracy does not need backward computation.
I1105 12:34:11.479243 22398 net.cpp:217] fc8_fc8_0_split needs backward computation.
I1105 12:34:11.479251 22398 net.cpp:217] fc8 needs backward computation.
I1105 12:34:11.479260 22398 net.cpp:217] drop7 needs backward computation.
I1105 12:34:11.479267 22398 net.cpp:217] relu7 needs backward computation.
I1105 12:34:11.479275 22398 net.cpp:217] fc7 needs backward computation.
I1105 12:34:11.479284 22398 net.cpp:217] drop6 needs backward computation.
I1105 12:34:11.479291 22398 net.cpp:217] relu6 needs backward computation.
I1105 12:34:11.479300 22398 net.cpp:217] fc6 needs backward computation.
I1105 12:34:11.479308 22398 net.cpp:217] pool5 needs backward computation.
I1105 12:34:11.479316 22398 net.cpp:217] relu4 needs backward computation.
I1105 12:34:11.479324 22398 net.cpp:217] conv4 needs backward computation.
I1105 12:34:11.479332 22398 net.cpp:217] relu3 needs backward computation.
I1105 12:34:11.479341 22398 net.cpp:217] conv3 needs backward computation.
I1105 12:34:11.479349 22398 net.cpp:217] norm2 needs backward computation.
I1105 12:34:11.479357 22398 net.cpp:217] pool2 needs backward computation.
I1105 12:34:11.479365 22398 net.cpp:217] relu2 needs backward computation.
I1105 12:34:11.479373 22398 net.cpp:217] conv2 needs backward computation.
I1105 12:34:11.479382 22398 net.cpp:217] norm1 needs backward computation.
I1105 12:34:11.479389 22398 net.cpp:217] pool1 needs backward computation.
I1105 12:34:11.479398 22398 net.cpp:217] relu1 needs backward computation.
I1105 12:34:11.479405 22398 net.cpp:217] conv1 needs backward computation.
I1105 12:34:11.479414 22398 net.cpp:219] label_img_1_split does not need backward computation.
I1105 12:34:11.479423 22398 net.cpp:219] img does not need backward computation.
I1105 12:34:11.479431 22398 net.cpp:261] This network produces output accuracy
I1105 12:34:11.479439 22398 net.cpp:261] This network produces output loss
I1105 12:34:11.479459 22398 net.cpp:274] Network initialization done.
I1105 12:34:11.479558 22398 solver.cpp:60] Solver scaffolding done.
I1105 12:34:11.515861 22398 solver.cpp:337] Iteration 0, Testing net (#0)
I1105 12:34:12.929081 22398 solver.cpp:228] Iteration 0, loss = 3.73113
I1105 12:34:12.929127 22398 solver.cpp:244]     Train net output #0: accuracy = 0.015625
I1105 12:34:12.929147 22398 solver.cpp:244]     Train net output #1: loss = 3.73113 (* 1 = 3.73113 loss)
I1105 12:34:12.929159 22398 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1105 12:37:04.343021 22398 solver.cpp:228] Iteration 50, loss = 2.96249
I1105 12:37:04.343060 22398 solver.cpp:244]     Train net output #0: accuracy = 0.234375
I1105 12:37:04.343072 22398 solver.cpp:244]     Train net output #1: loss = 2.96249 (* 1 = 2.96249 loss)
I1105 12:37:04.343081 22398 sgd_solver.cpp:106] Iteration 50, lr = 0.01
>>> 2016-11-05 12:39:53.605973 Begin model classification tests
/home/kevin/catkin_ws/src/romans_stack/model_net/caffe_net/score.py:81: RuntimeWarning: invalid value encountered in true_divide
  return accuracy/len(dataset), loss/len(dataset), confusion_mat/(np.tile(confusion_mat.sum(1),(40,1))).T
>>> 2016-11-05 12:41:18.646805 Iteration 100 mean classification accuracy  0.483050847458
>>> 2016-11-05 12:41:18.646836 Iteration 100 mean testing loss 1.66978238577
>>> 2016-11-05 12:41:18.646857 Iteration 100 mean confusion matrix [[ 1.         0.         0.        ...,  0.         0.         0.       ]
 [ 0.         0.         0.06      ...,  0.08       0.         0.       ]
 [ 0.         0.         0.1627907 ...,  0.         0.         0.       ]
 ..., 
 [       nan        nan        nan ...,        nan        nan        nan]
 [       nan        nan        nan ...,        nan        nan        nan]
 [       nan        nan        nan ...,        nan        nan        nan]]
I1105 12:41:21.978076 22398 solver.cpp:228] Iteration 100, loss = 2.70415
I1105 12:41:21.978127 22398 solver.cpp:244]     Train net output #0: accuracy = 0.273438
I1105 12:41:21.978147 22398 solver.cpp:244]     Train net output #1: loss = 2.70415 (* 1 = 2.70415 loss)
I1105 12:41:21.978163 22398 sgd_solver.cpp:106] Iteration 100, lr = 0.01
I1105 12:44:09.315943 22398 solver.cpp:228] Iteration 150, loss = 2.33614
I1105 12:44:09.315987 22398 solver.cpp:244]     Train net output #0: accuracy = 0.328125
I1105 12:44:09.316006 22398 solver.cpp:244]     Train net output #1: loss = 2.33614 (* 1 = 2.33614 loss)
I1105 12:44:09.316020 22398 sgd_solver.cpp:106] Iteration 150, lr = 0.01
^CTraceback (most recent call last):
  File "./solve.py", line 27, in <module>
    solver.step(100)
  File "/home/kevin/caffeplus/python_layer/data_layers/model_net_layer.py", line 69, in reshape
    self.data, self.label = self.load_data(self.indices, self.idx)
  File "/home/kevin/caffeplus/python_layer/data_layers/model_net_layer.py", line 116, in load_data
    datai = cv2.resize(datai, (self.img_size[0],self.img_size[0]))
KeyboardInterrupt
]0;kevin@kevin-desktop: ~/catkin_ws/src/romans_stack/model_net/caffe_netkevin@kevin-desktop:~/catkin_ws/src/romans_stack/model_net/caffe_net$ ./solve.py
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Net<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Blob<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Solver<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1105 12:47:28.767565 23994 solver.cpp:48] Initializing solver from parameters: 
train_net: "train.prototxt"
test_net: "test.prototxt"
test_iter: 0
test_interval: 9999999
base_lr: 0.01
display: 100
max_iter: 450000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 10000
snapshot: 100000
snapshot_prefix: "/home/kevin/snapshot"
solver_mode: GPU
I1105 12:47:28.767725 23994 solver.cpp:81] Creating training net from train_net file: train.prototxt
I1105 12:47:28.768332 23994 net.cpp:49] Initializing net from parameters: 
state {
  phase: TRAIN
}
layer {
  name: "img"
  type: "Python"
  top: "img"
  top: "label"
  python_param {
    module: "data_layers.model_net_layer"
    layer: "ModelNetDataLayer"
    param_str: "{\'img_size\': (250, 250), \'dtype\': \'frame\', \'batch_size\': 128, \'seed\': 1337, \'split\': \'train\', \'dataset_dir\': \'/home/kevin/dataset/processed_data\', \'mean\': 2}"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "img"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 0
    kernel_size: 11
    group: 1
    stride: 4
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    name: "conv3_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv3_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    name: "conv4_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv4_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv4"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 40
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1105 12:47:28.769441 23994 layer_factory.hpp:77] Creating layer img
I1105 12:47:28.855342 23994 net.cpp:91] Creating Layer img
I1105 12:47:28.855376 23994 net.cpp:399] img -> img
I1105 12:47:28.855398 23994 net.cpp:399] img -> label
{'img_size': (250, 250), 'dtype': 'frame', 'batch_size': 128, 'seed': 1337, 'split': 'train', 'dataset_dir': '/home/kevin/dataset/processed_data', 'mean': 2}
I1105 12:47:45.323424 23994 net.cpp:141] Setting up img
I1105 12:47:45.323467 23994 net.cpp:148] Top shape: 128 1 250 250 (8000000)
I1105 12:47:45.323480 23994 net.cpp:148] Top shape: 128 1 (128)
I1105 12:47:45.323496 23994 net.cpp:156] Memory required for data: 32000512
I1105 12:47:45.323508 23994 layer_factory.hpp:77] Creating layer label_img_1_split
I1105 12:47:45.323531 23994 net.cpp:91] Creating Layer label_img_1_split
I1105 12:47:45.323540 23994 net.cpp:425] label_img_1_split <- label
I1105 12:47:45.323551 23994 net.cpp:399] label_img_1_split -> label_img_1_split_0
I1105 12:47:45.323565 23994 net.cpp:399] label_img_1_split -> label_img_1_split_1
I1105 12:47:45.323611 23994 net.cpp:141] Setting up label_img_1_split
I1105 12:47:45.323623 23994 net.cpp:148] Top shape: 128 1 (128)
I1105 12:47:45.323634 23994 net.cpp:148] Top shape: 128 1 (128)
I1105 12:47:45.323642 23994 net.cpp:156] Memory required for data: 32001536
I1105 12:47:45.323649 23994 layer_factory.hpp:77] Creating layer conv1
I1105 12:47:45.323668 23994 net.cpp:91] Creating Layer conv1
I1105 12:47:45.323678 23994 net.cpp:425] conv1 <- img
I1105 12:47:45.323688 23994 net.cpp:399] conv1 -> conv1
I1105 12:47:45.325129 23994 net.cpp:141] Setting up conv1
I1105 12:47:45.325147 23994 net.cpp:148] Top shape: 128 96 60 60 (44236800)
I1105 12:47:45.325157 23994 net.cpp:156] Memory required for data: 208948736
I1105 12:47:45.325172 23994 layer_factory.hpp:77] Creating layer relu1
I1105 12:47:45.325183 23994 net.cpp:91] Creating Layer relu1
I1105 12:47:45.325192 23994 net.cpp:425] relu1 <- conv1
I1105 12:47:45.325201 23994 net.cpp:386] relu1 -> conv1 (in-place)
I1105 12:47:45.325212 23994 net.cpp:141] Setting up relu1
I1105 12:47:45.325222 23994 net.cpp:148] Top shape: 128 96 60 60 (44236800)
I1105 12:47:45.325229 23994 net.cpp:156] Memory required for data: 385895936
I1105 12:47:45.325237 23994 layer_factory.hpp:77] Creating layer pool1
I1105 12:47:45.325250 23994 net.cpp:91] Creating Layer pool1
I1105 12:47:45.325259 23994 net.cpp:425] pool1 <- conv1
I1105 12:47:45.325268 23994 net.cpp:399] pool1 -> pool1
I1105 12:47:45.325314 23994 net.cpp:141] Setting up pool1
I1105 12:47:45.325326 23994 net.cpp:148] Top shape: 128 96 30 30 (11059200)
I1105 12:47:45.325335 23994 net.cpp:156] Memory required for data: 430132736
I1105 12:47:45.325345 23994 layer_factory.hpp:77] Creating layer norm1
I1105 12:47:45.325356 23994 net.cpp:91] Creating Layer norm1
I1105 12:47:45.325363 23994 net.cpp:425] norm1 <- pool1
I1105 12:47:45.325373 23994 net.cpp:399] norm1 -> norm1
I1105 12:47:45.325412 23994 net.cpp:141] Setting up norm1
I1105 12:47:45.325424 23994 net.cpp:148] Top shape: 128 96 30 30 (11059200)
I1105 12:47:45.325433 23994 net.cpp:156] Memory required for data: 474369536
I1105 12:47:45.325440 23994 layer_factory.hpp:77] Creating layer conv2
I1105 12:47:45.325453 23994 net.cpp:91] Creating Layer conv2
I1105 12:47:45.325461 23994 net.cpp:425] conv2 <- norm1
I1105 12:47:45.325471 23994 net.cpp:399] conv2 -> conv2
I1105 12:47:45.328347 23994 net.cpp:141] Setting up conv2
I1105 12:47:45.328366 23994 net.cpp:148] Top shape: 128 256 30 30 (29491200)
I1105 12:47:45.328377 23994 net.cpp:156] Memory required for data: 592334336
I1105 12:47:45.328392 23994 layer_factory.hpp:77] Creating layer relu2
I1105 12:47:45.328402 23994 net.cpp:91] Creating Layer relu2
I1105 12:47:45.328409 23994 net.cpp:425] relu2 <- conv2
I1105 12:47:45.328419 23994 net.cpp:386] relu2 -> conv2 (in-place)
I1105 12:47:45.328431 23994 net.cpp:141] Setting up relu2
I1105 12:47:45.328440 23994 net.cpp:148] Top shape: 128 256 30 30 (29491200)
I1105 12:47:45.328449 23994 net.cpp:156] Memory required for data: 710299136
I1105 12:47:45.328455 23994 layer_factory.hpp:77] Creating layer pool2
I1105 12:47:45.328466 23994 net.cpp:91] Creating Layer pool2
I1105 12:47:45.328474 23994 net.cpp:425] pool2 <- conv2
I1105 12:47:45.328483 23994 net.cpp:399] pool2 -> pool2
I1105 12:47:45.328522 23994 net.cpp:141] Setting up pool2
I1105 12:47:45.328533 23994 net.cpp:148] Top shape: 128 256 15 15 (7372800)
I1105 12:47:45.328541 23994 net.cpp:156] Memory required for data: 739790336
I1105 12:47:45.328548 23994 layer_factory.hpp:77] Creating layer norm2
I1105 12:47:45.328558 23994 net.cpp:91] Creating Layer norm2
I1105 12:47:45.328567 23994 net.cpp:425] norm2 <- pool2
I1105 12:47:45.328578 23994 net.cpp:399] norm2 -> norm2
I1105 12:47:45.328614 23994 net.cpp:141] Setting up norm2
I1105 12:47:45.328626 23994 net.cpp:148] Top shape: 128 256 15 15 (7372800)
I1105 12:47:45.328634 23994 net.cpp:156] Memory required for data: 769281536
I1105 12:47:45.328641 23994 layer_factory.hpp:77] Creating layer conv3
I1105 12:47:45.328655 23994 net.cpp:91] Creating Layer conv3
I1105 12:47:45.328661 23994 net.cpp:425] conv3 <- norm2
I1105 12:47:45.328671 23994 net.cpp:399] conv3 -> conv3
I1105 12:47:45.332141 23994 net.cpp:141] Setting up conv3
I1105 12:47:45.332160 23994 net.cpp:148] Top shape: 128 384 15 15 (11059200)
I1105 12:47:45.332170 23994 net.cpp:156] Memory required for data: 813518336
I1105 12:47:45.332183 23994 layer_factory.hpp:77] Creating layer relu3
I1105 12:47:45.332202 23994 net.cpp:91] Creating Layer relu3
I1105 12:47:45.332209 23994 net.cpp:425] relu3 <- conv3
I1105 12:47:45.332214 23994 net.cpp:386] relu3 -> conv3 (in-place)
I1105 12:47:45.332221 23994 net.cpp:141] Setting up relu3
I1105 12:47:45.332226 23994 net.cpp:148] Top shape: 128 384 15 15 (11059200)
I1105 12:47:45.332227 23994 net.cpp:156] Memory required for data: 857755136
I1105 12:47:45.332231 23994 layer_factory.hpp:77] Creating layer conv4
I1105 12:47:45.332237 23994 net.cpp:91] Creating Layer conv4
I1105 12:47:45.332240 23994 net.cpp:425] conv4 <- conv3
I1105 12:47:45.332244 23994 net.cpp:399] conv4 -> conv4
I1105 12:47:45.334795 23994 net.cpp:141] Setting up conv4
I1105 12:47:45.334810 23994 net.cpp:148] Top shape: 128 256 15 15 (7372800)
I1105 12:47:45.334813 23994 net.cpp:156] Memory required for data: 887246336
I1105 12:47:45.334821 23994 layer_factory.hpp:77] Creating layer relu4
I1105 12:47:45.334828 23994 net.cpp:91] Creating Layer relu4
I1105 12:47:45.334833 23994 net.cpp:425] relu4 <- conv4
I1105 12:47:45.334838 23994 net.cpp:386] relu4 -> conv4 (in-place)
I1105 12:47:45.334846 23994 net.cpp:141] Setting up relu4
I1105 12:47:45.334851 23994 net.cpp:148] Top shape: 128 256 15 15 (7372800)
I1105 12:47:45.334854 23994 net.cpp:156] Memory required for data: 916737536
I1105 12:47:45.334858 23994 layer_factory.hpp:77] Creating layer pool5
I1105 12:47:45.334866 23994 net.cpp:91] Creating Layer pool5
I1105 12:47:45.334868 23994 net.cpp:425] pool5 <- conv4
I1105 12:47:45.334874 23994 net.cpp:399] pool5 -> pool5
I1105 12:47:45.334909 23994 net.cpp:141] Setting up pool5
I1105 12:47:45.334978 23994 net.cpp:148] Top shape: 128 256 7 7 (1605632)
I1105 12:47:45.334991 23994 net.cpp:156] Memory required for data: 923160064
I1105 12:47:45.335001 23994 layer_factory.hpp:77] Creating layer fc6
I1105 12:47:45.335013 23994 net.cpp:91] Creating Layer fc6
I1105 12:47:45.335019 23994 net.cpp:425] fc6 <- pool5
I1105 12:47:45.335026 23994 net.cpp:399] fc6 -> fc6
^CI1105 12:47:45.669773 23994 net.cpp:141] Setting up fc6
I1105 12:47:45.669824 23994 net.cpp:148] Top shape: 128 4096 (524288)
I1105 12:47:45.669836 23994 net.cpp:156] Memory required for data: 925257216
I1105 12:47:45.669862 23994 layer_factory.hpp:77] Creating layer relu6
I1105 12:47:45.669881 23994 net.cpp:91] Creating Layer relu6
I1105 12:47:45.669893 23994 net.cpp:425] relu6 <- fc6
I1105 12:47:45.669908 23994 net.cpp:386] relu6 -> fc6 (in-place)
I1105 12:47:45.669921 23994 net.cpp:141] Setting up relu6
I1105 12:47:45.669931 23994 net.cpp:148] Top shape: 128 4096 (524288)
I1105 12:47:45.669939 23994 net.cpp:156] Memory required for data: 927354368
I1105 12:47:45.669947 23994 layer_factory.hpp:77] Creating layer drop6
I1105 12:47:45.669972 23994 net.cpp:91] Creating Layer drop6
I1105 12:47:45.669981 23994 net.cpp:425] drop6 <- fc6
I1105 12:47:45.669992 23994 net.cpp:386] drop6 -> fc6 (in-place)
I1105 12:47:45.670017 23994 net.cpp:141] Setting up drop6
I1105 12:47:45.670028 23994 net.cpp:148] Top shape: 128 4096 (524288)
I1105 12:47:45.670035 23994 net.cpp:156] Memory required for data: 929451520
I1105 12:47:45.670043 23994 layer_factory.hpp:77] Creating layer fc7
I1105 12:47:45.670058 23994 net.cpp:91] Creating Layer fc7
I1105 12:47:45.670066 23994 net.cpp:425] fc7 <- fc6
I1105 12:47:45.670076 23994 net.cpp:399] fc7 -> fc7
I1105 12:47:45.770485 23994 net.cpp:141] Setting up fc7
I1105 12:47:45.770517 23994 net.cpp:148] Top shape: 128 4096 (524288)
I1105 12:47:45.770522 23994 net.cpp:156] Memory required for data: 931548672
I1105 12:47:45.770534 23994 layer_factory.hpp:77] Creating layer relu7
I1105 12:47:45.770545 23994 net.cpp:91] Creating Layer relu7
I1105 12:47:45.770550 23994 net.cpp:425] relu7 <- fc7
I1105 12:47:45.770557 23994 net.cpp:386] relu7 -> fc7 (in-place)
I1105 12:47:45.770565 23994 net.cpp:141] Setting up relu7
I1105 12:47:45.770571 23994 net.cpp:148] Top shape: 128 4096 (524288)
I1105 12:47:45.770576 23994 net.cpp:156] Memory required for data: 933645824
I1105 12:47:45.770579 23994 layer_factory.hpp:77] Creating layer drop7
I1105 12:47:45.770586 23994 net.cpp:91] Creating Layer drop7
I1105 12:47:45.770591 23994 net.cpp:425] drop7 <- fc7
I1105 12:47:45.770596 23994 net.cpp:386] drop7 -> fc7 (in-place)
I1105 12:47:45.770611 23994 net.cpp:141] Setting up drop7
I1105 12:47:45.770617 23994 net.cpp:148] Top shape: 128 4096 (524288)
I1105 12:47:45.770620 23994 net.cpp:156] Memory required for data: 935742976
I1105 12:47:45.770625 23994 layer_factory.hpp:77] Creating layer fc8
I1105 12:47:45.770633 23994 net.cpp:91] Creating Layer fc8
I1105 12:47:45.770637 23994 net.cpp:425] fc8 <- fc7
I1105 12:47:45.770643 23994 net.cpp:399] fc8 -> fc8
I1105 12:47:45.771812 23994 net.cpp:141] Setting up fc8
I1105 12:47:45.771826 23994 net.cpp:148] Top shape: 128 40 (5120)
I1105 12:47:45.771833 23994 net.cpp:156] Memory required for data: 935763456
I1105 12:47:45.771841 23994 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I1105 12:47:45.771848 23994 net.cpp:91] Creating Layer fc8_fc8_0_split
I1105 12:47:45.771853 23994 net.cpp:425] fc8_fc8_0_split <- fc8
I1105 12:47:45.771859 23994 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1105 12:47:45.771868 23994 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1105 12:47:45.771890 23994 net.cpp:141] Setting up fc8_fc8_0_split
I1105 12:47:45.771898 23994 net.cpp:148] Top shape: 128 40 (5120)
I1105 12:47:45.771904 23994 net.cpp:148] Top shape: 128 40 (5120)
I1105 12:47:45.771910 23994 net.cpp:156] Memory required for data: 935804416
I1105 12:47:45.771915 23994 layer_factory.hpp:77] Creating layer accuracy
I1105 12:47:45.771922 23994 net.cpp:91] Creating Layer accuracy
I1105 12:47:45.771929 23994 net.cpp:425] accuracy <- fc8_fc8_0_split_0
I1105 12:47:45.771934 23994 net.cpp:425] accuracy <- label_img_1_split_0
I1105 12:47:45.771941 23994 net.cpp:399] accuracy -> accuracy
I1105 12:47:45.771953 23994 net.cpp:141] Setting up accuracy
I1105 12:47:45.771960 23994 net.cpp:148] Top shape: (1)
I1105 12:47:45.771965 23994 net.cpp:156] Memory required for data: 935804420
I1105 12:47:45.771970 23994 layer_factory.hpp:77] Creating layer loss
I1105 12:47:45.771975 23994 net.cpp:91] Creating Layer loss
I1105 12:47:45.771981 23994 net.cpp:425] loss <- fc8_fc8_0_split_1
I1105 12:47:45.771986 23994 net.cpp:425] loss <- label_img_1_split_1
I1105 12:47:45.771993 23994 net.cpp:399] loss -> loss
I1105 12:47:45.772001 23994 layer_factory.hpp:77] Creating layer loss
I1105 12:47:45.772084 23994 net.cpp:141] Setting up loss
I1105 12:47:45.772096 23994 net.cpp:148] Top shape: (1)
I1105 12:47:45.772102 23994 net.cpp:151]     with loss weight 1
I1105 12:47:45.772114 23994 net.cpp:156] Memory required for data: 935804424
I1105 12:47:45.772119 23994 net.cpp:217] loss needs backward computation.
I1105 12:47:45.772124 23994 net.cpp:219] accuracy does not need backward computation.
I1105 12:47:45.772128 23994 net.cpp:217] fc8_fc8_0_split needs backward computation.
I1105 12:47:45.772133 23994 net.cpp:217] fc8 needs backward computation.
I1105 12:47:45.772137 23994 net.cpp:217] drop7 needs backward computation.
I1105 12:47:45.772145 23994 net.cpp:217] relu7 needs backward computation.
I1105 12:47:45.772155 23994 net.cpp:217] fc7 needs backward computation.
I1105 12:47:45.772163 23994 net.cpp:217] drop6 needs backward computation.
I1105 12:47:45.772171 23994 net.cpp:217] relu6 needs backward computation.
I1105 12:47:45.772176 23994 net.cpp:217] fc6 needs backward computation.
I1105 12:47:45.772181 23994 net.cpp:217] pool5 needs backward computation.
I1105 12:47:45.772186 23994 net.cpp:217] relu4 needs backward computation.
I1105 12:47:45.772192 23994 net.cpp:217] conv4 needs backward computation.
I1105 12:47:45.772197 23994 net.cpp:217] relu3 needs backward computation.
I1105 12:47:45.772202 23994 net.cpp:217] conv3 needs backward computation.
I1105 12:47:45.772207 23994 net.cpp:217] norm2 needs backward computation.
I1105 12:47:45.772212 23994 net.cpp:217] pool2 needs backward computation.
I1105 12:47:45.772218 23994 net.cpp:217] relu2 needs backward computation.
I1105 12:47:45.772223 23994 net.cpp:217] conv2 needs backward computation.
I1105 12:47:45.772229 23994 net.cpp:217] norm1 needs backward computation.
I1105 12:47:45.772234 23994 net.cpp:217] pool1 needs backward computation.
I1105 12:47:45.772239 23994 net.cpp:217] relu1 needs backward computation.
I1105 12:47:45.772244 23994 net.cpp:217] conv1 needs backward computation.
I1105 12:47:45.772250 23994 net.cpp:219] label_img_1_split does not need backward computation.
I1105 12:47:45.772256 23994 net.cpp:219] img does not need backward computation.
I1105 12:47:45.772261 23994 net.cpp:261] This network produces output accuracy
I1105 12:47:45.772267 23994 net.cpp:261] This network produces output loss
I1105 12:47:45.772281 23994 net.cpp:274] Network initialization done.
I1105 12:47:45.772614 23994 solver.cpp:181] Creating test net (#0) specified by test_net file: test.prototxt
I1105 12:47:45.772734 23994 net.cpp:49] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "img"
  type: "Python"
  top: "img"
  top: "label"
  python_param {
    module: "data_layers.model_net_layer"
    layer: "ModelNetDataLayer"
    param_str: "{\'img_size\': (250, 250), \'dtype\': \'object\', \'batch_size\': 128, \'seed\': 1337, \'split\': \'test\', \'dataset_dir\': \'/home/kevin/dataset/processed_data\', \'mean\': 2}"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "img"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 0
    kernel_size: 11
    group: 1
    stride: 4
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    name: "conv3_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv3_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    name: "conv4_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv4_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv4"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 40
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1105 12:47:45.773365 23994 layer_factory.hpp:77] Creating layer img
I1105 12:47:45.773406 23994 net.cpp:91] Creating Layer img
I1105 12:47:45.773413 23994 net.cpp:399] img -> img
I1105 12:47:45.773422 23994 net.cpp:399] img -> label
Traceback (most recent call last):
  File "./solve.py", line 20, in <module>
    solver = caffe.SGDSolver('solver.prototxt')
  File "/home/kevin/caffeplus/python_layer/data_layers/model_net_layer.py", line 14, in setup
    def setup(self, bottom, top):
KeyboardInterrupt
]0;kevin@kevin-desktop: ~/catkin_ws/src/romans_stack/model_net/caffe_netkevin@kevin-desktop:~/catkin_ws/src/romans_stack/model_net/caffe_net$ ./solve.py
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Net<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Blob<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Solver<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1105 12:47:48.649672 24062 solver.cpp:48] Initializing solver from parameters: 
train_net: "train.prototxt"
test_net: "test.prototxt"
test_iter: 0
test_interval: 9999999
base_lr: 0.01
display: 100
max_iter: 450000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 10000
snapshot: 1000
snapshot_prefix: "/home/kevin/snapshot"
solver_mode: GPU
I1105 12:47:48.649819 24062 solver.cpp:81] Creating training net from train_net file: train.prototxt
I1105 12:47:48.650207 24062 net.cpp:49] Initializing net from parameters: 
state {
  phase: TRAIN
}
layer {
  name: "img"
  type: "Python"
  top: "img"
  top: "label"
  python_param {
    module: "data_layers.model_net_layer"
    layer: "ModelNetDataLayer"
    param_str: "{\'img_size\': (250, 250), \'dtype\': \'frame\', \'batch_size\': 128, \'seed\': 1337, \'split\': \'train\', \'dataset_dir\': \'/home/kevin/dataset/processed_data\', \'mean\': 2}"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "img"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 0
    kernel_size: 11
    group: 1
    stride: 4
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    name: "conv3_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv3_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    name: "conv4_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv4_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv4"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 40
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1105 12:47:48.651131 24062 layer_factory.hpp:77] Creating layer img
I1105 12:47:48.672945 24062 net.cpp:91] Creating Layer img
I1105 12:47:48.672979 24062 net.cpp:399] img -> img
I1105 12:47:48.672996 24062 net.cpp:399] img -> label
{'img_size': (250, 250), 'dtype': 'frame', 'batch_size': 128, 'seed': 1337, 'split': 'train', 'dataset_dir': '/home/kevin/dataset/processed_data', 'mean': 2}
I1105 12:48:10.222316 24062 net.cpp:141] Setting up img
I1105 12:48:10.222347 24062 net.cpp:148] Top shape: 128 1 250 250 (8000000)
I1105 12:48:10.222353 24062 net.cpp:148] Top shape: 128 1 (128)
I1105 12:48:10.222359 24062 net.cpp:156] Memory required for data: 32000512
I1105 12:48:10.222367 24062 layer_factory.hpp:77] Creating layer label_img_1_split
I1105 12:48:10.222383 24062 net.cpp:91] Creating Layer label_img_1_split
I1105 12:48:10.222389 24062 net.cpp:425] label_img_1_split <- label
I1105 12:48:10.222396 24062 net.cpp:399] label_img_1_split -> label_img_1_split_0
I1105 12:48:10.222404 24062 net.cpp:399] label_img_1_split -> label_img_1_split_1
I1105 12:48:10.222431 24062 net.cpp:141] Setting up label_img_1_split
I1105 12:48:10.222440 24062 net.cpp:148] Top shape: 128 1 (128)
I1105 12:48:10.222445 24062 net.cpp:148] Top shape: 128 1 (128)
I1105 12:48:10.222450 24062 net.cpp:156] Memory required for data: 32001536
I1105 12:48:10.222453 24062 layer_factory.hpp:77] Creating layer conv1
I1105 12:48:10.222465 24062 net.cpp:91] Creating Layer conv1
I1105 12:48:10.222470 24062 net.cpp:425] conv1 <- img
I1105 12:48:10.222476 24062 net.cpp:399] conv1 -> conv1
I1105 12:48:10.223433 24062 net.cpp:141] Setting up conv1
I1105 12:48:10.223445 24062 net.cpp:148] Top shape: 128 96 60 60 (44236800)
I1105 12:48:10.223451 24062 net.cpp:156] Memory required for data: 208948736
I1105 12:48:10.223460 24062 layer_factory.hpp:77] Creating layer relu1
I1105 12:48:10.223469 24062 net.cpp:91] Creating Layer relu1
I1105 12:48:10.223472 24062 net.cpp:425] relu1 <- conv1
I1105 12:48:10.223477 24062 net.cpp:386] relu1 -> conv1 (in-place)
I1105 12:48:10.223492 24062 net.cpp:141] Setting up relu1
I1105 12:48:10.223500 24062 net.cpp:148] Top shape: 128 96 60 60 (44236800)
I1105 12:48:10.223505 24062 net.cpp:156] Memory required for data: 385895936
I1105 12:48:10.223508 24062 layer_factory.hpp:77] Creating layer pool1
I1105 12:48:10.223516 24062 net.cpp:91] Creating Layer pool1
I1105 12:48:10.223521 24062 net.cpp:425] pool1 <- conv1
I1105 12:48:10.223527 24062 net.cpp:399] pool1 -> pool1
I1105 12:48:10.223557 24062 net.cpp:141] Setting up pool1
I1105 12:48:10.223563 24062 net.cpp:148] Top shape: 128 96 30 30 (11059200)
I1105 12:48:10.223568 24062 net.cpp:156] Memory required for data: 430132736
I1105 12:48:10.223572 24062 layer_factory.hpp:77] Creating layer norm1
I1105 12:48:10.223580 24062 net.cpp:91] Creating Layer norm1
I1105 12:48:10.223585 24062 net.cpp:425] norm1 <- pool1
I1105 12:48:10.223592 24062 net.cpp:399] norm1 -> norm1
I1105 12:48:10.223614 24062 net.cpp:141] Setting up norm1
I1105 12:48:10.223620 24062 net.cpp:148] Top shape: 128 96 30 30 (11059200)
I1105 12:48:10.223624 24062 net.cpp:156] Memory required for data: 474369536
I1105 12:48:10.223628 24062 layer_factory.hpp:77] Creating layer conv2
I1105 12:48:10.223636 24062 net.cpp:91] Creating Layer conv2
I1105 12:48:10.223640 24062 net.cpp:425] conv2 <- norm1
I1105 12:48:10.223645 24062 net.cpp:399] conv2 -> conv2
I1105 12:48:10.225591 24062 net.cpp:141] Setting up conv2
I1105 12:48:10.225603 24062 net.cpp:148] Top shape: 128 256 30 30 (29491200)
I1105 12:48:10.225610 24062 net.cpp:156] Memory required for data: 592334336
I1105 12:48:10.225617 24062 layer_factory.hpp:77] Creating layer relu2
I1105 12:48:10.225625 24062 net.cpp:91] Creating Layer relu2
I1105 12:48:10.225628 24062 net.cpp:425] relu2 <- conv2
I1105 12:48:10.225633 24062 net.cpp:386] relu2 -> conv2 (in-place)
I1105 12:48:10.225639 24062 net.cpp:141] Setting up relu2
I1105 12:48:10.225646 24062 net.cpp:148] Top shape: 128 256 30 30 (29491200)
I1105 12:48:10.225649 24062 net.cpp:156] Memory required for data: 710299136
I1105 12:48:10.225656 24062 layer_factory.hpp:77] Creating layer pool2
I1105 12:48:10.225661 24062 net.cpp:91] Creating Layer pool2
I1105 12:48:10.225666 24062 net.cpp:425] pool2 <- conv2
I1105 12:48:10.225672 24062 net.cpp:399] pool2 -> pool2
I1105 12:48:10.225699 24062 net.cpp:141] Setting up pool2
I1105 12:48:10.225708 24062 net.cpp:148] Top shape: 128 256 15 15 (7372800)
I1105 12:48:10.225713 24062 net.cpp:156] Memory required for data: 739790336
I1105 12:48:10.225716 24062 layer_factory.hpp:77] Creating layer norm2
I1105 12:48:10.225723 24062 net.cpp:91] Creating Layer norm2
I1105 12:48:10.225728 24062 net.cpp:425] norm2 <- pool2
I1105 12:48:10.225733 24062 net.cpp:399] norm2 -> norm2
I1105 12:48:10.225754 24062 net.cpp:141] Setting up norm2
I1105 12:48:10.225761 24062 net.cpp:148] Top shape: 128 256 15 15 (7372800)
I1105 12:48:10.225765 24062 net.cpp:156] Memory required for data: 769281536
I1105 12:48:10.225770 24062 layer_factory.hpp:77] Creating layer conv3
I1105 12:48:10.225777 24062 net.cpp:91] Creating Layer conv3
I1105 12:48:10.225782 24062 net.cpp:425] conv3 <- norm2
I1105 12:48:10.225790 24062 net.cpp:399] conv3 -> conv3
I1105 12:48:10.228138 24062 net.cpp:141] Setting up conv3
I1105 12:48:10.228152 24062 net.cpp:148] Top shape: 128 384 15 15 (11059200)
I1105 12:48:10.228157 24062 net.cpp:156] Memory required for data: 813518336
I1105 12:48:10.228164 24062 layer_factory.hpp:77] Creating layer relu3
I1105 12:48:10.228173 24062 net.cpp:91] Creating Layer relu3
I1105 12:48:10.228178 24062 net.cpp:425] relu3 <- conv3
I1105 12:48:10.228183 24062 net.cpp:386] relu3 -> conv3 (in-place)
I1105 12:48:10.228190 24062 net.cpp:141] Setting up relu3
I1105 12:48:10.228195 24062 net.cpp:148] Top shape: 128 384 15 15 (11059200)
I1105 12:48:10.228200 24062 net.cpp:156] Memory required for data: 857755136
I1105 12:48:10.228204 24062 layer_factory.hpp:77] Creating layer conv4
I1105 12:48:10.228214 24062 net.cpp:91] Creating Layer conv4
I1105 12:48:10.228219 24062 net.cpp:425] conv4 <- conv3
I1105 12:48:10.228224 24062 net.cpp:399] conv4 -> conv4
I1105 12:48:10.230631 24062 net.cpp:141] Setting up conv4
I1105 12:48:10.230643 24062 net.cpp:148] Top shape: 128 256 15 15 (7372800)
I1105 12:48:10.230649 24062 net.cpp:156] Memory required for data: 887246336
I1105 12:48:10.230655 24062 layer_factory.hpp:77] Creating layer relu4
I1105 12:48:10.230662 24062 net.cpp:91] Creating Layer relu4
I1105 12:48:10.230667 24062 net.cpp:425] relu4 <- conv4
I1105 12:48:10.230674 24062 net.cpp:386] relu4 -> conv4 (in-place)
I1105 12:48:10.230679 24062 net.cpp:141] Setting up relu4
I1105 12:48:10.230684 24062 net.cpp:148] Top shape: 128 256 15 15 (7372800)
I1105 12:48:10.230687 24062 net.cpp:156] Memory required for data: 916737536
I1105 12:48:10.230691 24062 layer_factory.hpp:77] Creating layer pool5
I1105 12:48:10.230697 24062 net.cpp:91] Creating Layer pool5
I1105 12:48:10.230702 24062 net.cpp:425] pool5 <- conv4
I1105 12:48:10.230708 24062 net.cpp:399] pool5 -> pool5
I1105 12:48:10.230733 24062 net.cpp:141] Setting up pool5
I1105 12:48:10.230741 24062 net.cpp:148] Top shape: 128 256 7 7 (1605632)
I1105 12:48:10.230746 24062 net.cpp:156] Memory required for data: 923160064
I1105 12:48:10.230751 24062 layer_factory.hpp:77] Creating layer fc6
I1105 12:48:10.230763 24062 net.cpp:91] Creating Layer fc6
I1105 12:48:10.230768 24062 net.cpp:425] fc6 <- pool5
I1105 12:48:10.230773 24062 net.cpp:399] fc6 -> fc6
I1105 12:48:10.544280 24062 net.cpp:141] Setting up fc6
I1105 12:48:10.544320 24062 net.cpp:148] Top shape: 128 4096 (524288)
I1105 12:48:10.544332 24062 net.cpp:156] Memory required for data: 925257216
I1105 12:48:10.544353 24062 layer_factory.hpp:77] Creating layer relu6
I1105 12:48:10.544369 24062 net.cpp:91] Creating Layer relu6
I1105 12:48:10.544378 24062 net.cpp:425] relu6 <- fc6
I1105 12:48:10.544389 24062 net.cpp:386] relu6 -> fc6 (in-place)
I1105 12:48:10.544404 24062 net.cpp:141] Setting up relu6
I1105 12:48:10.544414 24062 net.cpp:148] Top shape: 128 4096 (524288)
I1105 12:48:10.544420 24062 net.cpp:156] Memory required for data: 927354368
I1105 12:48:10.544427 24062 layer_factory.hpp:77] Creating layer drop6
I1105 12:48:10.544448 24062 net.cpp:91] Creating Layer drop6
I1105 12:48:10.544456 24062 net.cpp:425] drop6 <- fc6
I1105 12:48:10.544466 24062 net.cpp:386] drop6 -> fc6 (in-place)
I1105 12:48:10.544488 24062 net.cpp:141] Setting up drop6
I1105 12:48:10.544500 24062 net.cpp:148] Top shape: 128 4096 (524288)
I1105 12:48:10.544507 24062 net.cpp:156] Memory required for data: 929451520
I1105 12:48:10.544514 24062 layer_factory.hpp:77] Creating layer fc7
I1105 12:48:10.544526 24062 net.cpp:91] Creating Layer fc7
I1105 12:48:10.544535 24062 net.cpp:425] fc7 <- fc6
I1105 12:48:10.544544 24062 net.cpp:399] fc7 -> fc7
I1105 12:48:10.673307 24062 net.cpp:141] Setting up fc7
I1105 12:48:10.673348 24062 net.cpp:148] Top shape: 128 4096 (524288)
I1105 12:48:10.673358 24062 net.cpp:156] Memory required for data: 931548672
I1105 12:48:10.673374 24062 layer_factory.hpp:77] Creating layer relu7
I1105 12:48:10.673389 24062 net.cpp:91] Creating Layer relu7
I1105 12:48:10.673399 24062 net.cpp:425] relu7 <- fc7
I1105 12:48:10.673410 24062 net.cpp:386] relu7 -> fc7 (in-place)
I1105 12:48:10.673424 24062 net.cpp:141] Setting up relu7
I1105 12:48:10.673432 24062 net.cpp:148] Top shape: 128 4096 (524288)
I1105 12:48:10.673440 24062 net.cpp:156] Memory required for data: 933645824
I1105 12:48:10.673449 24062 layer_factory.hpp:77] Creating layer drop7
I1105 12:48:10.673460 24062 net.cpp:91] Creating Layer drop7
I1105 12:48:10.673467 24062 net.cpp:425] drop7 <- fc7
I1105 12:48:10.673476 24062 net.cpp:386] drop7 -> fc7 (in-place)
I1105 12:48:10.673498 24062 net.cpp:141] Setting up drop7
I1105 12:48:10.673508 24062 net.cpp:148] Top shape: 128 4096 (524288)
I1105 12:48:10.673516 24062 net.cpp:156] Memory required for data: 935742976
I1105 12:48:10.673523 24062 layer_factory.hpp:77] Creating layer fc8
I1105 12:48:10.673537 24062 net.cpp:91] Creating Layer fc8
I1105 12:48:10.673543 24062 net.cpp:425] fc8 <- fc7
I1105 12:48:10.673554 24062 net.cpp:399] fc8 -> fc8
I1105 12:48:10.675086 24062 net.cpp:141] Setting up fc8
I1105 12:48:10.675103 24062 net.cpp:148] Top shape: 128 40 (5120)
I1105 12:48:10.675113 24062 net.cpp:156] Memory required for data: 935763456
I1105 12:48:10.675125 24062 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I1105 12:48:10.675137 24062 net.cpp:91] Creating Layer fc8_fc8_0_split
I1105 12:48:10.675145 24062 net.cpp:425] fc8_fc8_0_split <- fc8
I1105 12:48:10.675155 24062 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1105 12:48:10.675169 24062 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1105 12:48:10.675201 24062 net.cpp:141] Setting up fc8_fc8_0_split
I1105 12:48:10.675212 24062 net.cpp:148] Top shape: 128 40 (5120)
I1105 12:48:10.675221 24062 net.cpp:148] Top shape: 128 40 (5120)
I1105 12:48:10.675230 24062 net.cpp:156] Memory required for data: 935804416
I1105 12:48:10.675236 24062 layer_factory.hpp:77] Creating layer accuracy
I1105 12:48:10.675253 24062 net.cpp:91] Creating Layer accuracy
I1105 12:48:10.675261 24062 net.cpp:425] accuracy <- fc8_fc8_0_split_0
I1105 12:48:10.675271 24062 net.cpp:425] accuracy <- label_img_1_split_0
I1105 12:48:10.675278 24062 net.cpp:399] accuracy -> accuracy
I1105 12:48:10.675290 24062 net.cpp:141] Setting up accuracy
I1105 12:48:10.675299 24062 net.cpp:148] Top shape: (1)
I1105 12:48:10.675308 24062 net.cpp:156] Memory required for data: 935804420
I1105 12:48:10.675317 24062 layer_factory.hpp:77] Creating layer loss
I1105 12:48:10.675328 24062 net.cpp:91] Creating Layer loss
I1105 12:48:10.675336 24062 net.cpp:425] loss <- fc8_fc8_0_split_1
I1105 12:48:10.675346 24062 net.cpp:425] loss <- label_img_1_split_1
I1105 12:48:10.675355 24062 net.cpp:399] loss -> loss
I1105 12:48:10.675366 24062 layer_factory.hpp:77] Creating layer loss
I1105 12:48:10.675459 24062 net.cpp:141] Setting up loss
I1105 12:48:10.675470 24062 net.cpp:148] Top shape: (1)
I1105 12:48:10.675479 24062 net.cpp:151]     with loss weight 1
I1105 12:48:10.675498 24062 net.cpp:156] Memory required for data: 935804424
I1105 12:48:10.675508 24062 net.cpp:217] loss needs backward computation.
I1105 12:48:10.675515 24062 net.cpp:219] accuracy does not need backward computation.
I1105 12:48:10.675524 24062 net.cpp:217] fc8_fc8_0_split needs backward computation.
I1105 12:48:10.675531 24062 net.cpp:217] fc8 needs backward computation.
I1105 12:48:10.675539 24062 net.cpp:217] drop7 needs backward computation.
I1105 12:48:10.675546 24062 net.cpp:217] relu7 needs backward computation.
I1105 12:48:10.675554 24062 net.cpp:217] fc7 needs backward computation.
I1105 12:48:10.675561 24062 net.cpp:217] drop6 needs backward computation.
I1105 12:48:10.675568 24062 net.cpp:217] relu6 needs backward computation.
I1105 12:48:10.675575 24062 net.cpp:217] fc6 needs backward computation.
I1105 12:48:10.675583 24062 net.cpp:217] pool5 needs backward computation.
I1105 12:48:10.675591 24062 net.cpp:217] relu4 needs backward computation.
I1105 12:48:10.675598 24062 net.cpp:217] conv4 needs backward computation.
I1105 12:48:10.675606 24062 net.cpp:217] relu3 needs backward computation.
I1105 12:48:10.675613 24062 net.cpp:217] conv3 needs backward computation.
I1105 12:48:10.675621 24062 net.cpp:217] norm2 needs backward computation.
I1105 12:48:10.675629 24062 net.cpp:217] pool2 needs backward computation.
I1105 12:48:10.675637 24062 net.cpp:217] relu2 needs backward computation.
I1105 12:48:10.675644 24062 net.cpp:217] conv2 needs backward computation.
I1105 12:48:10.675652 24062 net.cpp:217] norm1 needs backward computation.
I1105 12:48:10.675659 24062 net.cpp:217] pool1 needs backward computation.
I1105 12:48:10.675668 24062 net.cpp:217] relu1 needs backward computation.
I1105 12:48:10.675674 24062 net.cpp:217] conv1 needs backward computation.
I1105 12:48:10.675683 24062 net.cpp:219] label_img_1_split does not need backward computation.
I1105 12:48:10.675691 24062 net.cpp:219] img does not need backward computation.
I1105 12:48:10.675698 24062 net.cpp:261] This network produces output accuracy
I1105 12:48:10.675706 24062 net.cpp:261] This network produces output loss
I1105 12:48:10.675725 24062 net.cpp:274] Network initialization done.
I1105 12:48:10.676260 24062 solver.cpp:181] Creating test net (#0) specified by test_net file: test.prototxt
I1105 12:48:10.676458 24062 net.cpp:49] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "img"
  type: "Python"
  top: "img"
  top: "label"
  python_param {
    module: "data_layers.model_net_layer"
    layer: "ModelNetDataLayer"
    param_str: "{\'img_size\': (250, 250), \'dtype\': \'object\', \'batch_size\': 128, \'seed\': 1337, \'split\': \'test\', \'dataset_dir\': \'/home/kevin/dataset/processed_data\', \'mean\': 2}"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "img"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 0
    kernel_size: 11
    group: 1
    stride: 4
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    name: "conv3_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv3_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    name: "conv4_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv4_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv4"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 40
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1105 12:48:10.677659 24062 layer_factory.hpp:77] Creating layer img
I1105 12:48:10.677708 24062 net.cpp:91] Creating Layer img
I1105 12:48:10.677721 24062 net.cpp:399] img -> img
I1105 12:48:10.677731 24062 net.cpp:399] img -> label
{'img_size': (250, 250), 'dtype': 'object', 'batch_size': 128, 'seed': 1337, 'split': 'test', 'dataset_dir': '/home/kevin/dataset/processed_data', 'mean': 2}
I1105 12:48:10.839237 24062 net.cpp:141] Setting up img
I1105 12:48:10.839289 24062 net.cpp:148] Top shape: 24 1 250 250 (1500000)
I1105 12:48:10.839318 24062 net.cpp:148] Top shape: 24 1 (24)
I1105 12:48:10.839325 24062 net.cpp:156] Memory required for data: 6000096
I1105 12:48:10.839354 24062 layer_factory.hpp:77] Creating layer label_img_1_split
I1105 12:48:10.839375 24062 net.cpp:91] Creating Layer label_img_1_split
I1105 12:48:10.839385 24062 net.cpp:425] label_img_1_split <- label
I1105 12:48:10.839398 24062 net.cpp:399] label_img_1_split -> label_img_1_split_0
I1105 12:48:10.839426 24062 net.cpp:399] label_img_1_split -> label_img_1_split_1
I1105 12:48:10.839498 24062 net.cpp:141] Setting up label_img_1_split
I1105 12:48:10.839520 24062 net.cpp:148] Top shape: 24 1 (24)
I1105 12:48:10.839546 24062 net.cpp:148] Top shape: 24 1 (24)
I1105 12:48:10.839568 24062 net.cpp:156] Memory required for data: 6000288
I1105 12:48:10.839586 24062 layer_factory.hpp:77] Creating layer conv1
I1105 12:48:10.839627 24062 net.cpp:91] Creating Layer conv1
I1105 12:48:10.839637 24062 net.cpp:425] conv1 <- img
I1105 12:48:10.839655 24062 net.cpp:399] conv1 -> conv1
I1105 12:48:10.839975 24062 net.cpp:141] Setting up conv1
I1105 12:48:10.839989 24062 net.cpp:148] Top shape: 24 96 60 60 (8294400)
I1105 12:48:10.840008 24062 net.cpp:156] Memory required for data: 39177888
I1105 12:48:10.840021 24062 layer_factory.hpp:77] Creating layer relu1
I1105 12:48:10.840034 24062 net.cpp:91] Creating Layer relu1
I1105 12:48:10.840042 24062 net.cpp:425] relu1 <- conv1
I1105 12:48:10.840051 24062 net.cpp:386] relu1 -> conv1 (in-place)
I1105 12:48:10.840062 24062 net.cpp:141] Setting up relu1
I1105 12:48:10.840072 24062 net.cpp:148] Top shape: 24 96 60 60 (8294400)
I1105 12:48:10.840080 24062 net.cpp:156] Memory required for data: 72355488
I1105 12:48:10.840092 24062 layer_factory.hpp:77] Creating layer pool1
I1105 12:48:10.840109 24062 net.cpp:91] Creating Layer pool1
I1105 12:48:10.840118 24062 net.cpp:425] pool1 <- conv1
I1105 12:48:10.840128 24062 net.cpp:399] pool1 -> pool1
I1105 12:48:10.840180 24062 net.cpp:141] Setting up pool1
I1105 12:48:10.840193 24062 net.cpp:148] Top shape: 24 96 30 30 (2073600)
I1105 12:48:10.840210 24062 net.cpp:156] Memory required for data: 80649888
I1105 12:48:10.840219 24062 layer_factory.hpp:77] Creating layer norm1
I1105 12:48:10.840230 24062 net.cpp:91] Creating Layer norm1
I1105 12:48:10.840239 24062 net.cpp:425] norm1 <- pool1
I1105 12:48:10.840250 24062 net.cpp:399] norm1 -> norm1
I1105 12:48:10.840306 24062 net.cpp:141] Setting up norm1
I1105 12:48:10.840318 24062 net.cpp:148] Top shape: 24 96 30 30 (2073600)
I1105 12:48:10.840335 24062 net.cpp:156] Memory required for data: 88944288
I1105 12:48:10.840342 24062 layer_factory.hpp:77] Creating layer conv2
I1105 12:48:10.840355 24062 net.cpp:91] Creating Layer conv2
I1105 12:48:10.840364 24062 net.cpp:425] conv2 <- norm1
I1105 12:48:10.840375 24062 net.cpp:399] conv2 -> conv2
I1105 12:48:10.842949 24062 net.cpp:141] Setting up conv2
I1105 12:48:10.843003 24062 net.cpp:148] Top shape: 24 256 30 30 (5529600)
I1105 12:48:10.843016 24062 net.cpp:156] Memory required for data: 111062688
I1105 12:48:10.843039 24062 layer_factory.hpp:77] Creating layer relu2
I1105 12:48:10.843055 24062 net.cpp:91] Creating Layer relu2
I1105 12:48:10.843073 24062 net.cpp:425] relu2 <- conv2
I1105 12:48:10.843094 24062 net.cpp:386] relu2 -> conv2 (in-place)
I1105 12:48:10.843108 24062 net.cpp:141] Setting up relu2
I1105 12:48:10.843118 24062 net.cpp:148] Top shape: 24 256 30 30 (5529600)
I1105 12:48:10.843129 24062 net.cpp:156] Memory required for data: 133181088
I1105 12:48:10.843140 24062 layer_factory.hpp:77] Creating layer pool2
I1105 12:48:10.843158 24062 net.cpp:91] Creating Layer pool2
I1105 12:48:10.843165 24062 net.cpp:425] pool2 <- conv2
I1105 12:48:10.843183 24062 net.cpp:399] pool2 -> pool2
I1105 12:48:10.843224 24062 net.cpp:141] Setting up pool2
I1105 12:48:10.843235 24062 net.cpp:148] Top shape: 24 256 15 15 (1382400)
I1105 12:48:10.843242 24062 net.cpp:156] Memory required for data: 138710688
I1105 12:48:10.843250 24062 layer_factory.hpp:77] Creating layer norm2
I1105 12:48:10.843261 24062 net.cpp:91] Creating Layer norm2
I1105 12:48:10.843267 24062 net.cpp:425] norm2 <- pool2
I1105 12:48:10.843276 24062 net.cpp:399] norm2 -> norm2
I1105 12:48:10.843308 24062 net.cpp:141] Setting up norm2
I1105 12:48:10.843319 24062 net.cpp:148] Top shape: 24 256 15 15 (1382400)
I1105 12:48:10.843327 24062 net.cpp:156] Memory required for data: 144240288
I1105 12:48:10.843333 24062 layer_factory.hpp:77] Creating layer conv3
I1105 12:48:10.843346 24062 net.cpp:91] Creating Layer conv3
I1105 12:48:10.843354 24062 net.cpp:425] conv3 <- norm2
I1105 12:48:10.843364 24062 net.cpp:399] conv3 -> conv3
I1105 12:48:10.846753 24062 net.cpp:141] Setting up conv3
I1105 12:48:10.846820 24062 net.cpp:148] Top shape: 24 384 15 15 (2073600)
I1105 12:48:10.846843 24062 net.cpp:156] Memory required for data: 152534688
I1105 12:48:10.846869 24062 layer_factory.hpp:77] Creating layer relu3
I1105 12:48:10.846889 24062 net.cpp:91] Creating Layer relu3
I1105 12:48:10.846899 24062 net.cpp:425] relu3 <- conv3
I1105 12:48:10.846910 24062 net.cpp:386] relu3 -> conv3 (in-place)
I1105 12:48:10.846925 24062 net.cpp:141] Setting up relu3
I1105 12:48:10.846933 24062 net.cpp:148] Top shape: 24 384 15 15 (2073600)
I1105 12:48:10.846940 24062 net.cpp:156] Memory required for data: 160829088
I1105 12:48:10.846947 24062 layer_factory.hpp:77] Creating layer conv4
I1105 12:48:10.846963 24062 net.cpp:91] Creating Layer conv4
I1105 12:48:10.846971 24062 net.cpp:425] conv4 <- conv3
I1105 12:48:10.846990 24062 net.cpp:399] conv4 -> conv4
I1105 12:48:10.850450 24062 net.cpp:141] Setting up conv4
I1105 12:48:10.850499 24062 net.cpp:148] Top shape: 24 256 15 15 (1382400)
I1105 12:48:10.850508 24062 net.cpp:156] Memory required for data: 166358688
I1105 12:48:10.850523 24062 layer_factory.hpp:77] Creating layer relu4
I1105 12:48:10.850536 24062 net.cpp:91] Creating Layer relu4
I1105 12:48:10.850543 24062 net.cpp:425] relu4 <- conv4
I1105 12:48:10.850553 24062 net.cpp:386] relu4 -> conv4 (in-place)
I1105 12:48:10.850564 24062 net.cpp:141] Setting up relu4
I1105 12:48:10.850571 24062 net.cpp:148] Top shape: 24 256 15 15 (1382400)
I1105 12:48:10.850577 24062 net.cpp:156] Memory required for data: 171888288
I1105 12:48:10.850584 24062 layer_factory.hpp:77] Creating layer pool5
I1105 12:48:10.850592 24062 net.cpp:91] Creating Layer pool5
I1105 12:48:10.850599 24062 net.cpp:425] pool5 <- conv4
I1105 12:48:10.850605 24062 net.cpp:399] pool5 -> pool5
I1105 12:48:10.850641 24062 net.cpp:141] Setting up pool5
I1105 12:48:10.850648 24062 net.cpp:148] Top shape: 24 256 7 7 (301056)
I1105 12:48:10.850654 24062 net.cpp:156] Memory required for data: 173092512
I1105 12:48:10.850661 24062 layer_factory.hpp:77] Creating layer fc6
I1105 12:48:10.850672 24062 net.cpp:91] Creating Layer fc6
I1105 12:48:10.850677 24062 net.cpp:425] fc6 <- pool5
I1105 12:48:10.850685 24062 net.cpp:399] fc6 -> fc6
I1105 12:48:11.202237 24062 net.cpp:141] Setting up fc6
I1105 12:48:11.202271 24062 net.cpp:148] Top shape: 24 4096 (98304)
I1105 12:48:11.202280 24062 net.cpp:156] Memory required for data: 173485728
I1105 12:48:11.202293 24062 layer_factory.hpp:77] Creating layer relu6
I1105 12:48:11.202306 24062 net.cpp:91] Creating Layer relu6
I1105 12:48:11.202312 24062 net.cpp:425] relu6 <- fc6
I1105 12:48:11.202319 24062 net.cpp:386] relu6 -> fc6 (in-place)
I1105 12:48:11.202329 24062 net.cpp:141] Setting up relu6
I1105 12:48:11.202334 24062 net.cpp:148] Top shape: 24 4096 (98304)
I1105 12:48:11.202338 24062 net.cpp:156] Memory required for data: 173878944
I1105 12:48:11.202342 24062 layer_factory.hpp:77] Creating layer drop6
I1105 12:48:11.202349 24062 net.cpp:91] Creating Layer drop6
I1105 12:48:11.202353 24062 net.cpp:425] drop6 <- fc6
I1105 12:48:11.202358 24062 net.cpp:386] drop6 -> fc6 (in-place)
I1105 12:48:11.202375 24062 net.cpp:141] Setting up drop6
I1105 12:48:11.202380 24062 net.cpp:148] Top shape: 24 4096 (98304)
I1105 12:48:11.202384 24062 net.cpp:156] Memory required for data: 174272160
I1105 12:48:11.202389 24062 layer_factory.hpp:77] Creating layer fc7
I1105 12:48:11.202396 24062 net.cpp:91] Creating Layer fc7
I1105 12:48:11.202400 24062 net.cpp:425] fc7 <- fc6
I1105 12:48:11.202405 24062 net.cpp:399] fc7 -> fc7
I1105 12:48:11.288399 24062 net.cpp:141] Setting up fc7
I1105 12:48:11.288429 24062 net.cpp:148] Top shape: 24 4096 (98304)
I1105 12:48:11.288434 24062 net.cpp:156] Memory required for data: 174665376
I1105 12:48:11.288444 24062 layer_factory.hpp:77] Creating layer relu7
I1105 12:48:11.288455 24062 net.cpp:91] Creating Layer relu7
I1105 12:48:11.288461 24062 net.cpp:425] relu7 <- fc7
I1105 12:48:11.288468 24062 net.cpp:386] relu7 -> fc7 (in-place)
I1105 12:48:11.288477 24062 net.cpp:141] Setting up relu7
I1105 12:48:11.288482 24062 net.cpp:148] Top shape: 24 4096 (98304)
I1105 12:48:11.288486 24062 net.cpp:156] Memory required for data: 175058592
I1105 12:48:11.288491 24062 layer_factory.hpp:77] Creating layer drop7
I1105 12:48:11.288498 24062 net.cpp:91] Creating Layer drop7
I1105 12:48:11.288502 24062 net.cpp:425] drop7 <- fc7
I1105 12:48:11.288508 24062 net.cpp:386] drop7 -> fc7 (in-place)
I1105 12:48:11.288527 24062 net.cpp:141] Setting up drop7
I1105 12:48:11.288532 24062 net.cpp:148] Top shape: 24 4096 (98304)
I1105 12:48:11.288537 24062 net.cpp:156] Memory required for data: 175451808
I1105 12:48:11.288542 24062 layer_factory.hpp:77] Creating layer fc8
I1105 12:48:11.288550 24062 net.cpp:91] Creating Layer fc8
I1105 12:48:11.288554 24062 net.cpp:425] fc8 <- fc7
I1105 12:48:11.288561 24062 net.cpp:399] fc8 -> fc8
I1105 12:48:11.289610 24062 net.cpp:141] Setting up fc8
I1105 12:48:11.289621 24062 net.cpp:148] Top shape: 24 40 (960)
I1105 12:48:11.289626 24062 net.cpp:156] Memory required for data: 175455648
I1105 12:48:11.289633 24062 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I1105 12:48:11.289639 24062 net.cpp:91] Creating Layer fc8_fc8_0_split
I1105 12:48:11.289644 24062 net.cpp:425] fc8_fc8_0_split <- fc8
I1105 12:48:11.289649 24062 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1105 12:48:11.289659 24062 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1105 12:48:11.289681 24062 net.cpp:141] Setting up fc8_fc8_0_split
I1105 12:48:11.289687 24062 net.cpp:148] Top shape: 24 40 (960)
I1105 12:48:11.289692 24062 net.cpp:148] Top shape: 24 40 (960)
I1105 12:48:11.289696 24062 net.cpp:156] Memory required for data: 175463328
I1105 12:48:11.289701 24062 layer_factory.hpp:77] Creating layer accuracy
I1105 12:48:11.289708 24062 net.cpp:91] Creating Layer accuracy
I1105 12:48:11.289712 24062 net.cpp:425] accuracy <- fc8_fc8_0_split_0
I1105 12:48:11.289718 24062 net.cpp:425] accuracy <- label_img_1_split_0
I1105 12:48:11.289723 24062 net.cpp:399] accuracy -> accuracy
I1105 12:48:11.289731 24062 net.cpp:141] Setting up accuracy
I1105 12:48:11.289736 24062 net.cpp:148] Top shape: (1)
I1105 12:48:11.289741 24062 net.cpp:156] Memory required for data: 175463332
I1105 12:48:11.289744 24062 layer_factory.hpp:77] Creating layer loss
I1105 12:48:11.289750 24062 net.cpp:91] Creating Layer loss
I1105 12:48:11.289755 24062 net.cpp:425] loss <- fc8_fc8_0_split_1
I1105 12:48:11.289759 24062 net.cpp:425] loss <- label_img_1_split_1
I1105 12:48:11.289765 24062 net.cpp:399] loss -> loss
I1105 12:48:11.289772 24062 layer_factory.hpp:77] Creating layer loss
I1105 12:48:11.289824 24062 net.cpp:141] Setting up loss
I1105 12:48:11.289829 24062 net.cpp:148] Top shape: (1)
I1105 12:48:11.289834 24062 net.cpp:151]     with loss weight 1
I1105 12:48:11.289844 24062 net.cpp:156] Memory required for data: 175463336
I1105 12:48:11.289847 24062 net.cpp:217] loss needs backward computation.
I1105 12:48:11.289852 24062 net.cpp:219] accuracy does not need backward computation.
I1105 12:48:11.289856 24062 net.cpp:217] fc8_fc8_0_split needs backward computation.
I1105 12:48:11.289860 24062 net.cpp:217] fc8 needs backward computation.
I1105 12:48:11.289865 24062 net.cpp:217] drop7 needs backward computation.
I1105 12:48:11.289868 24062 net.cpp:217] relu7 needs backward computation.
I1105 12:48:11.289873 24062 net.cpp:217] fc7 needs backward computation.
I1105 12:48:11.289876 24062 net.cpp:217] drop6 needs backward computation.
I1105 12:48:11.289881 24062 net.cpp:217] relu6 needs backward computation.
I1105 12:48:11.289886 24062 net.cpp:217] fc6 needs backward computation.
I1105 12:48:11.289891 24062 net.cpp:217] pool5 needs backward computation.
I1105 12:48:11.289896 24062 net.cpp:217] relu4 needs backward computation.
I1105 12:48:11.289901 24062 net.cpp:217] conv4 needs backward computation.
I1105 12:48:11.289904 24062 net.cpp:217] relu3 needs backward computation.
I1105 12:48:11.289911 24062 net.cpp:217] conv3 needs backward computation.
I1105 12:48:11.289914 24062 net.cpp:217] norm2 needs backward computation.
I1105 12:48:11.289918 24062 net.cpp:217] pool2 needs backward computation.
I1105 12:48:11.289923 24062 net.cpp:217] relu2 needs backward computation.
I1105 12:48:11.289928 24062 net.cpp:217] conv2 needs backward computation.
I1105 12:48:11.289932 24062 net.cpp:217] norm1 needs backward computation.
I1105 12:48:11.289937 24062 net.cpp:217] pool1 needs backward computation.
I1105 12:48:11.289940 24062 net.cpp:217] relu1 needs backward computation.
I1105 12:48:11.289944 24062 net.cpp:217] conv1 needs backward computation.
I1105 12:48:11.289948 24062 net.cpp:219] label_img_1_split does not need backward computation.
I1105 12:48:11.289953 24062 net.cpp:219] img does not need backward computation.
I1105 12:48:11.289958 24062 net.cpp:261] This network produces output accuracy
I1105 12:48:11.289963 24062 net.cpp:261] This network produces output loss
I1105 12:48:11.289974 24062 net.cpp:274] Network initialization done.
I1105 12:48:11.290031 24062 solver.cpp:60] Solver scaffolding done.
I1105 12:48:11.298465 24062 solver.cpp:337] Iteration 0, Testing net (#0)
I1105 12:48:12.738895 24062 solver.cpp:228] Iteration 0, loss = 3.66588
I1105 12:48:12.738970 24062 solver.cpp:244]     Train net output #0: accuracy = 0.0390625
I1105 12:48:12.738991 24062 solver.cpp:244]     Train net output #1: loss = 3.66588 (* 1 = 3.66588 loss)
I1105 12:48:12.739004 24062 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1105 12:50:26.381117 24062 solver.cpp:228] Iteration 100, loss = 2.64922
I1105 12:50:26.381150 24062 solver.cpp:244]     Train net output #0: accuracy = 0.28125
I1105 12:50:26.381160 24062 solver.cpp:244]     Train net output #1: loss = 2.64922 (* 1 = 2.64922 loss)
I1105 12:50:26.381168 24062 sgd_solver.cpp:106] Iteration 100, lr = 0.01
I1105 12:52:58.812260 24062 solver.cpp:228] Iteration 200, loss = 2.09452
I1105 12:52:58.812305 24062 solver.cpp:244]     Train net output #0: accuracy = 0.429688
I1105 12:52:58.812315 24062 solver.cpp:244]     Train net output #1: loss = 2.09452 (* 1 = 2.09452 loss)
I1105 12:52:58.812326 24062 sgd_solver.cpp:106] Iteration 200, lr = 0.01
I1105 12:58:33.235380 24062 solver.cpp:228] Iteration 300, loss = 1.79967
I1105 12:58:33.235432 24062 solver.cpp:244]     Train net output #0: accuracy = 0.476562
I1105 12:58:33.235476 24062 solver.cpp:244]     Train net output #1: loss = 1.79967 (* 1 = 1.79967 loss)
I1105 12:58:33.235496 24062 sgd_solver.cpp:106] Iteration 300, lr = 0.01
I1105 13:03:49.850591 24062 solver.cpp:228] Iteration 400, loss = 1.59451
I1105 13:03:49.850639 24062 solver.cpp:244]     Train net output #0: accuracy = 0.554688
I1105 13:03:49.850657 24062 solver.cpp:244]     Train net output #1: loss = 1.59451 (* 1 = 1.59451 loss)
I1105 13:03:49.850669 24062 sgd_solver.cpp:106] Iteration 400, lr = 0.01
>>> 2016-11-05 13:09:16.824488 Begin model classification tests
/home/kevin/catkin_ws/src/romans_stack/model_net/caffe_net/score.py:81: RuntimeWarning: invalid value encountered in true_divide
  return accuracy/len(dataset), loss/len(dataset), confusion_mat/(np.tile(confusion_mat.sum(1),(40,1))).T
>>> 2016-11-05 13:24:50.838710 Iteration 500 mean classification accuracy  0.603565365025
>>> 2016-11-05 13:24:50.838737 Iteration 500 mean testing loss 1.45626186358
>>> 2016-11-05 13:24:50.838756 Iteration 500 mean confusion matrix [[ 1.    0.    0.   ...,  0.    0.    0.  ]
 [ 0.    0.14  0.08 ...,  0.02  0.    0.  ]
 [ 0.    0.    0.65 ...,  0.    0.    0.  ]
 ..., 
 [ 0.    0.    0.   ...,  0.69  0.    0.  ]
 [  nan   nan   nan ...,   nan   nan   nan]
 [ 0.    0.    0.   ...,  0.    0.    0.  ]]
I1105 13:24:55.028194 24062 solver.cpp:228] Iteration 500, loss = 1.31994
I1105 13:24:55.028234 24062 solver.cpp:244]     Train net output #0: accuracy = 0.617188
I1105 13:24:55.028250 24062 solver.cpp:244]     Train net output #1: loss = 1.31994 (* 1 = 1.31994 loss)
I1105 13:24:55.028262 24062 sgd_solver.cpp:106] Iteration 500, lr = 0.01
I1105 13:31:58.017465 24062 solver.cpp:228] Iteration 600, loss = 0.965747
I1105 13:31:58.017520 24062 solver.cpp:244]     Train net output #0: accuracy = 0.742188
I1105 13:31:58.017539 24062 solver.cpp:244]     Train net output #1: loss = 0.965747 (* 1 = 0.965747 loss)
I1105 13:31:58.017554 24062 sgd_solver.cpp:106] Iteration 600, lr = 0.01
I1105 13:38:57.311523 24062 solver.cpp:228] Iteration 700, loss = 1.37362
I1105 13:38:57.311564 24062 solver.cpp:244]     Train net output #0: accuracy = 0.59375
I1105 13:38:57.311576 24062 solver.cpp:244]     Train net output #1: loss = 1.37362 (* 1 = 1.37362 loss)
I1105 13:38:57.311589 24062 sgd_solver.cpp:106] Iteration 700, lr = 0.01
I1105 13:46:31.904983 24062 solver.cpp:228] Iteration 800, loss = 1.05512
I1105 13:46:31.905055 24062 solver.cpp:244]     Train net output #0: accuracy = 0.664062
I1105 13:46:31.905088 24062 solver.cpp:244]     Train net output #1: loss = 1.05512 (* 1 = 1.05512 loss)
I1105 13:46:31.905112 24062 sgd_solver.cpp:106] Iteration 800, lr = 0.01
I1105 13:52:29.555500 24062 solver.cpp:228] Iteration 900, loss = 1.17831
I1105 13:52:29.555541 24062 solver.cpp:244]     Train net output #0: accuracy = 0.65625
I1105 13:52:29.555557 24062 solver.cpp:244]     Train net output #1: loss = 1.17831 (* 1 = 1.17831 loss)
I1105 13:52:29.555570 24062 sgd_solver.cpp:106] Iteration 900, lr = 0.01
I1105 13:58:14.140867 24062 solver.cpp:454] Snapshotting to binary proto file /home/kevin/snapshot_iter_1000.caffemodel
I1105 13:58:20.535399 24062 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/kevin/snapshot_iter_1000.solverstate
>>> 2016-11-05 13:58:21.036443 Begin model classification tests
>>> 2016-11-05 14:14:37.903021 Iteration 1000 mean classification accuracy  0.679966044143
>>> 2016-11-05 14:14:37.903072 Iteration 1000 mean testing loss 1.19648939246
>>> 2016-11-05 14:14:37.903095 Iteration 1000 mean confusion matrix [[ 1.    0.    0.   ...,  0.    0.    0.  ]
 [ 0.    0.84  0.   ...,  0.    0.    0.  ]
 [ 0.    0.    0.83 ...,  0.    0.    0.  ]
 ..., 
 [ 0.    0.03  0.   ...,  0.73  0.    0.  ]
 [  nan   nan   nan ...,   nan   nan   nan]
 [ 0.    0.    0.   ...,  0.    0.    0.  ]]
I1105 14:14:41.556438 24062 solver.cpp:228] Iteration 1000, loss = 1.23569
I1105 14:14:41.556489 24062 solver.cpp:244]     Train net output #0: accuracy = 0.585938
I1105 14:14:41.556535 24062 solver.cpp:244]     Train net output #1: loss = 1.23569 (* 1 = 1.23569 loss)
I1105 14:14:41.556593 24062 sgd_solver.cpp:106] Iteration 1000, lr = 0.01
I1105 14:20:31.705992 24062 solver.cpp:228] Iteration 1100, loss = 1.02837
I1105 14:20:31.706051 24062 solver.cpp:244]     Train net output #0: accuracy = 0.6875
I1105 14:20:31.706074 24062 solver.cpp:244]     Train net output #1: loss = 1.02837 (* 1 = 1.02837 loss)
I1105 14:20:31.706090 24062 sgd_solver.cpp:106] Iteration 1100, lr = 0.01
I1105 14:26:12.092990 24062 solver.cpp:228] Iteration 1200, loss = 0.937748
I1105 14:26:12.093031 24062 solver.cpp:244]     Train net output #0: accuracy = 0.726562
I1105 14:26:12.093044 24062 solver.cpp:244]     Train net output #1: loss = 0.937748 (* 1 = 0.937748 loss)
I1105 14:26:12.093057 24062 sgd_solver.cpp:106] Iteration 1200, lr = 0.01
I1105 14:31:46.715739 24062 solver.cpp:228] Iteration 1300, loss = 0.949229
I1105 14:31:46.715786 24062 solver.cpp:244]     Train net output #0: accuracy = 0.695312
I1105 14:31:46.715806 24062 solver.cpp:244]     Train net output #1: loss = 0.949229 (* 1 = 0.949229 loss)
I1105 14:31:46.715819 24062 sgd_solver.cpp:106] Iteration 1300, lr = 0.01
I1105 14:37:33.740882 24062 solver.cpp:228] Iteration 1400, loss = 0.778178
I1105 14:37:33.740947 24062 solver.cpp:244]     Train net output #0: accuracy = 0.75
I1105 14:37:33.740970 24062 solver.cpp:244]     Train net output #1: loss = 0.778178 (* 1 = 0.778178 loss)
I1105 14:37:33.740986 24062 sgd_solver.cpp:106] Iteration 1400, lr = 0.01
>>> 2016-11-05 14:43:21.199763 Begin model classification tests
>>> 2016-11-05 14:59:21.245457 Iteration 1500 mean classification accuracy  0.75636672326
>>> 2016-11-05 14:59:21.245504 Iteration 1500 mean testing loss 0.957106502223
>>> 2016-11-05 14:59:21.245529 Iteration 1500 mean confusion matrix [[ 1.    0.    0.   ...,  0.    0.    0.  ]
 [ 0.    0.76  0.06 ...,  0.02  0.    0.  ]
 [ 0.    0.    0.81 ...,  0.    0.    0.  ]
 ..., 
 [ 0.    0.    0.   ...,  0.87  0.    0.  ]
 [  nan   nan   nan ...,   nan   nan   nan]
 [ 0.    0.    0.   ...,  0.    0.    0.  ]]
I1105 14:59:24.677917 24062 solver.cpp:228] Iteration 1500, loss = 0.929726
I1105 14:59:24.677963 24062 solver.cpp:244]     Train net output #0: accuracy = 0.710938
I1105 14:59:24.677980 24062 solver.cpp:244]     Train net output #1: loss = 0.929726 (* 1 = 0.929726 loss)
I1105 14:59:24.677994 24062 sgd_solver.cpp:106] Iteration 1500, lr = 0.01
I1105 15:05:10.930418 24062 solver.cpp:228] Iteration 1600, loss = 0.678422
I1105 15:05:10.930506 24062 solver.cpp:244]     Train net output #0: accuracy = 0.789062
I1105 15:05:10.930536 24062 solver.cpp:244]     Train net output #1: loss = 0.678422 (* 1 = 0.678422 loss)
I1105 15:05:10.930554 24062 sgd_solver.cpp:106] Iteration 1600, lr = 0.01
I1105 15:10:51.961649 24062 solver.cpp:228] Iteration 1700, loss = 0.756535
I1105 15:10:51.961707 24062 solver.cpp:244]     Train net output #0: accuracy = 0.765625
I1105 15:10:51.961730 24062 solver.cpp:244]     Train net output #1: loss = 0.756535 (* 1 = 0.756535 loss)
I1105 15:10:51.961745 24062 sgd_solver.cpp:106] Iteration 1700, lr = 0.01
I1105 15:16:31.524899 24062 solver.cpp:228] Iteration 1800, loss = 0.6846
I1105 15:16:31.524946 24062 solver.cpp:244]     Train net output #0: accuracy = 0.796875
I1105 15:16:31.524957 24062 solver.cpp:244]     Train net output #1: loss = 0.6846 (* 1 = 0.6846 loss)
I1105 15:16:31.524968 24062 sgd_solver.cpp:106] Iteration 1800, lr = 0.01
I1105 15:21:29.928217 24062 solver.cpp:228] Iteration 1900, loss = 0.655663
I1105 15:21:29.928261 24062 solver.cpp:244]     Train net output #0: accuracy = 0.78125
I1105 15:21:29.928272 24062 solver.cpp:244]     Train net output #1: loss = 0.655663 (* 1 = 0.655663 loss)
I1105 15:21:29.928280 24062 sgd_solver.cpp:106] Iteration 1900, lr = 0.01
I1105 15:26:40.785806 24062 solver.cpp:454] Snapshotting to binary proto file /home/kevin/snapshot_iter_2000.caffemodel
I1105 15:26:43.595835 24062 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/kevin/snapshot_iter_2000.solverstate
>>> 2016-11-05 15:26:43.920030 Begin model classification tests
>>> 2016-11-05 15:39:13.257059 Iteration 2000 mean classification accuracy  0.794567062818
>>> 2016-11-05 15:39:13.257113 Iteration 2000 mean testing loss 0.840462119146
>>> 2016-11-05 15:39:13.257128 Iteration 2000 mean confusion matrix [[ 1.    0.    0.   ...,  0.    0.    0.  ]
 [ 0.    0.76  0.04 ...,  0.    0.    0.  ]
 [ 0.    0.    0.84 ...,  0.    0.    0.  ]
 ..., 
 [ 0.    0.    0.   ...,  0.92  0.    0.  ]
 [  nan   nan   nan ...,   nan   nan   nan]
 [ 0.    0.    0.   ...,  0.    0.    0.  ]]
I1105 15:39:16.570106 24062 solver.cpp:228] Iteration 2000, loss = 0.957884
I1105 15:39:16.570194 24062 solver.cpp:244]     Train net output #0: accuracy = 0.726562
I1105 15:39:16.570207 24062 solver.cpp:244]     Train net output #1: loss = 0.957884 (* 1 = 0.957884 loss)
I1105 15:39:16.570219 24062 sgd_solver.cpp:106] Iteration 2000, lr = 0.01
I1105 15:44:35.445407 24062 solver.cpp:228] Iteration 2100, loss = 0.918548
I1105 15:44:35.445458 24062 solver.cpp:244]     Train net output #0: accuracy = 0.695312
I1105 15:44:35.445477 24062 solver.cpp:244]     Train net output #1: loss = 0.918548 (* 1 = 0.918548 loss)
I1105 15:44:35.445490 24062 sgd_solver.cpp:106] Iteration 2100, lr = 0.01
I1105 15:50:18.576191 24062 solver.cpp:228] Iteration 2200, loss = 0.711238
I1105 15:50:18.576237 24062 solver.cpp:244]     Train net output #0: accuracy = 0.765625
I1105 15:50:18.576247 24062 solver.cpp:244]     Train net output #1: loss = 0.711238 (* 1 = 0.711238 loss)
I1105 15:50:18.576256 24062 sgd_solver.cpp:106] Iteration 2200, lr = 0.01
I1105 15:55:21.999863 24062 solver.cpp:228] Iteration 2300, loss = 0.654
I1105 15:55:21.999922 24062 solver.cpp:244]     Train net output #0: accuracy = 0.789062
I1105 15:55:21.999938 24062 solver.cpp:244]     Train net output #1: loss = 0.654 (* 1 = 0.654 loss)
I1105 15:55:21.999955 24062 sgd_solver.cpp:106] Iteration 2300, lr = 0.01
I1105 16:00:36.615206 24062 solver.cpp:228] Iteration 2400, loss = 0.612289
I1105 16:00:36.615257 24062 solver.cpp:244]     Train net output #0: accuracy = 0.804688
I1105 16:00:36.615270 24062 solver.cpp:244]     Train net output #1: loss = 0.612289 (* 1 = 0.612289 loss)
I1105 16:00:36.615280 24062 sgd_solver.cpp:106] Iteration 2400, lr = 0.01
>>> 2016-11-05 16:06:23.076453 Begin model classification tests
>>> 2016-11-05 16:19:14.254538 Iteration 2500 mean classification accuracy  0.798811544992
>>> 2016-11-05 16:19:14.254653 Iteration 2500 mean testing loss 0.809234389774
>>> 2016-11-05 16:19:14.254692 Iteration 2500 mean confusion matrix [[ 1.    0.    0.   ...,  0.    0.    0.  ]
 [ 0.    0.84  0.08 ...,  0.    0.    0.  ]
 [ 0.    0.01  0.82 ...,  0.    0.    0.  ]
 ..., 
 [ 0.    0.    0.   ...,  0.84  0.    0.  ]
 [  nan   nan   nan ...,   nan   nan   nan]
 [ 0.    0.    0.   ...,  0.    0.    0.  ]]
I1105 16:19:18.746804 24062 solver.cpp:228] Iteration 2500, loss = 0.819927
I1105 16:19:18.746887 24062 solver.cpp:244]     Train net output #0: accuracy = 0.742188
I1105 16:19:18.746927 24062 solver.cpp:244]     Train net output #1: loss = 0.819927 (* 1 = 0.819927 loss)
I1105 16:19:18.746958 24062 sgd_solver.cpp:106] Iteration 2500, lr = 0.01
I1105 16:25:34.404673 24062 solver.cpp:228] Iteration 2600, loss = 0.537827
I1105 16:25:34.404711 24062 solver.cpp:244]     Train net output #0: accuracy = 0.835938
I1105 16:25:34.404721 24062 solver.cpp:244]     Train net output #1: loss = 0.537827 (* 1 = 0.537827 loss)
I1105 16:25:34.404729 24062 sgd_solver.cpp:106] Iteration 2600, lr = 0.01
I1105 16:30:48.180551 24062 solver.cpp:228] Iteration 2700, loss = 0.660612
I1105 16:30:48.180632 24062 solver.cpp:244]     Train net output #0: accuracy = 0.789062
I1105 16:30:48.180654 24062 solver.cpp:244]     Train net output #1: loss = 0.660612 (* 1 = 0.660612 loss)
I1105 16:30:48.180678 24062 sgd_solver.cpp:106] Iteration 2700, lr = 0.01
I1105 16:35:59.703017 24062 solver.cpp:228] Iteration 2800, loss = 0.664255
I1105 16:35:59.703059 24062 solver.cpp:244]     Train net output #0: accuracy = 0.757812
I1105 16:35:59.703076 24062 solver.cpp:244]     Train net output #1: loss = 0.664255 (* 1 = 0.664255 loss)
I1105 16:35:59.703090 24062 sgd_solver.cpp:106] Iteration 2800, lr = 0.01
I1105 16:41:29.409574 24062 solver.cpp:228] Iteration 2900, loss = 0.635112
I1105 16:41:29.409631 24062 solver.cpp:244]     Train net output #0: accuracy = 0.804688
I1105 16:41:29.409643 24062 solver.cpp:244]     Train net output #1: loss = 0.635112 (* 1 = 0.635112 loss)
I1105 16:41:29.409654 24062 sgd_solver.cpp:106] Iteration 2900, lr = 0.01
I1105 16:46:21.369675 24062 solver.cpp:454] Snapshotting to binary proto file /home/kevin/snapshot_iter_3000.caffemodel
I1105 16:46:31.234711 24062 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/kevin/snapshot_iter_3000.solverstate
>>> 2016-11-05 16:46:31.493771 Begin model classification tests
>>> 2016-11-05 16:59:17.585919 Iteration 3000 mean classification accuracy  0.806027164686
>>> 2016-11-05 16:59:17.585963 Iteration 3000 mean testing loss 0.798860120514
>>> 2016-11-05 16:59:17.585984 Iteration 3000 mean confusion matrix [[ 1.    0.    0.   ...,  0.    0.    0.  ]
 [ 0.    0.9   0.02 ...,  0.    0.    0.  ]
 [ 0.    0.01  0.86 ...,  0.    0.    0.  ]
 ..., 
 [ 0.    0.    0.   ...,  0.68  0.    0.  ]
 [  nan   nan   nan ...,   nan   nan   nan]
 [ 0.    0.    0.   ...,  0.    0.    0.  ]]
I1105 16:59:22.070942 24062 solver.cpp:228] Iteration 3000, loss = 0.554771
I1105 16:59:22.070979 24062 solver.cpp:244]     Train net output #0: accuracy = 0.84375
I1105 16:59:22.070991 24062 solver.cpp:244]     Train net output #1: loss = 0.554771 (* 1 = 0.554771 loss)
I1105 16:59:22.070999 24062 sgd_solver.cpp:106] Iteration 3000, lr = 0.01
I1105 17:04:41.491158 24062 solver.cpp:228] Iteration 3100, loss = 0.56271
I1105 17:04:41.491196 24062 solver.cpp:244]     Train net output #0: accuracy = 0.828125
I1105 17:04:41.491209 24062 solver.cpp:244]     Train net output #1: loss = 0.56271 (* 1 = 0.56271 loss)
I1105 17:04:41.491219 24062 sgd_solver.cpp:106] Iteration 3100, lr = 0.01
I1105 17:10:16.669652 24062 solver.cpp:228] Iteration 3200, loss = 0.578524
I1105 17:10:16.669697 24062 solver.cpp:244]     Train net output #0: accuracy = 0.789062
I1105 17:10:16.669731 24062 solver.cpp:244]     Train net output #1: loss = 0.578524 (* 1 = 0.578524 loss)
I1105 17:10:16.669741 24062 sgd_solver.cpp:106] Iteration 3200, lr = 0.01
I1105 17:16:03.980473 24062 solver.cpp:228] Iteration 3300, loss = 0.753616
I1105 17:16:03.980541 24062 solver.cpp:244]     Train net output #0: accuracy = 0.8125
I1105 17:16:03.980564 24062 solver.cpp:244]     Train net output #1: loss = 0.753616 (* 1 = 0.753616 loss)
I1105 17:16:03.980592 24062 sgd_solver.cpp:106] Iteration 3300, lr = 0.01
I1105 17:22:05.111141 24062 solver.cpp:228] Iteration 3400, loss = 0.435527
I1105 17:22:05.111189 24062 solver.cpp:244]     Train net output #0: accuracy = 0.859375
I1105 17:22:05.111204 24062 solver.cpp:244]     Train net output #1: loss = 0.435527 (* 1 = 0.435527 loss)
I1105 17:22:05.111215 24062 sgd_solver.cpp:106] Iteration 3400, lr = 0.01
>>> 2016-11-05 17:27:41.839222 Begin model classification tests
>>> 2016-11-05 17:40:34.159576 Iteration 3500 mean classification accuracy  0.830220713073
>>> 2016-11-05 17:40:34.159626 Iteration 3500 mean testing loss 0.718747232242
>>> 2016-11-05 17:40:34.159648 Iteration 3500 mean confusion matrix [[ 1.    0.    0.   ...,  0.    0.    0.  ]
 [ 0.    0.92  0.02 ...,  0.    0.    0.  ]
 [ 0.    0.    0.79 ...,  0.    0.    0.  ]
 ..., 
 [ 0.    0.    0.   ...,  0.88  0.    0.  ]
 [  nan   nan   nan ...,   nan   nan   nan]
 [ 0.    0.    0.   ...,  0.    0.    0.  ]]
I1105 17:40:37.359714 24062 solver.cpp:228] Iteration 3500, loss = 0.705306
I1105 17:40:37.359757 24062 solver.cpp:244]     Train net output #0: accuracy = 0.78125
I1105 17:40:37.359767 24062 solver.cpp:244]     Train net output #1: loss = 0.705306 (* 1 = 0.705306 loss)
I1105 17:40:37.359777 24062 sgd_solver.cpp:106] Iteration 3500, lr = 0.01
I1105 17:46:03.260831 24062 solver.cpp:228] Iteration 3600, loss = 0.618358
I1105 17:46:03.260869 24062 solver.cpp:244]     Train net output #0: accuracy = 0.8125
I1105 17:46:03.260886 24062 solver.cpp:244]     Train net output #1: loss = 0.618358 (* 1 = 0.618358 loss)
I1105 17:46:03.260897 24062 sgd_solver.cpp:106] Iteration 3600, lr = 0.01
I1105 17:51:11.274564 24062 solver.cpp:228] Iteration 3700, loss = 0.597542
I1105 17:51:11.274605 24062 solver.cpp:244]     Train net output #0: accuracy = 0.796875
I1105 17:51:11.274617 24062 solver.cpp:244]     Train net output #1: loss = 0.597542 (* 1 = 0.597542 loss)
I1105 17:51:11.274626 24062 sgd_solver.cpp:106] Iteration 3700, lr = 0.01
I1105 17:56:20.791777 24062 solver.cpp:228] Iteration 3800, loss = 0.591415
I1105 17:56:20.791821 24062 solver.cpp:244]     Train net output #0: accuracy = 0.78125
I1105 17:56:20.791851 24062 solver.cpp:244]     Train net output #1: loss = 0.591415 (* 1 = 0.591415 loss)
I1105 17:56:20.791867 24062 sgd_solver.cpp:106] Iteration 3800, lr = 0.01
I1105 18:01:40.333920 24062 solver.cpp:228] Iteration 3900, loss = 0.442792
I1105 18:01:40.333961 24062 solver.cpp:244]     Train net output #0: accuracy = 0.859375
I1105 18:01:40.333976 24062 solver.cpp:244]     Train net output #1: loss = 0.442792 (* 1 = 0.442792 loss)
I1105 18:01:40.333987 24062 sgd_solver.cpp:106] Iteration 3900, lr = 0.01
I1105 18:06:36.204960 24062 solver.cpp:454] Snapshotting to binary proto file /home/kevin/snapshot_iter_4000.caffemodel
I1105 18:06:38.644667 24062 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/kevin/snapshot_iter_4000.solverstate
>>> 2016-11-05 18:06:38.915276 Begin model classification tests
>>> 2016-11-05 18:19:13.877919 Iteration 4000 mean classification accuracy  0.833191850594
>>> 2016-11-05 18:19:13.877953 Iteration 4000 mean testing loss 0.71846376505
>>> 2016-11-05 18:19:13.877983 Iteration 4000 mean confusion matrix [[ 1.    0.    0.   ...,  0.    0.    0.  ]
 [ 0.    0.94  0.02 ...,  0.    0.    0.  ]
 [ 0.    0.    0.9  ...,  0.    0.    0.  ]
 ..., 
 [ 0.    0.    0.   ...,  0.87  0.    0.  ]
 [  nan   nan   nan ...,   nan   nan   nan]
 [ 0.    0.    0.   ...,  0.    0.    0.  ]]
I1105 18:19:17.318675 24062 solver.cpp:228] Iteration 4000, loss = 0.378619
I1105 18:19:17.318720 24062 solver.cpp:244]     Train net output #0: accuracy = 0.867188
I1105 18:19:17.318735 24062 solver.cpp:244]     Train net output #1: loss = 0.378619 (* 1 = 0.378619 loss)
I1105 18:19:17.318758 24062 sgd_solver.cpp:106] Iteration 4000, lr = 0.01
I1105 18:24:53.554083 24062 solver.cpp:228] Iteration 4100, loss = 0.418876
I1105 18:24:53.554133 24062 solver.cpp:244]     Train net output #0: accuracy = 0.84375
I1105 18:24:53.554146 24062 solver.cpp:244]     Train net output #1: loss = 0.418876 (* 1 = 0.418876 loss)
I1105 18:24:53.554168 24062 sgd_solver.cpp:106] Iteration 4100, lr = 0.01
I1105 18:29:59.271006 24062 solver.cpp:228] Iteration 4200, loss = 0.364174
I1105 18:29:59.271061 24062 solver.cpp:244]     Train net output #0: accuracy = 0.882812
I1105 18:29:59.271075 24062 solver.cpp:244]     Train net output #1: loss = 0.364174 (* 1 = 0.364174 loss)
I1105 18:29:59.271100 24062 sgd_solver.cpp:106] Iteration 4200, lr = 0.01
I1105 18:35:07.114498 24062 solver.cpp:228] Iteration 4300, loss = 0.342719
I1105 18:35:07.114555 24062 solver.cpp:244]     Train net output #0: accuracy = 0.882812
I1105 18:35:07.114585 24062 solver.cpp:244]     Train net output #1: loss = 0.342719 (* 1 = 0.342719 loss)
I1105 18:35:07.114609 24062 sgd_solver.cpp:106] Iteration 4300, lr = 0.01
I1105 18:40:05.413384 24062 solver.cpp:228] Iteration 4400, loss = 0.604659
I1105 18:40:05.413430 24062 solver.cpp:244]     Train net output #0: accuracy = 0.804688
I1105 18:40:05.413441 24062 solver.cpp:244]     Train net output #1: loss = 0.604659 (* 1 = 0.604659 loss)
I1105 18:40:05.413453 24062 sgd_solver.cpp:106] Iteration 4400, lr = 0.01
>>> 2016-11-05 18:44:55.009909 Begin model classification tests
>>> 2016-11-05 18:57:18.534593 Iteration 4500 mean classification accuracy  0.832767402377
>>> 2016-11-05 18:57:18.534637 Iteration 4500 mean testing loss 0.694264605589
>>> 2016-11-05 18:57:18.534658 Iteration 4500 mean confusion matrix [[ 1.    0.    0.   ...,  0.    0.    0.  ]
 [ 0.    0.88  0.02 ...,  0.    0.    0.  ]
 [ 0.    0.    0.89 ...,  0.    0.    0.  ]
 ..., 
 [ 0.    0.01  0.   ...,  0.9   0.    0.  ]
 [  nan   nan   nan ...,   nan   nan   nan]
 [ 0.    0.    0.   ...,  0.    0.    0.  ]]
I1105 18:57:22.624147 24062 solver.cpp:228] Iteration 4500, loss = 0.502551
I1105 18:57:22.624197 24062 solver.cpp:244]     Train net output #0: accuracy = 0.859375
I1105 18:57:22.624209 24062 solver.cpp:244]     Train net output #1: loss = 0.502551 (* 1 = 0.502551 loss)
I1105 18:57:22.624230 24062 sgd_solver.cpp:106] Iteration 4500, lr = 0.01
I1105 19:02:41.712914 24062 solver.cpp:228] Iteration 4600, loss = 0.591432
I1105 19:02:41.712960 24062 solver.cpp:244]     Train net output #0: accuracy = 0.804688
I1105 19:02:41.712975 24062 solver.cpp:244]     Train net output #1: loss = 0.591432 (* 1 = 0.591432 loss)
I1105 19:02:41.712998 24062 sgd_solver.cpp:106] Iteration 4600, lr = 0.01
I1105 19:08:02.669301 24062 solver.cpp:228] Iteration 4700, loss = 0.647055
I1105 19:08:02.669353 24062 solver.cpp:244]     Train net output #0: accuracy = 0.835938
I1105 19:08:02.669365 24062 solver.cpp:244]     Train net output #1: loss = 0.647055 (* 1 = 0.647055 loss)
I1105 19:08:02.669385 24062 sgd_solver.cpp:106] Iteration 4700, lr = 0.01
I1105 19:12:59.312453 24062 solver.cpp:228] Iteration 4800, loss = 0.56425
I1105 19:12:59.312500 24062 solver.cpp:244]     Train net output #0: accuracy = 0.789062
I1105 19:12:59.312512 24062 solver.cpp:244]     Train net output #1: loss = 0.56425 (* 1 = 0.56425 loss)
I1105 19:12:59.312531 24062 sgd_solver.cpp:106] Iteration 4800, lr = 0.01
I1105 19:17:54.753038 24062 solver.cpp:228] Iteration 4900, loss = 0.312333
I1105 19:17:54.753087 24062 solver.cpp:244]     Train net output #0: accuracy = 0.929688
I1105 19:17:54.753096 24062 solver.cpp:244]     Train net output #1: loss = 0.312333 (* 1 = 0.312333 loss)
I1105 19:17:54.753115 24062 sgd_solver.cpp:106] Iteration 4900, lr = 0.01
I1105 19:22:52.175256 24062 solver.cpp:454] Snapshotting to binary proto file /home/kevin/snapshot_iter_5000.caffemodel
I1105 19:22:54.396647 24062 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/kevin/snapshot_iter_5000.solverstate
>>> 2016-11-05 19:22:54.663036 Begin model classification tests
>>> 2016-11-05 19:35:43.598583 Iteration 5000 mean classification accuracy  0.837860780985
>>> 2016-11-05 19:35:43.598612 Iteration 5000 mean testing loss 0.702231666861
>>> 2016-11-05 19:35:43.598633 Iteration 5000 mean confusion matrix [[ 1.    0.    0.   ...,  0.    0.    0.  ]
 [ 0.    0.9   0.04 ...,  0.    0.    0.  ]
 [ 0.    0.    0.88 ...,  0.    0.    0.  ]
 ..., 
 [ 0.    0.    0.   ...,  0.86  0.    0.  ]
 [  nan   nan   nan ...,   nan   nan   nan]
 [ 0.    0.    0.   ...,  0.    0.    0.  ]]
I1105 19:35:47.084529 24062 solver.cpp:228] Iteration 5000, loss = 0.528041
I1105 19:35:47.084579 24062 solver.cpp:244]     Train net output #0: accuracy = 0.835938
I1105 19:35:47.084591 24062 solver.cpp:244]     Train net output #1: loss = 0.528041 (* 1 = 0.528041 loss)
I1105 19:35:47.084612 24062 sgd_solver.cpp:106] Iteration 5000, lr = 0.01
I1105 19:41:49.033262 24062 solver.cpp:228] Iteration 5100, loss = 0.413001
I1105 19:41:49.033298 24062 solver.cpp:244]     Train net output #0: accuracy = 0.835938
I1105 19:41:49.033308 24062 solver.cpp:244]     Train net output #1: loss = 0.413001 (* 1 = 0.413001 loss)
I1105 19:41:49.033327 24062 sgd_solver.cpp:106] Iteration 5100, lr = 0.01
I1105 19:47:20.712498 24062 solver.cpp:228] Iteration 5200, loss = 0.553103
I1105 19:47:20.712550 24062 solver.cpp:244]     Train net output #0: accuracy = 0.820312
I1105 19:47:20.712579 24062 solver.cpp:244]     Train net output #1: loss = 0.553103 (* 1 = 0.553103 loss)
I1105 19:47:20.712615 24062 sgd_solver.cpp:106] Iteration 5200, lr = 0.01
I1105 19:52:11.368335 24062 solver.cpp:228] Iteration 5300, loss = 0.371825
I1105 19:52:11.368377 24062 solver.cpp:244]     Train net output #0: accuracy = 0.914062
I1105 19:52:11.368394 24062 solver.cpp:244]     Train net output #1: loss = 0.371825 (* 1 = 0.371825 loss)
I1105 19:52:11.368417 24062 sgd_solver.cpp:106] Iteration 5300, lr = 0.01
I1105 19:57:04.444138 24062 solver.cpp:228] Iteration 5400, loss = 0.370219
I1105 19:57:04.444186 24062 solver.cpp:244]     Train net output #0: accuracy = 0.898438
I1105 19:57:04.444203 24062 solver.cpp:244]     Train net output #1: loss = 0.370219 (* 1 = 0.370219 loss)
I1105 19:57:04.444216 24062 sgd_solver.cpp:106] Iteration 5400, lr = 0.01
>>> 2016-11-05 20:02:08.867790 Begin model classification tests
>>> 2016-11-05 20:14:24.164219 Iteration 5500 mean classification accuracy  0.818760611205
>>> 2016-11-05 20:14:24.164256 Iteration 5500 mean testing loss 0.723343354409
>>> 2016-11-05 20:14:24.164285 Iteration 5500 mean confusion matrix [[ 1.    0.    0.   ...,  0.    0.    0.  ]
 [ 0.    0.9   0.04 ...,  0.    0.    0.  ]
 [ 0.    0.    0.92 ...,  0.    0.    0.  ]
 ..., 
 [ 0.    0.    0.   ...,  0.74  0.    0.  ]
 [  nan   nan   nan ...,   nan   nan   nan]
 [ 0.    0.    0.   ...,  0.    0.    0.  ]]
I1105 20:14:27.365027 24062 solver.cpp:228] Iteration 5500, loss = 0.660379
I1105 20:14:27.365077 24062 solver.cpp:244]     Train net output #0: accuracy = 0.78125
I1105 20:14:27.365087 24062 solver.cpp:244]     Train net output #1: loss = 0.660379 (* 1 = 0.660379 loss)
I1105 20:14:27.365108 24062 sgd_solver.cpp:106] Iteration 5500, lr = 0.01
I1105 20:19:41.602582 24062 solver.cpp:228] Iteration 5600, loss = 0.497332
I1105 20:19:41.602680 24062 solver.cpp:244]     Train net output #0: accuracy = 0.851562
I1105 20:19:41.602738 24062 solver.cpp:244]     Train net output #1: loss = 0.497332 (* 1 = 0.497332 loss)
I1105 20:19:41.602784 24062 sgd_solver.cpp:106] Iteration 5600, lr = 0.01
I1105 20:24:37.255828 24062 solver.cpp:228] Iteration 5700, loss = 0.436026
I1105 20:24:37.255882 24062 solver.cpp:244]     Train net output #0: accuracy = 0.875
I1105 20:24:37.255893 24062 solver.cpp:244]     Train net output #1: loss = 0.436026 (* 1 = 0.436026 loss)
I1105 20:24:37.255916 24062 sgd_solver.cpp:106] Iteration 5700, lr = 0.01
I1105 20:29:41.422003 24062 solver.cpp:228] Iteration 5800, loss = 0.430843
I1105 20:29:41.422046 24062 solver.cpp:244]     Train net output #0: accuracy = 0.867188
I1105 20:29:41.422060 24062 solver.cpp:244]     Train net output #1: loss = 0.430843 (* 1 = 0.430843 loss)
I1105 20:29:41.422071 24062 sgd_solver.cpp:106] Iteration 5800, lr = 0.01
I1105 20:34:33.416878 24062 solver.cpp:228] Iteration 5900, loss = 0.298344
I1105 20:34:33.416911 24062 solver.cpp:244]     Train net output #0: accuracy = 0.882812
I1105 20:34:33.416921 24062 solver.cpp:244]     Train net output #1: loss = 0.298344 (* 1 = 0.298344 loss)
I1105 20:34:33.416929 24062 sgd_solver.cpp:106] Iteration 5900, lr = 0.01
I1105 20:39:01.990577 24062 solver.cpp:454] Snapshotting to binary proto file /home/kevin/snapshot_iter_6000.caffemodel
I1105 20:39:04.548727 24062 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/kevin/snapshot_iter_6000.solverstate
>>> 2016-11-05 20:39:04.793090 Begin model classification tests
>>> 2016-11-05 20:49:56.019859 Iteration 6000 mean classification accuracy  0.835314091681
>>> 2016-11-05 20:49:56.019893 Iteration 6000 mean testing loss 0.683124747644
>>> 2016-11-05 20:49:56.019905 Iteration 6000 mean confusion matrix [[ 1.    0.    0.   ...,  0.    0.    0.  ]
 [ 0.    0.88  0.04 ...,  0.    0.    0.  ]
 [ 0.    0.    0.83 ...,  0.    0.    0.  ]
 ..., 
 [ 0.    0.    0.   ...,  0.82  0.    0.  ]
 [  nan   nan   nan ...,   nan   nan   nan]
 [ 0.    0.    0.   ...,  0.    0.    0.  ]]
I1105 20:50:00.680776 24062 solver.cpp:228] Iteration 6000, loss = 0.376223
I1105 20:50:00.680809 24062 solver.cpp:244]     Train net output #0: accuracy = 0.875
I1105 20:50:00.680819 24062 solver.cpp:244]     Train net output #1: loss = 0.376223 (* 1 = 0.376223 loss)
I1105 20:50:00.680826 24062 sgd_solver.cpp:106] Iteration 6000, lr = 0.01
I1105 20:55:04.806056 24062 solver.cpp:228] Iteration 6100, loss = 0.413599
I1105 20:55:04.806088 24062 solver.cpp:244]     Train net output #0: accuracy = 0.835938
I1105 20:55:04.806098 24062 solver.cpp:244]     Train net output #1: loss = 0.413599 (* 1 = 0.413599 loss)
I1105 20:55:04.806105 24062 sgd_solver.cpp:106] Iteration 6100, lr = 0.01
I1105 21:00:54.011284 24062 solver.cpp:228] Iteration 6200, loss = 0.374403
I1105 21:00:54.011315 24062 solver.cpp:244]     Train net output #0: accuracy = 0.882812
I1105 21:00:54.011327 24062 solver.cpp:244]     Train net output #1: loss = 0.374403 (* 1 = 0.374403 loss)
I1105 21:00:54.011333 24062 sgd_solver.cpp:106] Iteration 6200, lr = 0.01
I1105 21:06:30.681843 24062 solver.cpp:228] Iteration 6300, loss = 0.410055
I1105 21:06:30.681874 24062 solver.cpp:244]     Train net output #0: accuracy = 0.835938
I1105 21:06:30.681890 24062 solver.cpp:244]     Train net output #1: loss = 0.410055 (* 1 = 0.410055 loss)
I1105 21:06:30.681917 24062 sgd_solver.cpp:106] Iteration 6300, lr = 0.01
I1105 21:11:25.529167 24062 solver.cpp:228] Iteration 6400, loss = 0.436207
I1105 21:11:25.529199 24062 solver.cpp:244]     Train net output #0: accuracy = 0.835938
I1105 21:11:25.529227 24062 solver.cpp:244]     Train net output #1: loss = 0.436207 (* 1 = 0.436207 loss)
I1105 21:11:25.529247 24062 sgd_solver.cpp:106] Iteration 6400, lr = 0.01
>>> 2016-11-05 21:16:54.373804 Begin model classification tests
>>> 2016-11-05 21:27:52.369004 Iteration 6500 mean classification accuracy  0.83701188455
>>> 2016-11-05 21:27:52.369039 Iteration 6500 mean testing loss 0.662604484471
>>> 2016-11-05 21:27:52.369052 Iteration 6500 mean confusion matrix [[ 1.    0.    0.   ...,  0.    0.    0.  ]
 [ 0.    0.88  0.04 ...,  0.    0.    0.  ]
 [ 0.    0.    0.91 ...,  0.    0.    0.  ]
 ..., 
 [ 0.    0.    0.   ...,  0.87  0.    0.  ]
 [  nan   nan   nan ...,   nan   nan   nan]
 [ 0.    0.    0.   ...,  0.    0.    0.  ]]
I1105 21:27:55.805663 24062 solver.cpp:228] Iteration 6500, loss = 0.665954
I1105 21:27:55.805701 24062 solver.cpp:244]     Train net output #0: accuracy = 0.78125
I1105 21:27:55.805711 24062 solver.cpp:244]     Train net output #1: loss = 0.665954 (* 1 = 0.665954 loss)
I1105 21:27:55.805721 24062 sgd_solver.cpp:106] Iteration 6500, lr = 0.01
I1105 21:33:53.643929 24062 solver.cpp:228] Iteration 6600, loss = 0.482332
I1105 21:33:53.643960 24062 solver.cpp:244]     Train net output #0: accuracy = 0.828125
I1105 21:33:53.643971 24062 solver.cpp:244]     Train net output #1: loss = 0.482332 (* 1 = 0.482332 loss)
I1105 21:33:53.643980 24062 sgd_solver.cpp:106] Iteration 6600, lr = 0.01
I1105 21:39:42.555797 24062 solver.cpp:228] Iteration 6700, loss = 0.414747
I1105 21:39:42.555833 24062 solver.cpp:244]     Train net output #0: accuracy = 0.882812
I1105 21:39:42.555845 24062 solver.cpp:244]     Train net output #1: loss = 0.414747 (* 1 = 0.414747 loss)
I1105 21:39:42.555853 24062 sgd_solver.cpp:106] Iteration 6700, lr = 0.01
I1105 21:45:25.075131 24062 solver.cpp:228] Iteration 6800, loss = 0.285147
I1105 21:45:25.075165 24062 solver.cpp:244]     Train net output #0: accuracy = 0.914062
I1105 21:45:25.075175 24062 solver.cpp:244]     Train net output #1: loss = 0.285147 (* 1 = 0.285147 loss)
I1105 21:45:25.075182 24062 sgd_solver.cpp:106] Iteration 6800, lr = 0.01
I1105 21:51:01.389971 24062 solver.cpp:228] Iteration 6900, loss = 0.373778
I1105 21:51:01.390003 24062 solver.cpp:244]     Train net output #0: accuracy = 0.875
I1105 21:51:01.390013 24062 solver.cpp:244]     Train net output #1: loss = 0.373778 (* 1 = 0.373778 loss)
I1105 21:51:01.390020 24062 sgd_solver.cpp:106] Iteration 6900, lr = 0.01
I1105 21:56:31.545891 24062 solver.cpp:454] Snapshotting to binary proto file /home/kevin/snapshot_iter_7000.caffemodel
I1105 21:56:40.244616 24062 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/kevin/snapshot_iter_7000.solverstate
>>> 2016-11-05 21:56:40.497726 Begin model classification tests
>>> 2016-11-05 22:08:07.994985 Iteration 7000 mean classification accuracy  0.855687606112
>>> 2016-11-05 22:08:07.995017 Iteration 7000 mean testing loss 0.637310338304
>>> 2016-11-05 22:08:07.995029 Iteration 7000 mean confusion matrix [[ 1.    0.    0.   ...,  0.    0.    0.  ]
 [ 0.    0.88  0.02 ...,  0.    0.    0.  ]
 [ 0.    0.    0.88 ...,  0.    0.    0.  ]
 ..., 
 [ 0.    0.    0.   ...,  0.85  0.    0.  ]
 [  nan   nan   nan ...,   nan   nan   nan]
 [ 0.    0.    0.   ...,  0.    0.    0.  ]]
I1105 22:08:12.068763 24062 solver.cpp:228] Iteration 7000, loss = 0.317503
I1105 22:08:12.068799 24062 solver.cpp:244]     Train net output #0: accuracy = 0.90625
I1105 22:08:12.068820 24062 solver.cpp:244]     Train net output #1: loss = 0.317503 (* 1 = 0.317503 loss)
I1105 22:08:12.068831 24062 sgd_solver.cpp:106] Iteration 7000, lr = 0.01
I1105 22:13:18.156910 24062 solver.cpp:228] Iteration 7100, loss = 0.31779
I1105 22:13:18.156942 24062 solver.cpp:244]     Train net output #0: accuracy = 0.890625
I1105 22:13:18.156954 24062 solver.cpp:244]     Train net output #1: loss = 0.31779 (* 1 = 0.31779 loss)
I1105 22:13:18.156960 24062 sgd_solver.cpp:106] Iteration 7100, lr = 0.01
I1105 22:19:11.989969 24062 solver.cpp:228] Iteration 7200, loss = 0.37088
I1105 22:19:11.990003 24062 solver.cpp:244]     Train net output #0: accuracy = 0.890625
I1105 22:19:11.990013 24062 solver.cpp:244]     Train net output #1: loss = 0.37088 (* 1 = 0.37088 loss)
I1105 22:19:11.990020 24062 sgd_solver.cpp:106] Iteration 7200, lr = 0.01
I1105 22:24:55.824661 24062 solver.cpp:228] Iteration 7300, loss = 0.423101
I1105 22:24:55.824693 24062 solver.cpp:244]     Train net output #0: accuracy = 0.859375
I1105 22:24:55.824704 24062 solver.cpp:244]     Train net output #1: loss = 0.423101 (* 1 = 0.423101 loss)
I1105 22:24:55.824712 24062 sgd_solver.cpp:106] Iteration 7300, lr = 0.01
I1105 22:30:44.698653 24062 solver.cpp:228] Iteration 7400, loss = 0.334524
I1105 22:30:44.698693 24062 solver.cpp:244]     Train net output #0: accuracy = 0.882812
I1105 22:30:44.698711 24062 solver.cpp:244]     Train net output #1: loss = 0.334524 (* 1 = 0.334524 loss)
I1105 22:30:44.698720 24062 sgd_solver.cpp:106] Iteration 7400, lr = 0.01
>>> 2016-11-05 22:36:21.389341 Begin model classification tests
>>> 2016-11-05 22:47:34.932937 Iteration 7500 mean classification accuracy  0.854838709677
>>> 2016-11-05 22:47:34.932972 Iteration 7500 mean testing loss 0.645853680325
>>> 2016-11-05 22:47:34.932984 Iteration 7500 mean confusion matrix [[ 1.    0.    0.   ...,  0.    0.    0.  ]
 [ 0.    0.86  0.04 ...,  0.    0.    0.  ]
 [ 0.    0.    0.88 ...,  0.    0.    0.  ]
 ..., 
 [ 0.    0.    0.01 ...,  0.9   0.    0.  ]
 [  nan   nan   nan ...,   nan   nan   nan]
 [ 0.    0.    0.   ...,  0.    0.    0.  ]]
I1105 22:47:38.521081 24062 solver.cpp:228] Iteration 7500, loss = 0.55575
I1105 22:47:38.521116 24062 solver.cpp:244]     Train net output #0: accuracy = 0.820312
I1105 22:47:38.521127 24062 solver.cpp:244]     Train net output #1: loss = 0.55575 (* 1 = 0.55575 loss)
I1105 22:47:38.521134 24062 sgd_solver.cpp:106] Iteration 7500, lr = 0.01
I1105 22:53:54.057256 24062 solver.cpp:228] Iteration 7600, loss = 0.329379
I1105 22:53:54.057287 24062 solver.cpp:244]     Train net output #0: accuracy = 0.875
I1105 22:53:54.057297 24062 solver.cpp:244]     Train net output #1: loss = 0.329379 (* 1 = 0.329379 loss)
I1105 22:53:54.057306 24062 sgd_solver.cpp:106] Iteration 7600, lr = 0.01
I1105 22:59:47.090721 24062 solver.cpp:228] Iteration 7700, loss = 0.264779
I1105 22:59:47.090755 24062 solver.cpp:244]     Train net output #0: accuracy = 0.921875
I1105 22:59:47.090772 24062 solver.cpp:244]     Train net output #1: loss = 0.264779 (* 1 = 0.264779 loss)
I1105 22:59:47.090797 24062 sgd_solver.cpp:106] Iteration 7700, lr = 0.01
I1105 23:05:25.389917 24062 solver.cpp:228] Iteration 7800, loss = 0.44315
I1105 23:05:25.389950 24062 solver.cpp:244]     Train net output #0: accuracy = 0.84375
I1105 23:05:25.389961 24062 solver.cpp:244]     Train net output #1: loss = 0.44315 (* 1 = 0.44315 loss)
I1105 23:05:25.389968 24062 sgd_solver.cpp:106] Iteration 7800, lr = 0.01
I1105 23:10:59.788925 24062 solver.cpp:228] Iteration 7900, loss = 0.39746
I1105 23:10:59.788959 24062 solver.cpp:244]     Train net output #0: accuracy = 0.882812
I1105 23:10:59.788969 24062 solver.cpp:244]     Train net output #1: loss = 0.39746 (* 1 = 0.39746 loss)
I1105 23:10:59.788976 24062 sgd_solver.cpp:106] Iteration 7900, lr = 0.01
I1105 23:16:27.862414 24062 solver.cpp:454] Snapshotting to binary proto file /home/kevin/snapshot_iter_8000.caffemodel
I1105 23:16:37.726121 24062 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/kevin/snapshot_iter_8000.solverstate
>>> 2016-11-05 23:16:37.971964 Begin model classification tests
>>> 2016-11-05 23:28:10.241001 Iteration 8000 mean classification accuracy  0.866723259762
>>> 2016-11-05 23:28:10.241034 Iteration 8000 mean testing loss 0.639855268174
>>> 2016-11-05 23:28:10.241055 Iteration 8000 mean confusion matrix [[ 1.    0.    0.   ...,  0.    0.    0.  ]
 [ 0.    0.92  0.02 ...,  0.    0.    0.  ]
 [ 0.    0.    0.95 ...,  0.    0.    0.  ]
 ..., 
 [ 0.    0.    0.   ...,  0.9   0.    0.  ]
 [  nan   nan   nan ...,   nan   nan   nan]
 [ 0.    0.    0.   ...,  0.    0.    0.  ]]
I1105 23:28:15.716686 24062 solver.cpp:228] Iteration 8000, loss = 0.526379
I1105 23:28:15.716725 24062 solver.cpp:244]     Train net output #0: accuracy = 0.820312
I1105 23:28:15.716737 24062 solver.cpp:244]     Train net output #1: loss = 0.526379 (* 1 = 0.526379 loss)
I1105 23:28:15.716745 24062 sgd_solver.cpp:106] Iteration 8000, lr = 0.01
I1105 23:33:30.178074 24062 solver.cpp:228] Iteration 8100, loss = 0.347609
I1105 23:33:30.178108 24062 solver.cpp:244]     Train net output #0: accuracy = 0.90625
I1105 23:33:30.178119 24062 solver.cpp:244]     Train net output #1: loss = 0.347609 (* 1 = 0.347609 loss)
I1105 23:33:30.178127 24062 sgd_solver.cpp:106] Iteration 8100, lr = 0.01
I1105 23:38:23.450474 24062 solver.cpp:228] Iteration 8200, loss = 0.254432
I1105 23:38:23.450505 24062 solver.cpp:244]     Train net output #0: accuracy = 0.898438
I1105 23:38:23.450526 24062 solver.cpp:244]     Train net output #1: loss = 0.254432 (* 1 = 0.254432 loss)
I1105 23:38:23.450538 24062 sgd_solver.cpp:106] Iteration 8200, lr = 0.01
I1105 23:43:43.058496 24062 solver.cpp:228] Iteration 8300, loss = 0.364611
I1105 23:43:43.058531 24062 solver.cpp:244]     Train net output #0: accuracy = 0.898438
I1105 23:43:43.058540 24062 solver.cpp:244]     Train net output #1: loss = 0.364611 (* 1 = 0.364611 loss)
I1105 23:43:43.058548 24062 sgd_solver.cpp:106] Iteration 8300, lr = 0.01
I1105 23:49:18.835448 24062 solver.cpp:228] Iteration 8400, loss = 0.26377
I1105 23:49:18.835494 24062 solver.cpp:244]     Train net output #0: accuracy = 0.914062
I1105 23:49:18.835505 24062 solver.cpp:244]     Train net output #1: loss = 0.26377 (* 1 = 0.26377 loss)
I1105 23:49:18.835516 24062 sgd_solver.cpp:106] Iteration 8400, lr = 0.01
>>> 2016-11-05 23:54:55.565160 Begin model classification tests
>>> 2016-11-06 00:06:08.490494 Iteration 8500 mean classification accuracy  0.862054329372
>>> 2016-11-06 00:06:08.490528 Iteration 8500 mean testing loss 0.649541365897
>>> 2016-11-06 00:06:08.490539 Iteration 8500 mean confusion matrix [[ 1.    0.    0.   ...,  0.    0.    0.  ]
 [ 0.    0.94  0.04 ...,  0.    0.    0.  ]
 [ 0.    0.    0.95 ...,  0.    0.    0.  ]
 ..., 
 [ 0.    0.    0.01 ...,  0.95  0.    0.  ]
 [  nan   nan   nan ...,   nan   nan   nan]
 [ 0.    0.    0.   ...,  0.    0.    0.  ]]
I1106 00:06:11.916424 24062 solver.cpp:228] Iteration 8500, loss = 0.473768
I1106 00:06:11.916456 24062 solver.cpp:244]     Train net output #0: accuracy = 0.867188
I1106 00:06:11.916466 24062 solver.cpp:244]     Train net output #1: loss = 0.473768 (* 1 = 0.473768 loss)
I1106 00:06:11.916474 24062 sgd_solver.cpp:106] Iteration 8500, lr = 0.01
I1106 00:11:23.532711 24062 solver.cpp:228] Iteration 8600, loss = 0.28496
I1106 00:11:23.532750 24062 solver.cpp:244]     Train net output #0: accuracy = 0.890625
I1106 00:11:23.532760 24062 solver.cpp:244]     Train net output #1: loss = 0.28496 (* 1 = 0.28496 loss)
I1106 00:11:23.532768 24062 sgd_solver.cpp:106] Iteration 8600, lr = 0.01
I1106 00:17:17.675034 24062 solver.cpp:228] Iteration 8700, loss = 0.402657
I1106 00:17:17.675065 24062 solver.cpp:244]     Train net output #0: accuracy = 0.859375
I1106 00:17:17.675076 24062 solver.cpp:244]     Train net output #1: loss = 0.402657 (* 1 = 0.402657 loss)
I1106 00:17:17.675082 24062 sgd_solver.cpp:106] Iteration 8700, lr = 0.01
I1106 00:22:55.939020 24062 solver.cpp:228] Iteration 8800, loss = 0.375919
I1106 00:22:55.939052 24062 solver.cpp:244]     Train net output #0: accuracy = 0.851562
I1106 00:22:55.939062 24062 solver.cpp:244]     Train net output #1: loss = 0.375919 (* 1 = 0.375919 loss)
I1106 00:22:55.939070 24062 sgd_solver.cpp:106] Iteration 8800, lr = 0.01
I1106 00:28:36.253325 24062 solver.cpp:228] Iteration 8900, loss = 0.404118
I1106 00:28:36.253361 24062 solver.cpp:244]     Train net output #0: accuracy = 0.867188
I1106 00:28:36.253378 24062 solver.cpp:244]     Train net output #1: loss = 0.404118 (* 1 = 0.404118 loss)
I1106 00:28:36.253386 24062 sgd_solver.cpp:106] Iteration 8900, lr = 0.01
I1106 00:34:13.474700 24062 solver.cpp:454] Snapshotting to binary proto file /home/kevin/snapshot_iter_9000.caffemodel
I1106 00:34:23.388028 24062 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/kevin/snapshot_iter_9000.solverstate
>>> 2016-11-06 00:34:23.633125 Begin model classification tests
>>> 2016-11-06 00:45:42.598700 Iteration 9000 mean classification accuracy  0.852292020374
>>> 2016-11-06 00:45:42.598734 Iteration 9000 mean testing loss 0.659456458052
>>> 2016-11-06 00:45:42.598764 Iteration 9000 mean confusion matrix [[ 1.    0.    0.   ...,  0.    0.    0.  ]
 [ 0.    0.94  0.02 ...,  0.    0.    0.  ]
 [ 0.    0.    0.91 ...,  0.    0.    0.  ]
 ..., 
 [ 0.    0.    0.   ...,  0.8   0.    0.  ]
 [  nan   nan   nan ...,   nan   nan   nan]
 [ 0.    0.    0.   ...,  0.    0.    0.  ]]
I1106 00:45:47.342923 24062 solver.cpp:228] Iteration 9000, loss = 0.426154
I1106 00:45:47.342957 24062 solver.cpp:244]     Train net output #0: accuracy = 0.867188
I1106 00:45:47.342967 24062 solver.cpp:244]     Train net output #1: loss = 0.426154 (* 1 = 0.426154 loss)
I1106 00:45:47.342974 24062 sgd_solver.cpp:106] Iteration 9000, lr = 0.01
I1106 00:51:15.513326 24062 solver.cpp:228] Iteration 9100, loss = 0.45836
I1106 00:51:15.513360 24062 solver.cpp:244]     Train net output #0: accuracy = 0.835938
I1106 00:51:15.513370 24062 solver.cpp:244]     Train net output #1: loss = 0.45836 (* 1 = 0.45836 loss)
I1106 00:51:15.513377 24062 sgd_solver.cpp:106] Iteration 9100, lr = 0.01
I1106 00:57:00.910128 24062 solver.cpp:228] Iteration 9200, loss = 0.357829
I1106 00:57:00.910161 24062 solver.cpp:244]     Train net output #0: accuracy = 0.867188
I1106 00:57:00.910171 24062 solver.cpp:244]     Train net output #1: loss = 0.357829 (* 1 = 0.357829 loss)
I1106 00:57:00.910178 24062 sgd_solver.cpp:106] Iteration 9200, lr = 0.01
I1106 01:02:45.602730 24062 solver.cpp:228] Iteration 9300, loss = 0.411633
I1106 01:02:45.602777 24062 solver.cpp:244]     Train net output #0: accuracy = 0.84375
I1106 01:02:45.602818 24062 solver.cpp:244]     Train net output #1: loss = 0.411633 (* 1 = 0.411633 loss)
I1106 01:02:45.602838 24062 sgd_solver.cpp:106] Iteration 9300, lr = 0.01
I1106 01:08:30.813145 24062 solver.cpp:228] Iteration 9400, loss = 0.379203
I1106 01:08:30.813200 24062 solver.cpp:244]     Train net output #0: accuracy = 0.84375
I1106 01:08:30.813212 24062 solver.cpp:244]     Train net output #1: loss = 0.379203 (* 1 = 0.379203 loss)
I1106 01:08:30.813233 24062 sgd_solver.cpp:106] Iteration 9400, lr = 0.01
>>> 2016-11-06 01:13:31.522609 Begin model classification tests
>>> 2016-11-06 01:25:10.711258 Iteration 9500 mean classification accuracy  0.865874363328
>>> 2016-11-06 01:25:10.711310 Iteration 9500 mean testing loss 0.654236039379
>>> 2016-11-06 01:25:10.711343 Iteration 9500 mean confusion matrix [[ 1.    0.    0.   ...,  0.    0.    0.  ]
 [ 0.    0.92  0.02 ...,  0.    0.    0.  ]
 [ 0.    0.    0.95 ...,  0.    0.    0.  ]
 ..., 
 [ 0.    0.    0.   ...,  0.82  0.    0.  ]
 [  nan   nan   nan ...,   nan   nan   nan]
 [ 0.    0.    0.   ...,  0.    0.    0.  ]]
I1106 01:25:14.105142 24062 solver.cpp:228] Iteration 9500, loss = 0.369261
I1106 01:25:14.105180 24062 solver.cpp:244]     Train net output #0: accuracy = 0.882812
I1106 01:25:14.105190 24062 solver.cpp:244]     Train net output #1: loss = 0.369261 (* 1 = 0.369261 loss)
I1106 01:25:14.105200 24062 sgd_solver.cpp:106] Iteration 9500, lr = 0.01
I1106 01:31:03.173059 24062 solver.cpp:228] Iteration 9600, loss = 0.372997
I1106 01:31:03.173099 24062 solver.cpp:244]     Train net output #0: accuracy = 0.898438
I1106 01:31:03.173111 24062 solver.cpp:244]     Train net output #1: loss = 0.372997 (* 1 = 0.372997 loss)
I1106 01:31:03.173120 24062 sgd_solver.cpp:106] Iteration 9600, lr = 0.01
I1106 01:37:00.546486 24062 solver.cpp:228] Iteration 9700, loss = 0.29219
I1106 01:37:00.546532 24062 solver.cpp:244]     Train net output #0: accuracy = 0.929688
I1106 01:37:00.546555 24062 solver.cpp:244]     Train net output #1: loss = 0.29219 (* 1 = 0.29219 loss)
I1106 01:37:00.546562 24062 sgd_solver.cpp:106] Iteration 9700, lr = 0.01
I1106 01:42:48.736666 24062 solver.cpp:228] Iteration 9800, loss = 0.612265
I1106 01:42:48.736701 24062 solver.cpp:244]     Train net output #0: accuracy = 0.820312
I1106 01:42:48.736712 24062 solver.cpp:244]     Train net output #1: loss = 0.612265 (* 1 = 0.612265 loss)
I1106 01:42:48.736730 24062 sgd_solver.cpp:106] Iteration 9800, lr = 0.01
I1106 01:48:29.660372 24062 solver.cpp:228] Iteration 9900, loss = 0.300728
I1106 01:48:29.660406 24062 solver.cpp:244]     Train net output #0: accuracy = 0.890625
I1106 01:48:29.660416 24062 solver.cpp:244]     Train net output #1: loss = 0.300728 (* 1 = 0.300728 loss)
I1106 01:48:29.660423 24062 sgd_solver.cpp:106] Iteration 9900, lr = 0.01
I1106 01:53:26.290608 24062 solver.cpp:454] Snapshotting to binary proto file /home/kevin/snapshot_iter_10000.caffemodel
I1106 01:53:35.593897 24062 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/kevin/snapshot_iter_10000.solverstate
>>> 2016-11-06 01:53:35.849823 Begin model classification tests
>>> 2016-11-06 02:05:48.750157 Iteration 10000 mean classification accuracy  0.854838709677
>>> 2016-11-06 02:05:48.750242 Iteration 10000 mean testing loss 0.643240456886
>>> 2016-11-06 02:05:48.750278 Iteration 10000 mean confusion matrix [[ 1.    0.    0.   ...,  0.    0.    0.  ]
 [ 0.    0.88  0.02 ...,  0.    0.    0.  ]
 [ 0.    0.    0.94 ...,  0.    0.    0.  ]
 ..., 
 [ 0.    0.    0.   ...,  0.85  0.    0.  ]
 [  nan   nan   nan ...,   nan   nan   nan]
 [ 0.    0.    0.   ...,  0.    0.    0.  ]]
I1106 02:05:52.977996 24062 solver.cpp:228] Iteration 10000, loss = 0.383159
I1106 02:05:52.978047 24062 solver.cpp:244]     Train net output #0: accuracy = 0.867188
I1106 02:05:52.978057 24062 solver.cpp:244]     Train net output #1: loss = 0.383159 (* 1 = 0.383159 loss)
I1106 02:05:52.978076 24062 sgd_solver.cpp:106] Iteration 10000, lr = 0.001
I1106 02:11:11.630616 24062 solver.cpp:228] Iteration 10100, loss = 0.381787
I1106 02:11:11.630661 24062 solver.cpp:244]     Train net output #0: accuracy = 0.882812
I1106 02:11:11.630681 24062 solver.cpp:244]     Train net output #1: loss = 0.381787 (* 1 = 0.381787 loss)
I1106 02:11:11.630692 24062 sgd_solver.cpp:106] Iteration 10100, lr = 0.001
I1106 02:16:59.776721 24062 solver.cpp:228] Iteration 10200, loss = 0.392709
I1106 02:16:59.776767 24062 solver.cpp:244]     Train net output #0: accuracy = 0.875
I1106 02:16:59.776779 24062 solver.cpp:244]     Train net output #1: loss = 0.392709 (* 1 = 0.392709 loss)
I1106 02:16:59.776799 24062 sgd_solver.cpp:106] Iteration 10200, lr = 0.001
I1106 02:22:03.024400 24062 solver.cpp:228] Iteration 10300, loss = 0.196348
I1106 02:22:03.024467 24062 solver.cpp:244]     Train net output #0: accuracy = 0.9375
I1106 02:22:03.024477 24062 solver.cpp:244]     Train net output #1: loss = 0.196348 (* 1 = 0.196348 loss)
I1106 02:22:03.024497 24062 sgd_solver.cpp:106] Iteration 10300, lr = 0.001
I1106 02:27:45.376395 24062 solver.cpp:228] Iteration 10400, loss = 0.227721
I1106 02:27:45.376433 24062 solver.cpp:244]     Train net output #0: accuracy = 0.914062
I1106 02:27:45.376448 24062 solver.cpp:244]     Train net output #1: loss = 0.227721 (* 1 = 0.227721 loss)
I1106 02:27:45.376458 24062 sgd_solver.cpp:106] Iteration 10400, lr = 0.001
>>> 2016-11-06 02:33:08.882132 Begin model classification tests
>>> 2016-11-06 02:45:23.337263 Iteration 10500 mean classification accuracy  0.869269949066
>>> 2016-11-06 02:45:23.337321 Iteration 10500 mean testing loss 0.597490766573
>>> 2016-11-06 02:45:23.337348 Iteration 10500 mean confusion matrix [[ 1.    0.    0.   ...,  0.    0.    0.  ]
 [ 0.    0.94  0.02 ...,  0.    0.    0.  ]
 [ 0.    0.    0.94 ...,  0.    0.    0.  ]
 ..., 
 [ 0.    0.    0.   ...,  0.85  0.    0.  ]
 [  nan   nan   nan ...,   nan   nan   nan]
 [ 0.    0.    0.   ...,  0.    0.    0.  ]]
I1106 02:45:26.472962 24062 solver.cpp:228] Iteration 10500, loss = 0.271148
I1106 02:45:26.473019 24062 solver.cpp:244]     Train net output #0: accuracy = 0.90625
I1106 02:45:26.473032 24062 solver.cpp:244]     Train net output #1: loss = 0.271148 (* 1 = 0.271148 loss)
I1106 02:45:26.473052 24062 sgd_solver.cpp:106] Iteration 10500, lr = 0.001
I1106 02:50:40.056829 24062 solver.cpp:228] Iteration 10600, loss = 0.222917
I1106 02:50:40.056874 24062 solver.cpp:244]     Train net output #0: accuracy = 0.90625
I1106 02:50:40.056886 24062 solver.cpp:244]     Train net output #1: loss = 0.222917 (* 1 = 0.222917 loss)
I1106 02:50:40.056906 24062 sgd_solver.cpp:106] Iteration 10600, lr = 0.001
I1106 02:56:27.529078 24062 solver.cpp:228] Iteration 10700, loss = 0.272223
I1106 02:56:27.529119 24062 solver.cpp:244]     Train net output #0: accuracy = 0.890625
I1106 02:56:27.529146 24062 solver.cpp:244]     Train net output #1: loss = 0.272223 (* 1 = 0.272223 loss)
I1106 02:56:27.529173 24062 sgd_solver.cpp:106] Iteration 10700, lr = 0.001
I1106 03:01:31.230864 24062 solver.cpp:228] Iteration 10800, loss = 0.147405
I1106 03:01:31.230911 24062 solver.cpp:244]     Train net output #0: accuracy = 0.976562
I1106 03:01:31.230921 24062 solver.cpp:244]     Train net output #1: loss = 0.147405 (* 1 = 0.147405 loss)
I1106 03:01:31.230942 24062 sgd_solver.cpp:106] Iteration 10800, lr = 0.001
I1106 03:06:32.711884 24062 solver.cpp:228] Iteration 10900, loss = 0.275885
I1106 03:06:32.711930 24062 solver.cpp:244]     Train net output #0: accuracy = 0.921875
I1106 03:06:32.711941 24062 solver.cpp:244]     Train net output #1: loss = 0.275885 (* 1 = 0.275885 loss)
I1106 03:06:32.711961 24062 sgd_solver.cpp:106] Iteration 10900, lr = 0.001
I1106 03:11:22.307281 24062 solver.cpp:454] Snapshotting to binary proto file /home/kevin/snapshot_iter_11000.caffemodel
I1106 03:11:24.518584 24062 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/kevin/snapshot_iter_11000.solverstate
>>> 2016-11-06 03:11:24.783085 Begin model classification tests
>>> 2016-11-06 03:23:34.526757 Iteration 11000 mean classification accuracy  0.873089983022
>>> 2016-11-06 03:23:34.526807 Iteration 11000 mean testing loss 0.591951384474
>>> 2016-11-06 03:23:34.526826 Iteration 11000 mean confusion matrix [[ 1.    0.    0.   ...,  0.    0.    0.  ]
 [ 0.    0.94  0.02 ...,  0.    0.    0.  ]
 [ 0.    0.    0.95 ...,  0.    0.    0.  ]
 ..., 
 [ 0.    0.    0.   ...,  0.87  0.    0.  ]
 [  nan   nan   nan ...,   nan   nan   nan]
 [ 0.    0.    0.   ...,  0.    0.    0.  ]]
I1106 03:23:38.953630 24062 solver.cpp:228] Iteration 11000, loss = 0.225106
I1106 03:23:38.953666 24062 solver.cpp:244]     Train net output #0: accuracy = 0.90625
I1106 03:23:38.953676 24062 solver.cpp:244]     Train net output #1: loss = 0.225106 (* 1 = 0.225106 loss)
I1106 03:23:38.953696 24062 sgd_solver.cpp:106] Iteration 11000, lr = 0.001
I1106 03:28:35.503594 24062 solver.cpp:228] Iteration 11100, loss = 0.281446
I1106 03:28:35.503639 24062 solver.cpp:244]     Train net output #0: accuracy = 0.90625
I1106 03:28:35.503653 24062 solver.cpp:244]     Train net output #1: loss = 0.281446 (* 1 = 0.281446 loss)
I1106 03:28:35.503674 24062 sgd_solver.cpp:106] Iteration 11100, lr = 0.001
I1106 03:34:15.070777 24062 solver.cpp:228] Iteration 11200, loss = 0.25505
I1106 03:34:15.070816 24062 solver.cpp:244]     Train net output #0: accuracy = 0.90625
I1106 03:34:15.070827 24062 solver.cpp:244]     Train net output #1: loss = 0.25505 (* 1 = 0.25505 loss)
I1106 03:34:15.070834 24062 sgd_solver.cpp:106] Iteration 11200, lr = 0.001
I1106 03:40:16.506858 24062 solver.cpp:228] Iteration 11300, loss = 0.204722
I1106 03:40:16.506911 24062 solver.cpp:244]     Train net output #0: accuracy = 0.945312
I1106 03:40:16.506940 24062 solver.cpp:244]     Train net output #1: loss = 0.204722 (* 1 = 0.204722 loss)
I1106 03:40:16.506953 24062 sgd_solver.cpp:106] Iteration 11300, lr = 0.001
I1106 03:45:16.472472 24062 solver.cpp:228] Iteration 11400, loss = 0.300038
I1106 03:45:16.472517 24062 solver.cpp:244]     Train net output #0: accuracy = 0.90625
I1106 03:45:16.472527 24062 solver.cpp:244]     Train net output #1: loss = 0.300038 (* 1 = 0.300038 loss)
I1106 03:45:16.472545 24062 sgd_solver.cpp:106] Iteration 11400, lr = 0.001
>>> 2016-11-06 03:50:28.967984 Begin model classification tests
>>> 2016-11-06 04:07:45.196642 Iteration 11500 mean classification accuracy  0.873089983022
>>> 2016-11-06 04:07:45.196697 Iteration 11500 mean testing loss 0.601922292956
>>> 2016-11-06 04:07:45.196728 Iteration 11500 mean confusion matrix [[ 1.    0.    0.   ...,  0.    0.    0.  ]
 [ 0.    0.9   0.02 ...,  0.    0.    0.  ]
 [ 0.    0.    0.95 ...,  0.    0.    0.  ]
 ..., 
 [ 0.    0.    0.   ...,  0.9   0.    0.  ]
 [  nan   nan   nan ...,   nan   nan   nan]
 [ 0.    0.    0.   ...,  0.    0.    0.  ]]
I1106 04:07:49.426411 24062 solver.cpp:228] Iteration 11500, loss = 0.135946
I1106 04:07:49.426460 24062 solver.cpp:244]     Train net output #0: accuracy = 0.953125
I1106 04:07:49.426501 24062 solver.cpp:244]     Train net output #1: loss = 0.135946 (* 1 = 0.135946 loss)
I1106 04:07:49.426522 24062 sgd_solver.cpp:106] Iteration 11500, lr = 0.001
^CTraceback (most recent call last):
  File "./solve.py", line 27, in <module>
    solver.step(100)
  File "/home/kevin/caffeplus/python_layer/data_layers/model_net_layer.py", line 69, in reshape
    self.data, self.label = self.load_data(self.indices, self.idx)
  File "/home/kevin/caffeplus/python_layer/data_layers/model_net_layer.py", line 112, in load_data
    mat = scipy.io.loadmat('{}/{}.mat'.format(self.dataset_dir, filei))
  File "/usr/lib/python2.7/dist-packages/scipy/io/matlab/mio.py", line 126, in loadmat
    matfile_dict = MR.get_variables(variable_names)
  File "/usr/lib/python2.7/dist-packages/scipy/io/matlab/mio5.py", line 288, in get_variables
    res = self.read_var_array(hdr, process)
  File "/usr/lib/python2.7/dist-packages/scipy/io/matlab/mio5.py", line 248, in read_var_array
    return self._matrix_reader.array_from_header(header, process)
KeyboardInterrupt
]0;kevin@kevin-desktop: ~/catkin_ws/src/romans_stack/model_net/caffe_netkevin@kevin-desktop:~/catkin_ws/src/romans_stack/model_net/caffe_net$ ./solve.py[4Ppythongit statuspush origin kevin-dev status[K[4Ppython./solve.py[K。、cr[K[K[K[K./create_net.py 
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Net<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Blob<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Solver<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
]0;kevin@kevin-desktop: ~/catkin_ws/src/romans_stack/model_net/caffe_netkevin@kevin-desktop:~/catkin_ws/src/romans_stack/model_net/caffe_net$ ./create_net.py [6Psolve.py
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Net<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Blob<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Solver<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1106 04:09:14.190223 16758 solver.cpp:48] Initializing solver from parameters: 
train_net: "train.prototxt"
test_net: "test.prototxt"
test_iter: 0
test_interval: 9999999
base_lr: 0.01
display: 100
max_iter: 450000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 5000
snapshot: 1000
snapshot_prefix: "/home/kevin/snapshot/"
solver_mode: GPU
I1106 04:09:14.217525 16758 solver.cpp:81] Creating training net from train_net file: train.prototxt
I1106 04:09:14.231990 16758 net.cpp:49] Initializing net from parameters: 
state {
  phase: TRAIN
}
layer {
  name: "img"
  type: "Python"
  top: "img"
  top: "label"
  python_param {
    module: "data_layers.model_net_layer"
    layer: "ModelNetDataLayer"
    param_str: "{\'img_size\': (250, 250), \'dtype\': \'frame\', \'batch_size\': 256, \'seed\': 1337, \'split\': \'train\', \'dataset_dir\': \'/home/kevin/dataset/processed_data\', \'mean\': 2}"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "img"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 0
    kernel_size: 11
    group: 1
    stride: 4
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    name: "conv3_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv3_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    name: "conv4_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv4_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv4"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 40
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1106 04:09:14.237421 16758 layer_factory.hpp:77] Creating layer img
I1106 04:09:14.595166 16758 net.cpp:91] Creating Layer img
I1106 04:09:14.595226 16758 net.cpp:399] img -> img
I1106 04:09:14.595263 16758 net.cpp:399] img -> label
{'img_size': (250, 250), 'dtype': 'frame', 'batch_size': 256, 'seed': 1337, 'split': 'train', 'dataset_dir': '/home/kevin/dataset/processed_data', 'mean': 2}
I1106 04:09:44.356123 16758 net.cpp:141] Setting up img
I1106 04:09:44.356151 16758 net.cpp:148] Top shape: 256 1 250 250 (16000000)
I1106 04:09:44.356158 16758 net.cpp:148] Top shape: 256 1 (256)
I1106 04:09:44.356161 16758 net.cpp:156] Memory required for data: 64001024
I1106 04:09:44.356168 16758 layer_factory.hpp:77] Creating layer label_img_1_split
I1106 04:09:44.356184 16758 net.cpp:91] Creating Layer label_img_1_split
I1106 04:09:44.356189 16758 net.cpp:425] label_img_1_split <- label
I1106 04:09:44.356196 16758 net.cpp:399] label_img_1_split -> label_img_1_split_0
I1106 04:09:44.356206 16758 net.cpp:399] label_img_1_split -> label_img_1_split_1
I1106 04:09:44.356245 16758 net.cpp:141] Setting up label_img_1_split
I1106 04:09:44.356251 16758 net.cpp:148] Top shape: 256 1 (256)
I1106 04:09:44.356254 16758 net.cpp:148] Top shape: 256 1 (256)
I1106 04:09:44.356258 16758 net.cpp:156] Memory required for data: 64003072
I1106 04:09:44.356261 16758 layer_factory.hpp:77] Creating layer conv1
I1106 04:09:44.356273 16758 net.cpp:91] Creating Layer conv1
I1106 04:09:44.356277 16758 net.cpp:425] conv1 <- img
I1106 04:09:44.356283 16758 net.cpp:399] conv1 -> conv1
I1106 04:09:44.371728 16758 net.cpp:141] Setting up conv1
I1106 04:09:44.371752 16758 net.cpp:148] Top shape: 256 96 60 60 (88473600)
I1106 04:09:44.371757 16758 net.cpp:156] Memory required for data: 417897472
I1106 04:09:44.371770 16758 layer_factory.hpp:77] Creating layer relu1
I1106 04:09:44.371779 16758 net.cpp:91] Creating Layer relu1
I1106 04:09:44.371783 16758 net.cpp:425] relu1 <- conv1
I1106 04:09:44.371788 16758 net.cpp:386] relu1 -> conv1 (in-place)
I1106 04:09:44.371800 16758 net.cpp:141] Setting up relu1
I1106 04:09:44.371805 16758 net.cpp:148] Top shape: 256 96 60 60 (88473600)
I1106 04:09:44.371809 16758 net.cpp:156] Memory required for data: 771791872
I1106 04:09:44.371812 16758 layer_factory.hpp:77] Creating layer pool1
I1106 04:09:44.371820 16758 net.cpp:91] Creating Layer pool1
I1106 04:09:44.371824 16758 net.cpp:425] pool1 <- conv1
I1106 04:09:44.371829 16758 net.cpp:399] pool1 -> pool1
I1106 04:09:44.371867 16758 net.cpp:141] Setting up pool1
I1106 04:09:44.371873 16758 net.cpp:148] Top shape: 256 96 30 30 (22118400)
I1106 04:09:44.371876 16758 net.cpp:156] Memory required for data: 860265472
I1106 04:09:44.371879 16758 layer_factory.hpp:77] Creating layer norm1
I1106 04:09:44.371887 16758 net.cpp:91] Creating Layer norm1
I1106 04:09:44.371891 16758 net.cpp:425] norm1 <- pool1
I1106 04:09:44.371896 16758 net.cpp:399] norm1 -> norm1
I1106 04:09:44.371923 16758 net.cpp:141] Setting up norm1
I1106 04:09:44.371928 16758 net.cpp:148] Top shape: 256 96 30 30 (22118400)
I1106 04:09:44.371932 16758 net.cpp:156] Memory required for data: 948739072
I1106 04:09:44.371935 16758 layer_factory.hpp:77] Creating layer conv2
I1106 04:09:44.371944 16758 net.cpp:91] Creating Layer conv2
I1106 04:09:44.371948 16758 net.cpp:425] conv2 <- norm1
I1106 04:09:44.371953 16758 net.cpp:399] conv2 -> conv2
I1106 04:09:44.374692 16758 net.cpp:141] Setting up conv2
I1106 04:09:44.374703 16758 net.cpp:148] Top shape: 256 256 30 30 (58982400)
I1106 04:09:44.374707 16758 net.cpp:156] Memory required for data: 1184668672
I1106 04:09:44.374716 16758 layer_factory.hpp:77] Creating layer relu2
I1106 04:09:44.374722 16758 net.cpp:91] Creating Layer relu2
I1106 04:09:44.374727 16758 net.cpp:425] relu2 <- conv2
I1106 04:09:44.374732 16758 net.cpp:386] relu2 -> conv2 (in-place)
I1106 04:09:44.374737 16758 net.cpp:141] Setting up relu2
I1106 04:09:44.374742 16758 net.cpp:148] Top shape: 256 256 30 30 (58982400)
I1106 04:09:44.374745 16758 net.cpp:156] Memory required for data: 1420598272
I1106 04:09:44.374748 16758 layer_factory.hpp:77] Creating layer pool2
I1106 04:09:44.374755 16758 net.cpp:91] Creating Layer pool2
I1106 04:09:44.374758 16758 net.cpp:425] pool2 <- conv2
I1106 04:09:44.374763 16758 net.cpp:399] pool2 -> pool2
I1106 04:09:44.374797 16758 net.cpp:141] Setting up pool2
I1106 04:09:44.374804 16758 net.cpp:148] Top shape: 256 256 15 15 (14745600)
I1106 04:09:44.374807 16758 net.cpp:156] Memory required for data: 1479580672
I1106 04:09:44.374810 16758 layer_factory.hpp:77] Creating layer norm2
I1106 04:09:44.374816 16758 net.cpp:91] Creating Layer norm2
I1106 04:09:44.374819 16758 net.cpp:425] norm2 <- pool2
I1106 04:09:44.374825 16758 net.cpp:399] norm2 -> norm2
I1106 04:09:44.374851 16758 net.cpp:141] Setting up norm2
I1106 04:09:44.374856 16758 net.cpp:148] Top shape: 256 256 15 15 (14745600)
I1106 04:09:44.374860 16758 net.cpp:156] Memory required for data: 1538563072
I1106 04:09:44.374863 16758 layer_factory.hpp:77] Creating layer conv3
I1106 04:09:44.374871 16758 net.cpp:91] Creating Layer conv3
I1106 04:09:44.374874 16758 net.cpp:425] conv3 <- norm2
I1106 04:09:44.374881 16758 net.cpp:399] conv3 -> conv3
I1106 04:09:44.378144 16758 net.cpp:141] Setting up conv3
I1106 04:09:44.378154 16758 net.cpp:148] Top shape: 256 384 15 15 (22118400)
I1106 04:09:44.378159 16758 net.cpp:156] Memory required for data: 1627036672
I1106 04:09:44.378166 16758 layer_factory.hpp:77] Creating layer relu3
I1106 04:09:44.378172 16758 net.cpp:91] Creating Layer relu3
I1106 04:09:44.378176 16758 net.cpp:425] relu3 <- conv3
I1106 04:09:44.378181 16758 net.cpp:386] relu3 -> conv3 (in-place)
I1106 04:09:44.378187 16758 net.cpp:141] Setting up relu3
I1106 04:09:44.378192 16758 net.cpp:148] Top shape: 256 384 15 15 (22118400)
I1106 04:09:44.378196 16758 net.cpp:156] Memory required for data: 1715510272
I1106 04:09:44.378199 16758 layer_factory.hpp:77] Creating layer conv4
I1106 04:09:44.378207 16758 net.cpp:91] Creating Layer conv4
I1106 04:09:44.378211 16758 net.cpp:425] conv4 <- conv3
I1106 04:09:44.378216 16758 net.cpp:399] conv4 -> conv4
I1106 04:09:44.381542 16758 net.cpp:141] Setting up conv4
I1106 04:09:44.381580 16758 net.cpp:148] Top shape: 256 256 15 15 (14745600)
I1106 04:09:44.381585 16758 net.cpp:156] Memory required for data: 1774492672
I1106 04:09:44.381598 16758 layer_factory.hpp:77] Creating layer relu4
I1106 04:09:44.381613 16758 net.cpp:91] Creating Layer relu4
I1106 04:09:44.381618 16758 net.cpp:425] relu4 <- conv4
I1106 04:09:44.381626 16758 net.cpp:386] relu4 -> conv4 (in-place)
I1106 04:09:44.381636 16758 net.cpp:141] Setting up relu4
I1106 04:09:44.381641 16758 net.cpp:148] Top shape: 256 256 15 15 (14745600)
I1106 04:09:44.381644 16758 net.cpp:156] Memory required for data: 1833475072
I1106 04:09:44.381649 16758 layer_factory.hpp:77] Creating layer pool5
I1106 04:09:44.381657 16758 net.cpp:91] Creating Layer pool5
I1106 04:09:44.381660 16758 net.cpp:425] pool5 <- conv4
I1106 04:09:44.381665 16758 net.cpp:399] pool5 -> pool5
I1106 04:09:44.381707 16758 net.cpp:141] Setting up pool5
I1106 04:09:44.381713 16758 net.cpp:148] Top shape: 256 256 7 7 (3211264)
I1106 04:09:44.381716 16758 net.cpp:156] Memory required for data: 1846320128
I1106 04:09:44.381721 16758 layer_factory.hpp:77] Creating layer fc6
I1106 04:09:44.381742 16758 net.cpp:91] Creating Layer fc6
I1106 04:09:44.381747 16758 net.cpp:425] fc6 <- pool5
I1106 04:09:44.381752 16758 net.cpp:399] fc6 -> fc6
I1106 04:09:44.708454 16758 net.cpp:141] Setting up fc6
I1106 04:09:44.708492 16758 net.cpp:148] Top shape: 256 4096 (1048576)
I1106 04:09:44.708503 16758 net.cpp:156] Memory required for data: 1850514432
I1106 04:09:44.708523 16758 layer_factory.hpp:77] Creating layer relu6
I1106 04:09:44.708539 16758 net.cpp:91] Creating Layer relu6
I1106 04:09:44.708549 16758 net.cpp:425] relu6 <- fc6
I1106 04:09:44.708559 16758 net.cpp:386] relu6 -> fc6 (in-place)
I1106 04:09:44.708571 16758 net.cpp:141] Setting up relu6
I1106 04:09:44.708580 16758 net.cpp:148] Top shape: 256 4096 (1048576)
I1106 04:09:44.708587 16758 net.cpp:156] Memory required for data: 1854708736
I1106 04:09:44.708595 16758 layer_factory.hpp:77] Creating layer drop6
I1106 04:09:44.708611 16758 net.cpp:91] Creating Layer drop6
I1106 04:09:44.708618 16758 net.cpp:425] drop6 <- fc6
I1106 04:09:44.708626 16758 net.cpp:386] drop6 -> fc6 (in-place)
I1106 04:09:44.708647 16758 net.cpp:141] Setting up drop6
I1106 04:09:44.708654 16758 net.cpp:148] Top shape: 256 4096 (1048576)
I1106 04:09:44.708662 16758 net.cpp:156] Memory required for data: 1858903040
I1106 04:09:44.708668 16758 layer_factory.hpp:77] Creating layer fc7
I1106 04:09:44.708678 16758 net.cpp:91] Creating Layer fc7
I1106 04:09:44.708684 16758 net.cpp:425] fc7 <- fc6
I1106 04:09:44.708693 16758 net.cpp:399] fc7 -> fc7
I1106 04:09:44.799295 16758 net.cpp:141] Setting up fc7
I1106 04:09:44.799337 16758 net.cpp:148] Top shape: 256 4096 (1048576)
I1106 04:09:44.799342 16758 net.cpp:156] Memory required for data: 1863097344
I1106 04:09:44.799351 16758 layer_factory.hpp:77] Creating layer relu7
I1106 04:09:44.799362 16758 net.cpp:91] Creating Layer relu7
I1106 04:09:44.799368 16758 net.cpp:425] relu7 <- fc7
I1106 04:09:44.799376 16758 net.cpp:386] relu7 -> fc7 (in-place)
I1106 04:09:44.799384 16758 net.cpp:141] Setting up relu7
I1106 04:09:44.799391 16758 net.cpp:148] Top shape: 256 4096 (1048576)
I1106 04:09:44.799396 16758 net.cpp:156] Memory required for data: 1867291648
I1106 04:09:44.799401 16758 layer_factory.hpp:77] Creating layer drop7
I1106 04:09:44.799408 16758 net.cpp:91] Creating Layer drop7
I1106 04:09:44.799412 16758 net.cpp:425] drop7 <- fc7
I1106 04:09:44.799419 16758 net.cpp:386] drop7 -> fc7 (in-place)
I1106 04:09:44.799432 16758 net.cpp:141] Setting up drop7
I1106 04:09:44.799438 16758 net.cpp:148] Top shape: 256 4096 (1048576)
I1106 04:09:44.799443 16758 net.cpp:156] Memory required for data: 1871485952
I1106 04:09:44.799448 16758 layer_factory.hpp:77] Creating layer fc8
I1106 04:09:44.799454 16758 net.cpp:91] Creating Layer fc8
I1106 04:09:44.799458 16758 net.cpp:425] fc8 <- fc7
I1106 04:09:44.799463 16758 net.cpp:399] fc8 -> fc8
I1106 04:09:44.800544 16758 net.cpp:141] Setting up fc8
I1106 04:09:44.800565 16758 net.cpp:148] Top shape: 256 40 (10240)
I1106 04:09:44.800570 16758 net.cpp:156] Memory required for data: 1871526912
I1106 04:09:44.800575 16758 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I1106 04:09:44.800582 16758 net.cpp:91] Creating Layer fc8_fc8_0_split
I1106 04:09:44.800587 16758 net.cpp:425] fc8_fc8_0_split <- fc8
I1106 04:09:44.800595 16758 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1106 04:09:44.800603 16758 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1106 04:09:44.800623 16758 net.cpp:141] Setting up fc8_fc8_0_split
I1106 04:09:44.800631 16758 net.cpp:148] Top shape: 256 40 (10240)
I1106 04:09:44.800635 16758 net.cpp:148] Top shape: 256 40 (10240)
I1106 04:09:44.800639 16758 net.cpp:156] Memory required for data: 1871608832
I1106 04:09:44.800644 16758 layer_factory.hpp:77] Creating layer accuracy
I1106 04:09:44.800657 16758 net.cpp:91] Creating Layer accuracy
I1106 04:09:44.800660 16758 net.cpp:425] accuracy <- fc8_fc8_0_split_0
I1106 04:09:44.800667 16758 net.cpp:425] accuracy <- label_img_1_split_0
I1106 04:09:44.800671 16758 net.cpp:399] accuracy -> accuracy
I1106 04:09:44.800678 16758 net.cpp:141] Setting up accuracy
I1106 04:09:44.800683 16758 net.cpp:148] Top shape: (1)
I1106 04:09:44.800688 16758 net.cpp:156] Memory required for data: 1871608836
I1106 04:09:44.800693 16758 layer_factory.hpp:77] Creating layer loss
I1106 04:09:44.800701 16758 net.cpp:91] Creating Layer loss
I1106 04:09:44.800705 16758 net.cpp:425] loss <- fc8_fc8_0_split_1
I1106 04:09:44.800710 16758 net.cpp:425] loss <- label_img_1_split_1
I1106 04:09:44.800716 16758 net.cpp:399] loss -> loss
I1106 04:09:44.800724 16758 layer_factory.hpp:77] Creating layer loss
I1106 04:09:44.800784 16758 net.cpp:141] Setting up loss
I1106 04:09:44.800791 16758 net.cpp:148] Top shape: (1)
I1106 04:09:44.800796 16758 net.cpp:151]     with loss weight 1
I1106 04:09:44.800806 16758 net.cpp:156] Memory required for data: 1871608840
I1106 04:09:44.800810 16758 net.cpp:217] loss needs backward computation.
I1106 04:09:44.800815 16758 net.cpp:219] accuracy does not need backward computation.
I1106 04:09:44.800820 16758 net.cpp:217] fc8_fc8_0_split needs backward computation.
I1106 04:09:44.800824 16758 net.cpp:217] fc8 needs backward computation.
I1106 04:09:44.800829 16758 net.cpp:217] drop7 needs backward computation.
I1106 04:09:44.800833 16758 net.cpp:217] relu7 needs backward computation.
I1106 04:09:44.800837 16758 net.cpp:217] fc7 needs backward computation.
I1106 04:09:44.800843 16758 net.cpp:217] drop6 needs backward computation.
I1106 04:09:44.800848 16758 net.cpp:217] relu6 needs backward computation.
I1106 04:09:44.800853 16758 net.cpp:217] fc6 needs backward computation.
I1106 04:09:44.800858 16758 net.cpp:217] pool5 needs backward computation.
I1106 04:09:44.800863 16758 net.cpp:217] relu4 needs backward computation.
I1106 04:09:44.800866 16758 net.cpp:217] conv4 needs backward computation.
I1106 04:09:44.800871 16758 net.cpp:217] relu3 needs backward computation.
I1106 04:09:44.800876 16758 net.cpp:217] conv3 needs backward computation.
I1106 04:09:44.800881 16758 net.cpp:217] norm2 needs backward computation.
I1106 04:09:44.800885 16758 net.cpp:217] pool2 needs backward computation.
I1106 04:09:44.800889 16758 net.cpp:217] relu2 needs backward computation.
I1106 04:09:44.800894 16758 net.cpp:217] conv2 needs backward computation.
I1106 04:09:44.800899 16758 net.cpp:217] norm1 needs backward computation.
I1106 04:09:44.800904 16758 net.cpp:217] pool1 needs backward computation.
I1106 04:09:44.800907 16758 net.cpp:217] relu1 needs backward computation.
I1106 04:09:44.800912 16758 net.cpp:217] conv1 needs backward computation.
I1106 04:09:44.800917 16758 net.cpp:219] label_img_1_split does not need backward computation.
I1106 04:09:44.800922 16758 net.cpp:219] img does not need backward computation.
I1106 04:09:44.800926 16758 net.cpp:261] This network produces output accuracy
I1106 04:09:44.800930 16758 net.cpp:261] This network produces output loss
I1106 04:09:44.800942 16758 net.cpp:274] Network initialization done.
I1106 04:09:44.801252 16758 solver.cpp:181] Creating test net (#0) specified by test_net file: test.prototxt
I1106 04:09:44.801365 16758 net.cpp:49] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "img"
  type: "Python"
  top: "img"
  top: "label"
  python_param {
    module: "data_layers.model_net_layer"
    layer: "ModelNetDataLayer"
    param_str: "{\'img_size\': (250, 250), \'dtype\': \'object\', \'batch_size\': 256, \'seed\': 1337, \'split\': \'test\', \'dataset_dir\': \'/home/kevin/dataset/processed_data\', \'mean\': 2}"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "img"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 0
    kernel_size: 11
    group: 1
    stride: 4
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    name: "conv3_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv3_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    name: "conv4_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv4_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv4"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 40
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1106 04:09:44.802014 16758 layer_factory.hpp:77] Creating layer img
I1106 04:09:44.802052 16758 net.cpp:91] Creating Layer img
I1106 04:09:44.802059 16758 net.cpp:399] img -> img
I1106 04:09:44.802067 16758 net.cpp:399] img -> label
{'img_size': (250, 250), 'dtype': 'object', 'batch_size': 256, 'seed': 1337, 'split': 'test', 'dataset_dir': '/home/kevin/dataset/processed_data', 'mean': 2}
I1106 04:09:45.173903 16758 net.cpp:141] Setting up img
I1106 04:09:45.173950 16758 net.cpp:148] Top shape: 24 1 250 250 (1500000)
I1106 04:09:45.173962 16758 net.cpp:148] Top shape: 24 1 (24)
I1106 04:09:45.173970 16758 net.cpp:156] Memory required for data: 6000096
I1106 04:09:45.173980 16758 layer_factory.hpp:77] Creating layer label_img_1_split
I1106 04:09:45.173997 16758 net.cpp:91] Creating Layer label_img_1_split
I1106 04:09:45.174006 16758 net.cpp:425] label_img_1_split <- label
I1106 04:09:45.174017 16758 net.cpp:399] label_img_1_split -> label_img_1_split_0
I1106 04:09:45.174031 16758 net.cpp:399] label_img_1_split -> label_img_1_split_1
I1106 04:09:45.174069 16758 net.cpp:141] Setting up label_img_1_split
I1106 04:09:45.174080 16758 net.cpp:148] Top shape: 24 1 (24)
I1106 04:09:45.174088 16758 net.cpp:148] Top shape: 24 1 (24)
I1106 04:09:45.174095 16758 net.cpp:156] Memory required for data: 6000288
I1106 04:09:45.174103 16758 layer_factory.hpp:77] Creating layer conv1
I1106 04:09:45.174118 16758 net.cpp:91] Creating Layer conv1
I1106 04:09:45.174125 16758 net.cpp:425] conv1 <- img
I1106 04:09:45.174134 16758 net.cpp:399] conv1 -> conv1
I1106 04:09:45.174407 16758 net.cpp:141] Setting up conv1
I1106 04:09:45.174420 16758 net.cpp:148] Top shape: 24 96 60 60 (8294400)
I1106 04:09:45.174428 16758 net.cpp:156] Memory required for data: 39177888
I1106 04:09:45.174440 16758 layer_factory.hpp:77] Creating layer relu1
I1106 04:09:45.174451 16758 net.cpp:91] Creating Layer relu1
I1106 04:09:45.174458 16758 net.cpp:425] relu1 <- conv1
I1106 04:09:45.174466 16758 net.cpp:386] relu1 -> conv1 (in-place)
I1106 04:09:45.174476 16758 net.cpp:141] Setting up relu1
I1106 04:09:45.174484 16758 net.cpp:148] Top shape: 24 96 60 60 (8294400)
I1106 04:09:45.174491 16758 net.cpp:156] Memory required for data: 72355488
I1106 04:09:45.174499 16758 layer_factory.hpp:77] Creating layer pool1
I1106 04:09:45.174510 16758 net.cpp:91] Creating Layer pool1
I1106 04:09:45.174516 16758 net.cpp:425] pool1 <- conv1
I1106 04:09:45.174525 16758 net.cpp:399] pool1 -> pool1
I1106 04:09:45.174561 16758 net.cpp:141] Setting up pool1
I1106 04:09:45.174571 16758 net.cpp:148] Top shape: 24 96 30 30 (2073600)
I1106 04:09:45.174577 16758 net.cpp:156] Memory required for data: 80649888
I1106 04:09:45.174584 16758 layer_factory.hpp:77] Creating layer norm1
I1106 04:09:45.174595 16758 net.cpp:91] Creating Layer norm1
I1106 04:09:45.174602 16758 net.cpp:425] norm1 <- pool1
I1106 04:09:45.174610 16758 net.cpp:399] norm1 -> norm1
I1106 04:09:45.174643 16758 net.cpp:141] Setting up norm1
I1106 04:09:45.174654 16758 net.cpp:148] Top shape: 24 96 30 30 (2073600)
I1106 04:09:45.174659 16758 net.cpp:156] Memory required for data: 88944288
I1106 04:09:45.174667 16758 layer_factory.hpp:77] Creating layer conv2
I1106 04:09:45.174679 16758 net.cpp:91] Creating Layer conv2
I1106 04:09:45.174685 16758 net.cpp:425] conv2 <- norm1
I1106 04:09:45.174693 16758 net.cpp:399] conv2 -> conv2
I1106 04:09:45.177064 16758 net.cpp:141] Setting up conv2
I1106 04:09:45.177083 16758 net.cpp:148] Top shape: 24 256 30 30 (5529600)
I1106 04:09:45.177091 16758 net.cpp:156] Memory required for data: 111062688
I1106 04:09:45.177103 16758 layer_factory.hpp:77] Creating layer relu2
I1106 04:09:45.177112 16758 net.cpp:91] Creating Layer relu2
I1106 04:09:45.177120 16758 net.cpp:425] relu2 <- conv2
I1106 04:09:45.177129 16758 net.cpp:386] relu2 -> conv2 (in-place)
I1106 04:09:45.177140 16758 net.cpp:141] Setting up relu2
I1106 04:09:45.177148 16758 net.cpp:148] Top shape: 24 256 30 30 (5529600)
I1106 04:09:45.177156 16758 net.cpp:156] Memory required for data: 133181088
I1106 04:09:45.177163 16758 layer_factory.hpp:77] Creating layer pool2
I1106 04:09:45.177173 16758 net.cpp:91] Creating Layer pool2
I1106 04:09:45.177181 16758 net.cpp:425] pool2 <- conv2
I1106 04:09:45.177191 16758 net.cpp:399] pool2 -> pool2
I1106 04:09:45.177227 16758 net.cpp:141] Setting up pool2
I1106 04:09:45.177238 16758 net.cpp:148] Top shape: 24 256 15 15 (1382400)
I1106 04:09:45.177245 16758 net.cpp:156] Memory required for data: 138710688
I1106 04:09:45.177253 16758 layer_factory.hpp:77] Creating layer norm2
I1106 04:09:45.177263 16758 net.cpp:91] Creating Layer norm2
I1106 04:09:45.177270 16758 net.cpp:425] norm2 <- pool2
I1106 04:09:45.177278 16758 net.cpp:399] norm2 -> norm2
I1106 04:09:45.177310 16758 net.cpp:141] Setting up norm2
I1106 04:09:45.177320 16758 net.cpp:148] Top shape: 24 256 15 15 (1382400)
I1106 04:09:45.177326 16758 net.cpp:156] Memory required for data: 144240288
I1106 04:09:45.177333 16758 layer_factory.hpp:77] Creating layer conv3
I1106 04:09:45.177345 16758 net.cpp:91] Creating Layer conv3
I1106 04:09:45.177352 16758 net.cpp:425] conv3 <- norm2
I1106 04:09:45.177362 16758 net.cpp:399] conv3 -> conv3
I1106 04:09:45.180538 16758 net.cpp:141] Setting up conv3
I1106 04:09:45.180555 16758 net.cpp:148] Top shape: 24 384 15 15 (2073600)
I1106 04:09:45.180564 16758 net.cpp:156] Memory required for data: 152534688
I1106 04:09:45.180577 16758 layer_factory.hpp:77] Creating layer relu3
I1106 04:09:45.180586 16758 net.cpp:91] Creating Layer relu3
I1106 04:09:45.180593 16758 net.cpp:425] relu3 <- conv3
I1106 04:09:45.180603 16758 net.cpp:386] relu3 -> conv3 (in-place)
I1106 04:09:45.180613 16758 net.cpp:141] Setting up relu3
I1106 04:09:45.180622 16758 net.cpp:148] Top shape: 24 384 15 15 (2073600)
I1106 04:09:45.180629 16758 net.cpp:156] Memory required for data: 160829088
I1106 04:09:45.180635 16758 layer_factory.hpp:77] Creating layer conv4
I1106 04:09:45.180647 16758 net.cpp:91] Creating Layer conv4
I1106 04:09:45.180655 16758 net.cpp:425] conv4 <- conv3
I1106 04:09:45.180665 16758 net.cpp:399] conv4 -> conv4
I1106 04:09:45.183879 16758 net.cpp:141] Setting up conv4
I1106 04:09:45.183898 16758 net.cpp:148] Top shape: 24 256 15 15 (1382400)
I1106 04:09:45.183907 16758 net.cpp:156] Memory required for data: 166358688
I1106 04:09:45.183917 16758 layer_factory.hpp:77] Creating layer relu4
I1106 04:09:45.183925 16758 net.cpp:91] Creating Layer relu4
I1106 04:09:45.183933 16758 net.cpp:425] relu4 <- conv4
I1106 04:09:45.183941 16758 net.cpp:386] relu4 -> conv4 (in-place)
I1106 04:09:45.183953 16758 net.cpp:141] Setting up relu4
I1106 04:09:45.183960 16758 net.cpp:148] Top shape: 24 256 15 15 (1382400)
I1106 04:09:45.183967 16758 net.cpp:156] Memory required for data: 171888288
I1106 04:09:45.183974 16758 layer_factory.hpp:77] Creating layer pool5
I1106 04:09:45.183984 16758 net.cpp:91] Creating Layer pool5
I1106 04:09:45.184000 16758 net.cpp:425] pool5 <- conv4
I1106 04:09:45.184008 16758 net.cpp:399] pool5 -> pool5
I1106 04:09:45.184057 16758 net.cpp:141] Setting up pool5
I1106 04:09:45.184067 16758 net.cpp:148] Top shape: 24 256 7 7 (301056)
I1106 04:09:45.184075 16758 net.cpp:156] Memory required for data: 173092512
I1106 04:09:45.184092 16758 layer_factory.hpp:77] Creating layer fc6
I1106 04:09:45.184100 16758 net.cpp:91] Creating Layer fc6
I1106 04:09:45.184109 16758 net.cpp:425] fc6 <- pool5
I1106 04:09:45.184118 16758 net.cpp:399] fc6 -> fc6
I1106 04:09:45.515667 16758 net.cpp:141] Setting up fc6
I1106 04:09:45.515707 16758 net.cpp:148] Top shape: 24 4096 (98304)
I1106 04:09:45.515717 16758 net.cpp:156] Memory required for data: 173485728
I1106 04:09:45.515738 16758 layer_factory.hpp:77] Creating layer relu6
I1106 04:09:45.515755 16758 net.cpp:91] Creating Layer relu6
I1106 04:09:45.515764 16758 net.cpp:425] relu6 <- fc6
I1106 04:09:45.515774 16758 net.cpp:386] relu6 -> fc6 (in-place)
I1106 04:09:45.515787 16758 net.cpp:141] Setting up relu6
I1106 04:09:45.515796 16758 net.cpp:148] Top shape: 24 4096 (98304)
I1106 04:09:45.515805 16758 net.cpp:156] Memory required for data: 173878944
I1106 04:09:45.515811 16758 layer_factory.hpp:77] Creating layer drop6
I1106 04:09:45.515821 16758 net.cpp:91] Creating Layer drop6
I1106 04:09:45.515830 16758 net.cpp:425] drop6 <- fc6
I1106 04:09:45.515838 16758 net.cpp:386] drop6 -> fc6 (in-place)
I1106 04:09:45.515862 16758 net.cpp:141] Setting up drop6
I1106 04:09:45.515872 16758 net.cpp:148] Top shape: 24 4096 (98304)
I1106 04:09:45.515880 16758 net.cpp:156] Memory required for data: 174272160
I1106 04:09:45.515887 16758 layer_factory.hpp:77] Creating layer fc7
I1106 04:09:45.515898 16758 net.cpp:91] Creating Layer fc7
I1106 04:09:45.515907 16758 net.cpp:425] fc7 <- fc6
I1106 04:09:45.515915 16758 net.cpp:399] fc7 -> fc7
I1106 04:09:45.614195 16758 net.cpp:141] Setting up fc7
I1106 04:09:45.614239 16758 net.cpp:148] Top shape: 24 4096 (98304)
I1106 04:09:45.614244 16758 net.cpp:156] Memory required for data: 174665376
I1106 04:09:45.614254 16758 layer_factory.hpp:77] Creating layer relu7
I1106 04:09:45.614264 16758 net.cpp:91] Creating Layer relu7
I1106 04:09:45.614270 16758 net.cpp:425] relu7 <- fc7
I1106 04:09:45.614277 16758 net.cpp:386] relu7 -> fc7 (in-place)
I1106 04:09:45.614286 16758 net.cpp:141] Setting up relu7
I1106 04:09:45.614291 16758 net.cpp:148] Top shape: 24 4096 (98304)
I1106 04:09:45.614295 16758 net.cpp:156] Memory required for data: 175058592
I1106 04:09:45.614300 16758 layer_factory.hpp:77] Creating layer drop7
I1106 04:09:45.614307 16758 net.cpp:91] Creating Layer drop7
I1106 04:09:45.614312 16758 net.cpp:425] drop7 <- fc7
I1106 04:09:45.614317 16758 net.cpp:386] drop7 -> fc7 (in-place)
I1106 04:09:45.614336 16758 net.cpp:141] Setting up drop7
I1106 04:09:45.614342 16758 net.cpp:148] Top shape: 24 4096 (98304)
I1106 04:09:45.614347 16758 net.cpp:156] Memory required for data: 175451808
I1106 04:09:45.614351 16758 layer_factory.hpp:77] Creating layer fc8
I1106 04:09:45.614361 16758 net.cpp:91] Creating Layer fc8
I1106 04:09:45.614364 16758 net.cpp:425] fc8 <- fc7
I1106 04:09:45.614370 16758 net.cpp:399] fc8 -> fc8
I1106 04:09:45.615432 16758 net.cpp:141] Setting up fc8
I1106 04:09:45.615444 16758 net.cpp:148] Top shape: 24 40 (960)
I1106 04:09:45.615449 16758 net.cpp:156] Memory required for data: 175455648
I1106 04:09:45.615456 16758 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I1106 04:09:45.615463 16758 net.cpp:91] Creating Layer fc8_fc8_0_split
I1106 04:09:45.615468 16758 net.cpp:425] fc8_fc8_0_split <- fc8
I1106 04:09:45.615473 16758 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1106 04:09:45.615486 16758 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1106 04:09:45.615509 16758 net.cpp:141] Setting up fc8_fc8_0_split
I1106 04:09:45.615525 16758 net.cpp:148] Top shape: 24 40 (960)
I1106 04:09:45.615540 16758 net.cpp:148] Top shape: 24 40 (960)
I1106 04:09:45.615543 16758 net.cpp:156] Memory required for data: 175463328
I1106 04:09:45.615547 16758 layer_factory.hpp:77] Creating layer accuracy
I1106 04:09:45.615553 16758 net.cpp:91] Creating Layer accuracy
I1106 04:09:45.615558 16758 net.cpp:425] accuracy <- fc8_fc8_0_split_0
I1106 04:09:45.615566 16758 net.cpp:425] accuracy <- label_img_1_split_0
I1106 04:09:45.615569 16758 net.cpp:399] accuracy -> accuracy
I1106 04:09:45.615576 16758 net.cpp:141] Setting up accuracy
I1106 04:09:45.615581 16758 net.cpp:148] Top shape: (1)
I1106 04:09:45.615586 16758 net.cpp:156] Memory required for data: 175463332
I1106 04:09:45.615591 16758 layer_factory.hpp:77] Creating layer loss
I1106 04:09:45.615597 16758 net.cpp:91] Creating Layer loss
I1106 04:09:45.615602 16758 net.cpp:425] loss <- fc8_fc8_0_split_1
I1106 04:09:45.615607 16758 net.cpp:425] loss <- label_img_1_split_1
I1106 04:09:45.615612 16758 net.cpp:399] loss -> loss
I1106 04:09:45.615619 16758 layer_factory.hpp:77] Creating layer loss
I1106 04:09:45.615671 16758 net.cpp:141] Setting up loss
I1106 04:09:45.615679 16758 net.cpp:148] Top shape: (1)
I1106 04:09:45.615682 16758 net.cpp:151]     with loss weight 1
I1106 04:09:45.615694 16758 net.cpp:156] Memory required for data: 175463336
I1106 04:09:45.615697 16758 net.cpp:217] loss needs backward computation.
I1106 04:09:45.615702 16758 net.cpp:219] accuracy does not need backward computation.
I1106 04:09:45.615707 16758 net.cpp:217] fc8_fc8_0_split needs backward computation.
I1106 04:09:45.615711 16758 net.cpp:217] fc8 needs backward computation.
I1106 04:09:45.615716 16758 net.cpp:217] drop7 needs backward computation.
I1106 04:09:45.615721 16758 net.cpp:217] relu7 needs backward computation.
I1106 04:09:45.615725 16758 net.cpp:217] fc7 needs backward computation.
I1106 04:09:45.615730 16758 net.cpp:217] drop6 needs backward computation.
I1106 04:09:45.615733 16758 net.cpp:217] relu6 needs backward computation.
I1106 04:09:45.615737 16758 net.cpp:217] fc6 needs backward computation.
I1106 04:09:45.615742 16758 net.cpp:217] pool5 needs backward computation.
I1106 04:09:45.615746 16758 net.cpp:217] relu4 needs backward computation.
I1106 04:09:45.615751 16758 net.cpp:217] conv4 needs backward computation.
I1106 04:09:45.615756 16758 net.cpp:217] relu3 needs backward computation.
I1106 04:09:45.615761 16758 net.cpp:217] conv3 needs backward computation.
I1106 04:09:45.615766 16758 net.cpp:217] norm2 needs backward computation.
I1106 04:09:45.615770 16758 net.cpp:217] pool2 needs backward computation.
I1106 04:09:45.615774 16758 net.cpp:217] relu2 needs backward computation.
I1106 04:09:45.615779 16758 net.cpp:217] conv2 needs backward computation.
I1106 04:09:45.615783 16758 net.cpp:217] norm1 needs backward computation.
I1106 04:09:45.615787 16758 net.cpp:217] pool1 needs backward computation.
I1106 04:09:45.615792 16758 net.cpp:217] relu1 needs backward computation.
I1106 04:09:45.615797 16758 net.cpp:217] conv1 needs backward computation.
I1106 04:09:45.615802 16758 net.cpp:219] label_img_1_split does not need backward computation.
I1106 04:09:45.615806 16758 net.cpp:219] img does not need backward computation.
I1106 04:09:45.615811 16758 net.cpp:261] This network produces output accuracy
I1106 04:09:45.615815 16758 net.cpp:261] This network produces output loss
I1106 04:09:45.615828 16758 net.cpp:274] Network initialization done.
I1106 04:09:45.615881 16758 solver.cpp:60] Solver scaffolding done.
I1106 04:09:45.642985 16758 solver.cpp:337] Iteration 0, Testing net (#0)
I1106 04:09:49.054555 16758 solver.cpp:228] Iteration 0, loss = 3.74251
I1106 04:09:49.054594 16758 solver.cpp:244]     Train net output #0: accuracy = 0.03125
I1106 04:09:49.054605 16758 solver.cpp:244]     Train net output #1: loss = 3.74251 (* 1 = 3.74251 loss)
I1106 04:09:49.054612 16758 sgd_solver.cpp:106] Iteration 0, lr = 0.01
>>> 2016-11-06 04:23:27.775791 Begin model classification tests
/home/kevin/catkin_ws/src/romans_stack/model_net/caffe_net/score.py:93: RuntimeWarning: invalid value encountered in true_divide
  return accuracy_max/len(dataset), accuracy_ave/len(dataset), loss/len(dataset), confusion_mat/(np.tile(confusion_mat.sum(1),(40,1))).T
>>> 2016-11-06 04:24:57.219533 Iteration 100 mean classification accuracy (max)  0.485829959514
>>> 2016-11-06 04:24:57.219576 Iteration 100 mean classification accuracy (ave) 0.0404858299595
>>> 2016-11-06 04:24:57.219592 Iteration 100 mean testing loss 1.90520721176
>>> 2016-11-06 04:24:57.219611 Iteration 100 mean confusion matrix [[ 0.08        0.01        0.22       ...,  0.          0.          0.        ]
 [ 0.06        0.02        0.14       ...,  0.          0.          0.        ]
 [ 0.          0.          0.01030928 ...,  0.          0.          0.        ]
 ..., 
 [        nan         nan         nan ...,         nan         nan
          nan]
 [        nan         nan         nan ...,         nan         nan
          nan]
 [        nan         nan         nan ...,         nan         nan
          nan]]
I1106 04:25:04.611940 16758 solver.cpp:228] Iteration 100, loss = 2.35698
I1106 04:25:04.611984 16758 solver.cpp:244]     Train net output #0: accuracy = 0.359375
I1106 04:25:04.612004 16758 solver.cpp:244]     Train net output #1: loss = 2.35698 (* 1 = 2.35698 loss)
I1106 04:25:04.612016 16758 sgd_solver.cpp:106] Iteration 100, lr = 0.01
>>> 2016-11-06 04:36:42.589621 Begin model classification tests
>>> 2016-11-06 04:38:21.212275 Iteration 200 mean classification accuracy (max)  0.522267206478
>>> 2016-11-06 04:38:21.212320 Iteration 200 mean classification accuracy (ave) 0.0283400809717
>>> 2016-11-06 04:38:21.212334 Iteration 200 mean testing loss 1.72960145705
>>> 2016-11-06 04:38:21.212352 Iteration 200 mean confusion matrix [[ nan  nan  nan ...,  nan  nan  nan]
 [ nan  nan  nan ...,  nan  nan  nan]
 [  0.   0.   0. ...,   0.   0.   0.]
 ..., 
 [ nan  nan  nan ...,  nan  nan  nan]
 [ nan  nan  nan ...,  nan  nan  nan]
 [ nan  nan  nan ...,  nan  nan  nan]]
I1106 04:38:30.547689 16758 solver.cpp:228] Iteration 200, loss = 1.90229
I1106 04:38:30.547749 16758 solver.cpp:244]     Train net output #0: accuracy = 0.449219
I1106 04:38:30.547777 16758 solver.cpp:244]     Train net output #1: loss = 1.90229 (* 1 = 1.90229 loss)
I1106 04:38:30.547798 16758 sgd_solver.cpp:106] Iteration 200, lr = 0.01
^CTraceback (most recent call last):
  File "./solve.py", line 27, in <module>
    solver.step(100)
  File "/home/kevin/caffeplus/python_layer/data_layers/model_net_layer.py", line 69, in reshape
    self.data, self.label = self.load_data(self.indices, self.idx)
  File "/home/kevin/caffeplus/python_layer/data_layers/model_net_layer.py", line 112, in load_data
    mat = scipy.io.loadmat('{}/{}.mat'.format(self.dataset_dir, filei))
  File "/usr/lib/python2.7/dist-packages/scipy/io/matlab/mio.py", line 125, in loadmat
    MR = mat_reader_factory(file_name, appendmat, **kwargs)
  File "/usr/lib/python2.7/dist-packages/scipy/io/matlab/mio.py", line 55, in mat_reader_factory
    mjv, mnv = get_matfile_version(byte_stream)
  File "/usr/lib/python2.7/dist-packages/scipy/io/matlab/miobase.py", line 217, in get_matfile_version
    buffer = fileobj.read(4))
KeyboardInterrupt
]0;kevin@kevin-desktop: ~/catkin_ws/src/romans_stack/model_net/caffe_netkevin@kevin-desktop:~/catkin_ws/src/romans_stack/model_net/caffe_net$ ./solve.py
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Net<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Blob<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Solver<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1106 04:39:34.260875 21715 solver.cpp:48] Initializing solver from parameters: 
train_net: "train.prototxt"
test_net: "test.prototxt"
test_iter: 0
test_interval: 9999999
base_lr: 0.01
display: 100
max_iter: 450000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 5000
snapshot: 1000
snapshot_prefix: "/home/kevin/snapshot/"
solver_mode: GPU
I1106 04:39:34.261035 21715 solver.cpp:81] Creating training net from train_net file: train.prototxt
I1106 04:39:34.261734 21715 net.cpp:49] Initializing net from parameters: 
state {
  phase: TRAIN
}
layer {
  name: "img"
  type: "Python"
  top: "img"
  top: "label"
  python_param {
    module: "data_layers.model_net_layer"
    layer: "ModelNetDataLayer"
    param_str: "{\'img_size\': (250, 250), \'dtype\': \'frame\', \'batch_size\': 256, \'seed\': 1337, \'split\': \'train\', \'dataset_dir\': \'/home/kevin/dataset/processed_data\', \'mean\': 2}"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "img"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 0
    kernel_size: 11
    group: 1
    stride: 4
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    name: "conv3_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv3_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    name: "conv4_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv4_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv4"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 40
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1106 04:39:34.263221 21715 layer_factory.hpp:77] Creating layer img
I1106 04:39:34.326536 21715 net.cpp:91] Creating Layer img
I1106 04:39:34.326575 21715 net.cpp:399] img -> img
I1106 04:39:34.326594 21715 net.cpp:399] img -> label
{'img_size': (250, 250), 'dtype': 'frame', 'batch_size': 256, 'seed': 1337, 'split': 'train', 'dataset_dir': '/home/kevin/dataset/processed_data', 'mean': 2}
I1106 04:39:56.004355 21715 net.cpp:141] Setting up img
I1106 04:39:56.004392 21715 net.cpp:148] Top shape: 256 1 250 250 (16000000)
I1106 04:39:56.004401 21715 net.cpp:148] Top shape: 256 1 (256)
I1106 04:39:56.004408 21715 net.cpp:156] Memory required for data: 64001024
I1106 04:39:56.004428 21715 layer_factory.hpp:77] Creating layer label_img_1_split
I1106 04:39:56.004478 21715 net.cpp:91] Creating Layer label_img_1_split
I1106 04:39:56.004487 21715 net.cpp:425] label_img_1_split <- label
I1106 04:39:56.004498 21715 net.cpp:399] label_img_1_split -> label_img_1_split_0
I1106 04:39:56.004513 21715 net.cpp:399] label_img_1_split -> label_img_1_split_1
I1106 04:39:56.004575 21715 net.cpp:141] Setting up label_img_1_split
I1106 04:39:56.004585 21715 net.cpp:148] Top shape: 256 1 (256)
I1106 04:39:56.004591 21715 net.cpp:148] Top shape: 256 1 (256)
I1106 04:39:56.004597 21715 net.cpp:156] Memory required for data: 64003072
I1106 04:39:56.004613 21715 layer_factory.hpp:77] Creating layer conv1
I1106 04:39:56.004637 21715 net.cpp:91] Creating Layer conv1
I1106 04:39:56.004653 21715 net.cpp:425] conv1 <- img
I1106 04:39:56.004673 21715 net.cpp:399] conv1 -> conv1
I1106 04:39:56.005972 21715 net.cpp:141] Setting up conv1
I1106 04:39:56.005987 21715 net.cpp:148] Top shape: 256 96 60 60 (88473600)
I1106 04:39:56.005993 21715 net.cpp:156] Memory required for data: 417897472
I1106 04:39:56.006005 21715 layer_factory.hpp:77] Creating layer relu1
I1106 04:39:56.006023 21715 net.cpp:91] Creating Layer relu1
I1106 04:39:56.006042 21715 net.cpp:425] relu1 <- conv1
I1106 04:39:56.006060 21715 net.cpp:386] relu1 -> conv1 (in-place)
I1106 04:39:56.006078 21715 net.cpp:141] Setting up relu1
I1106 04:39:56.006086 21715 net.cpp:148] Top shape: 256 96 60 60 (88473600)
I1106 04:39:56.006093 21715 net.cpp:156] Memory required for data: 771791872
I1106 04:39:56.006100 21715 layer_factory.hpp:77] Creating layer pool1
I1106 04:39:56.006121 21715 net.cpp:91] Creating Layer pool1
I1106 04:39:56.006127 21715 net.cpp:425] pool1 <- conv1
I1106 04:39:56.006145 21715 net.cpp:399] pool1 -> pool1
I1106 04:39:56.006193 21715 net.cpp:141] Setting up pool1
I1106 04:39:56.006202 21715 net.cpp:148] Top shape: 256 96 30 30 (22118400)
I1106 04:39:56.006219 21715 net.cpp:156] Memory required for data: 860265472
I1106 04:39:56.006225 21715 layer_factory.hpp:77] Creating layer norm1
I1106 04:39:56.006234 21715 net.cpp:91] Creating Layer norm1
I1106 04:39:56.006242 21715 net.cpp:425] norm1 <- pool1
I1106 04:39:56.006258 21715 net.cpp:399] norm1 -> norm1
I1106 04:39:56.006299 21715 net.cpp:141] Setting up norm1
I1106 04:39:56.006306 21715 net.cpp:148] Top shape: 256 96 30 30 (22118400)
I1106 04:39:56.006314 21715 net.cpp:156] Memory required for data: 948739072
I1106 04:39:56.006327 21715 layer_factory.hpp:77] Creating layer conv2
I1106 04:39:56.006348 21715 net.cpp:91] Creating Layer conv2
I1106 04:39:56.006355 21715 net.cpp:425] conv2 <- norm1
I1106 04:39:56.006362 21715 net.cpp:399] conv2 -> conv2
I1106 04:39:56.008976 21715 net.cpp:141] Setting up conv2
I1106 04:39:56.008994 21715 net.cpp:148] Top shape: 256 256 30 30 (58982400)
I1106 04:39:56.009011 21715 net.cpp:156] Memory required for data: 1184668672
I1106 04:39:56.009026 21715 layer_factory.hpp:77] Creating layer relu2
I1106 04:39:56.009035 21715 net.cpp:91] Creating Layer relu2
I1106 04:39:56.009042 21715 net.cpp:425] relu2 <- conv2
I1106 04:39:56.009052 21715 net.cpp:386] relu2 -> conv2 (in-place)
I1106 04:39:56.009062 21715 net.cpp:141] Setting up relu2
I1106 04:39:56.009069 21715 net.cpp:148] Top shape: 256 256 30 30 (58982400)
I1106 04:39:56.009076 21715 net.cpp:156] Memory required for data: 1420598272
I1106 04:39:56.009083 21715 layer_factory.hpp:77] Creating layer pool2
I1106 04:39:56.009093 21715 net.cpp:91] Creating Layer pool2
I1106 04:39:56.009100 21715 net.cpp:425] pool2 <- conv2
I1106 04:39:56.009109 21715 net.cpp:399] pool2 -> pool2
I1106 04:39:56.009147 21715 net.cpp:141] Setting up pool2
I1106 04:39:56.009157 21715 net.cpp:148] Top shape: 256 256 15 15 (14745600)
I1106 04:39:56.009165 21715 net.cpp:156] Memory required for data: 1479580672
I1106 04:39:56.009171 21715 layer_factory.hpp:77] Creating layer norm2
I1106 04:39:56.009181 21715 net.cpp:91] Creating Layer norm2
I1106 04:39:56.009188 21715 net.cpp:425] norm2 <- pool2
I1106 04:39:56.009196 21715 net.cpp:399] norm2 -> norm2
I1106 04:39:56.009227 21715 net.cpp:141] Setting up norm2
I1106 04:39:56.009238 21715 net.cpp:148] Top shape: 256 256 15 15 (14745600)
I1106 04:39:56.009243 21715 net.cpp:156] Memory required for data: 1538563072
I1106 04:39:56.009250 21715 layer_factory.hpp:77] Creating layer conv3
I1106 04:39:56.009263 21715 net.cpp:91] Creating Layer conv3
I1106 04:39:56.009269 21715 net.cpp:425] conv3 <- norm2
I1106 04:39:56.009279 21715 net.cpp:399] conv3 -> conv3
I1106 04:39:56.012266 21715 net.cpp:141] Setting up conv3
I1106 04:39:56.012302 21715 net.cpp:148] Top shape: 256 384 15 15 (22118400)
I1106 04:39:56.012311 21715 net.cpp:156] Memory required for data: 1627036672
I1106 04:39:56.012346 21715 layer_factory.hpp:77] Creating layer relu3
I1106 04:39:56.012364 21715 net.cpp:91] Creating Layer relu3
I1106 04:39:56.012380 21715 net.cpp:425] relu3 <- conv3
I1106 04:39:56.012389 21715 net.cpp:386] relu3 -> conv3 (in-place)
I1106 04:39:56.012420 21715 net.cpp:141] Setting up relu3
I1106 04:39:56.012428 21715 net.cpp:148] Top shape: 256 384 15 15 (22118400)
I1106 04:39:56.012442 21715 net.cpp:156] Memory required for data: 1715510272
I1106 04:39:56.012449 21715 layer_factory.hpp:77] Creating layer conv4
I1106 04:39:56.012462 21715 net.cpp:91] Creating Layer conv4
I1106 04:39:56.012470 21715 net.cpp:425] conv4 <- conv3
I1106 04:39:56.012475 21715 net.cpp:399] conv4 -> conv4
I1106 04:39:56.014937 21715 net.cpp:141] Setting up conv4
I1106 04:39:56.014961 21715 net.cpp:148] Top shape: 256 256 15 15 (14745600)
I1106 04:39:56.014966 21715 net.cpp:156] Memory required for data: 1774492672
I1106 04:39:56.014986 21715 layer_factory.hpp:77] Creating layer relu4
I1106 04:39:56.015000 21715 net.cpp:91] Creating Layer relu4
I1106 04:39:56.015005 21715 net.cpp:425] relu4 <- conv4
I1106 04:39:56.015010 21715 net.cpp:386] relu4 -> conv4 (in-place)
I1106 04:39:56.015017 21715 net.cpp:141] Setting up relu4
I1106 04:39:56.015022 21715 net.cpp:148] Top shape: 256 256 15 15 (14745600)
I1106 04:39:56.015027 21715 net.cpp:156] Memory required for data: 1833475072
I1106 04:39:56.015041 21715 layer_factory.hpp:77] Creating layer pool5
I1106 04:39:56.015048 21715 net.cpp:91] Creating Layer pool5
I1106 04:39:56.015063 21715 net.cpp:425] pool5 <- conv4
I1106 04:39:56.015079 21715 net.cpp:399] pool5 -> pool5
I1106 04:39:56.015116 21715 net.cpp:141] Setting up pool5
I1106 04:39:56.015122 21715 net.cpp:148] Top shape: 256 256 7 7 (3211264)
I1106 04:39:56.015126 21715 net.cpp:156] Memory required for data: 1846320128
I1106 04:39:56.015141 21715 layer_factory.hpp:77] Creating layer fc6
I1106 04:39:56.015153 21715 net.cpp:91] Creating Layer fc6
I1106 04:39:56.015158 21715 net.cpp:425] fc6 <- pool5
I1106 04:39:56.015173 21715 net.cpp:399] fc6 -> fc6
I1106 04:39:56.360628 21715 net.cpp:141] Setting up fc6
I1106 04:39:56.360671 21715 net.cpp:148] Top shape: 256 4096 (1048576)
I1106 04:39:56.360682 21715 net.cpp:156] Memory required for data: 1850514432
I1106 04:39:56.360705 21715 layer_factory.hpp:77] Creating layer relu6
I1106 04:39:56.360723 21715 net.cpp:91] Creating Layer relu6
I1106 04:39:56.360734 21715 net.cpp:425] relu6 <- fc6
I1106 04:39:56.360747 21715 net.cpp:386] relu6 -> fc6 (in-place)
I1106 04:39:56.360762 21715 net.cpp:141] Setting up relu6
I1106 04:39:56.360772 21715 net.cpp:148] Top shape: 256 4096 (1048576)
I1106 04:39:56.360780 21715 net.cpp:156] Memory required for data: 1854708736
I1106 04:39:56.360788 21715 layer_factory.hpp:77] Creating layer drop6
I1106 04:39:56.360808 21715 net.cpp:91] Creating Layer drop6
I1106 04:39:56.360817 21715 net.cpp:425] drop6 <- fc6
I1106 04:39:56.360827 21715 net.cpp:386] drop6 -> fc6 (in-place)
I1106 04:39:56.360852 21715 net.cpp:141] Setting up drop6
I1106 04:39:56.360863 21715 net.cpp:148] Top shape: 256 4096 (1048576)
I1106 04:39:56.360872 21715 net.cpp:156] Memory required for data: 1858903040
I1106 04:39:56.360880 21715 layer_factory.hpp:77] Creating layer fc7
I1106 04:39:56.360893 21715 net.cpp:91] Creating Layer fc7
I1106 04:39:56.360900 21715 net.cpp:425] fc7 <- fc6
I1106 04:39:56.360909 21715 net.cpp:399] fc7 -> fc7
I1106 04:39:56.476219 21715 net.cpp:141] Setting up fc7
I1106 04:39:56.476250 21715 net.cpp:148] Top shape: 256 4096 (1048576)
I1106 04:39:56.476256 21715 net.cpp:156] Memory required for data: 1863097344
I1106 04:39:56.476265 21715 layer_factory.hpp:77] Creating layer relu7
I1106 04:39:56.476275 21715 net.cpp:91] Creating Layer relu7
I1106 04:39:56.476280 21715 net.cpp:425] relu7 <- fc7
I1106 04:39:56.476286 21715 net.cpp:386] relu7 -> fc7 (in-place)
I1106 04:39:56.476294 21715 net.cpp:141] Setting up relu7
I1106 04:39:56.476300 21715 net.cpp:148] Top shape: 256 4096 (1048576)
I1106 04:39:56.476303 21715 net.cpp:156] Memory required for data: 1867291648
I1106 04:39:56.476306 21715 layer_factory.hpp:77] Creating layer drop7
I1106 04:39:56.476313 21715 net.cpp:91] Creating Layer drop7
I1106 04:39:56.476317 21715 net.cpp:425] drop7 <- fc7
I1106 04:39:56.476321 21715 net.cpp:386] drop7 -> fc7 (in-place)
I1106 04:39:56.476335 21715 net.cpp:141] Setting up drop7
I1106 04:39:56.476339 21715 net.cpp:148] Top shape: 256 4096 (1048576)
I1106 04:39:56.476343 21715 net.cpp:156] Memory required for data: 1871485952
I1106 04:39:56.476347 21715 layer_factory.hpp:77] Creating layer fc8
I1106 04:39:56.476354 21715 net.cpp:91] Creating Layer fc8
I1106 04:39:56.476358 21715 net.cpp:425] fc8 <- fc7
I1106 04:39:56.476363 21715 net.cpp:399] fc8 -> fc8
I1106 04:39:56.477476 21715 net.cpp:141] Setting up fc8
I1106 04:39:56.477490 21715 net.cpp:148] Top shape: 256 40 (10240)
I1106 04:39:56.477495 21715 net.cpp:156] Memory required for data: 1871526912
I1106 04:39:56.477504 21715 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I1106 04:39:56.477512 21715 net.cpp:91] Creating Layer fc8_fc8_0_split
I1106 04:39:56.477519 21715 net.cpp:425] fc8_fc8_0_split <- fc8
I1106 04:39:56.477524 21715 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1106 04:39:56.477533 21715 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1106 04:39:56.477556 21715 net.cpp:141] Setting up fc8_fc8_0_split
I1106 04:39:56.477563 21715 net.cpp:148] Top shape: 256 40 (10240)
I1106 04:39:56.477568 21715 net.cpp:148] Top shape: 256 40 (10240)
I1106 04:39:56.477572 21715 net.cpp:156] Memory required for data: 1871608832
I1106 04:39:56.477577 21715 layer_factory.hpp:77] Creating layer accuracy
I1106 04:39:56.477589 21715 net.cpp:91] Creating Layer accuracy
I1106 04:39:56.477593 21715 net.cpp:425] accuracy <- fc8_fc8_0_split_0
I1106 04:39:56.477598 21715 net.cpp:425] accuracy <- label_img_1_split_0
I1106 04:39:56.477603 21715 net.cpp:399] accuracy -> accuracy
I1106 04:39:56.477610 21715 net.cpp:141] Setting up accuracy
I1106 04:39:56.477615 21715 net.cpp:148] Top shape: (1)
I1106 04:39:56.477618 21715 net.cpp:156] Memory required for data: 1871608836
I1106 04:39:56.477622 21715 layer_factory.hpp:77] Creating layer loss
I1106 04:39:56.477632 21715 net.cpp:91] Creating Layer loss
I1106 04:39:56.477635 21715 net.cpp:425] loss <- fc8_fc8_0_split_1
I1106 04:39:56.477639 21715 net.cpp:425] loss <- label_img_1_split_1
I1106 04:39:56.477644 21715 net.cpp:399] loss -> loss
I1106 04:39:56.477650 21715 layer_factory.hpp:77] Creating layer loss
I1106 04:39:56.477716 21715 net.cpp:141] Setting up loss
I1106 04:39:56.477725 21715 net.cpp:148] Top shape: (1)
I1106 04:39:56.477728 21715 net.cpp:151]     with loss weight 1
I1106 04:39:56.477740 21715 net.cpp:156] Memory required for data: 1871608840
I1106 04:39:56.477744 21715 net.cpp:217] loss needs backward computation.
I1106 04:39:56.477751 21715 net.cpp:219] accuracy does not need backward computation.
I1106 04:39:56.477756 21715 net.cpp:217] fc8_fc8_0_split needs backward computation.
I1106 04:39:56.477761 21715 net.cpp:217] fc8 needs backward computation.
I1106 04:39:56.477766 21715 net.cpp:217] drop7 needs backward computation.
I1106 04:39:56.477771 21715 net.cpp:217] relu7 needs backward computation.
I1106 04:39:56.477776 21715 net.cpp:217] fc7 needs backward computation.
I1106 04:39:56.477779 21715 net.cpp:217] drop6 needs backward computation.
I1106 04:39:56.477783 21715 net.cpp:217] relu6 needs backward computation.
I1106 04:39:56.477788 21715 net.cpp:217] fc6 needs backward computation.
I1106 04:39:56.477793 21715 net.cpp:217] pool5 needs backward computation.
I1106 04:39:56.477798 21715 net.cpp:217] relu4 needs backward computation.
I1106 04:39:56.477803 21715 net.cpp:217] conv4 needs backward computation.
I1106 04:39:56.477808 21715 net.cpp:217] relu3 needs backward computation.
I1106 04:39:56.477813 21715 net.cpp:217] conv3 needs backward computation.
I1106 04:39:56.477818 21715 net.cpp:217] norm2 needs backward computation.
I1106 04:39:56.477823 21715 net.cpp:217] pool2 needs backward computation.
I1106 04:39:56.477828 21715 net.cpp:217] relu2 needs backward computation.
I1106 04:39:56.477833 21715 net.cpp:217] conv2 needs backward computation.
I1106 04:39:56.477836 21715 net.cpp:217] norm1 needs backward computation.
I1106 04:39:56.477841 21715 net.cpp:217] pool1 needs backward computation.
I1106 04:39:56.477845 21715 net.cpp:217] relu1 needs backward computation.
I1106 04:39:56.477850 21715 net.cpp:217] conv1 needs backward computation.
I1106 04:39:56.477855 21715 net.cpp:219] label_img_1_split does not need backward computation.
I1106 04:39:56.477861 21715 net.cpp:219] img does not need backward computation.
I1106 04:39:56.477865 21715 net.cpp:261] This network produces output accuracy
I1106 04:39:56.477870 21715 net.cpp:261] This network produces output loss
I1106 04:39:56.477882 21715 net.cpp:274] Network initialization done.
I1106 04:39:56.478214 21715 solver.cpp:181] Creating test net (#0) specified by test_net file: test.prototxt
I1106 04:39:56.478332 21715 net.cpp:49] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "img"
  type: "Python"
  top: "img"
  top: "label"
  python_param {
    module: "data_layers.model_net_layer"
    layer: "ModelNetDataLayer"
    param_str: "{\'img_size\': (250, 250), \'dtype\': \'object\', \'batch_size\': 256, \'seed\': 1337, \'split\': \'test\', \'dataset_dir\': \'/home/kevin/dataset/processed_data\', \'mean\': 2}"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "img"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 0
    kernel_size: 11
    group: 1
    stride: 4
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    name: "conv3_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv3_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    name: "conv4_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv4_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv4"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 40
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1106 04:39:56.478938 21715 layer_factory.hpp:77] Creating layer img
I1106 04:39:56.478988 21715 net.cpp:91] Creating Layer img
I1106 04:39:56.479001 21715 net.cpp:399] img -> img
I1106 04:39:56.479015 21715 net.cpp:399] img -> label
{'img_size': (250, 250), 'dtype': 'object', 'batch_size': 256, 'seed': 1337, 'split': 'test', 'dataset_dir': '/home/kevin/dataset/processed_data', 'mean': 2}
I1106 04:39:56.631803 21715 net.cpp:141] Setting up img
I1106 04:39:56.631866 21715 net.cpp:148] Top shape: 24 1 250 250 (1500000)
I1106 04:39:56.631882 21715 net.cpp:148] Top shape: 24 1 (24)
I1106 04:39:56.631893 21715 net.cpp:156] Memory required for data: 6000096
I1106 04:39:56.631911 21715 layer_factory.hpp:77] Creating layer label_img_1_split
I1106 04:39:56.631942 21715 net.cpp:91] Creating Layer label_img_1_split
I1106 04:39:56.631963 21715 net.cpp:425] label_img_1_split <- label
I1106 04:39:56.631989 21715 net.cpp:399] label_img_1_split -> label_img_1_split_0
I1106 04:39:56.632009 21715 net.cpp:399] label_img_1_split -> label_img_1_split_1
I1106 04:39:56.632084 21715 net.cpp:141] Setting up label_img_1_split
I1106 04:39:56.632098 21715 net.cpp:148] Top shape: 24 1 (24)
I1106 04:39:56.632118 21715 net.cpp:148] Top shape: 24 1 (24)
I1106 04:39:56.632125 21715 net.cpp:156] Memory required for data: 6000288
I1106 04:39:56.632134 21715 layer_factory.hpp:77] Creating layer conv1
I1106 04:39:56.632159 21715 net.cpp:91] Creating Layer conv1
I1106 04:39:56.632169 21715 net.cpp:425] conv1 <- img
I1106 04:39:56.632182 21715 net.cpp:399] conv1 -> conv1
I1106 04:39:56.632648 21715 net.cpp:141] Setting up conv1
I1106 04:39:56.632668 21715 net.cpp:148] Top shape: 24 96 60 60 (8294400)
I1106 04:39:56.632683 21715 net.cpp:156] Memory required for data: 39177888
I1106 04:39:56.632699 21715 layer_factory.hpp:77] Creating layer relu1
I1106 04:39:56.632714 21715 net.cpp:91] Creating Layer relu1
I1106 04:39:56.632724 21715 net.cpp:425] relu1 <- conv1
I1106 04:39:56.632735 21715 net.cpp:386] relu1 -> conv1 (in-place)
I1106 04:39:56.632750 21715 net.cpp:141] Setting up relu1
I1106 04:39:56.632762 21715 net.cpp:148] Top shape: 24 96 60 60 (8294400)
I1106 04:39:56.632772 21715 net.cpp:156] Memory required for data: 72355488
I1106 04:39:56.632783 21715 layer_factory.hpp:77] Creating layer pool1
I1106 04:39:56.632802 21715 net.cpp:91] Creating Layer pool1
I1106 04:39:56.632812 21715 net.cpp:425] pool1 <- conv1
I1106 04:39:56.632824 21715 net.cpp:399] pool1 -> pool1
I1106 04:39:56.632895 21715 net.cpp:141] Setting up pool1
I1106 04:39:56.632905 21715 net.cpp:148] Top shape: 24 96 30 30 (2073600)
I1106 04:39:56.632915 21715 net.cpp:156] Memory required for data: 80649888
I1106 04:39:56.632927 21715 layer_factory.hpp:77] Creating layer norm1
I1106 04:39:56.632946 21715 net.cpp:91] Creating Layer norm1
I1106 04:39:56.632956 21715 net.cpp:425] norm1 <- pool1
I1106 04:39:56.632968 21715 net.cpp:399] norm1 -> norm1
I1106 04:39:56.633016 21715 net.cpp:141] Setting up norm1
I1106 04:39:56.633030 21715 net.cpp:148] Top shape: 24 96 30 30 (2073600)
I1106 04:39:56.633040 21715 net.cpp:156] Memory required for data: 88944288
I1106 04:39:56.633051 21715 layer_factory.hpp:77] Creating layer conv2
I1106 04:39:56.633074 21715 net.cpp:91] Creating Layer conv2
I1106 04:39:56.633093 21715 net.cpp:425] conv2 <- norm1
I1106 04:39:56.633116 21715 net.cpp:399] conv2 -> conv2
I1106 04:39:56.635859 21715 net.cpp:141] Setting up conv2
I1106 04:39:56.635921 21715 net.cpp:148] Top shape: 24 256 30 30 (5529600)
I1106 04:39:56.635931 21715 net.cpp:156] Memory required for data: 111062688
I1106 04:39:56.635952 21715 layer_factory.hpp:77] Creating layer relu2
I1106 04:39:56.635973 21715 net.cpp:91] Creating Layer relu2
I1106 04:39:56.635982 21715 net.cpp:425] relu2 <- conv2
I1106 04:39:56.635993 21715 net.cpp:386] relu2 -> conv2 (in-place)
I1106 04:39:56.636006 21715 net.cpp:141] Setting up relu2
I1106 04:39:56.636013 21715 net.cpp:148] Top shape: 24 256 30 30 (5529600)
I1106 04:39:56.636016 21715 net.cpp:156] Memory required for data: 133181088
I1106 04:39:56.636020 21715 layer_factory.hpp:77] Creating layer pool2
I1106 04:39:56.636036 21715 net.cpp:91] Creating Layer pool2
I1106 04:39:56.636045 21715 net.cpp:425] pool2 <- conv2
I1106 04:39:56.636055 21715 net.cpp:399] pool2 -> pool2
I1106 04:39:56.636123 21715 net.cpp:141] Setting up pool2
I1106 04:39:56.636138 21715 net.cpp:148] Top shape: 24 256 15 15 (1382400)
I1106 04:39:56.636149 21715 net.cpp:156] Memory required for data: 138710688
I1106 04:39:56.636159 21715 layer_factory.hpp:77] Creating layer norm2
I1106 04:39:56.636178 21715 net.cpp:91] Creating Layer norm2
I1106 04:39:56.636188 21715 net.cpp:425] norm2 <- pool2
I1106 04:39:56.636200 21715 net.cpp:399] norm2 -> norm2
I1106 04:39:56.636246 21715 net.cpp:141] Setting up norm2
I1106 04:39:56.636270 21715 net.cpp:148] Top shape: 24 256 15 15 (1382400)
I1106 04:39:56.636286 21715 net.cpp:156] Memory required for data: 144240288
I1106 04:39:56.636296 21715 layer_factory.hpp:77] Creating layer conv3
I1106 04:39:56.636314 21715 net.cpp:91] Creating Layer conv3
I1106 04:39:56.636323 21715 net.cpp:425] conv3 <- norm2
I1106 04:39:56.636337 21715 net.cpp:399] conv3 -> conv3
I1106 04:39:56.641084 21715 net.cpp:141] Setting up conv3
I1106 04:39:56.641125 21715 net.cpp:148] Top shape: 24 384 15 15 (2073600)
I1106 04:39:56.641145 21715 net.cpp:156] Memory required for data: 152534688
I1106 04:39:56.641175 21715 layer_factory.hpp:77] Creating layer relu3
I1106 04:39:56.641191 21715 net.cpp:91] Creating Layer relu3
I1106 04:39:56.641211 21715 net.cpp:425] relu3 <- conv3
I1106 04:39:56.641232 21715 net.cpp:386] relu3 -> conv3 (in-place)
I1106 04:39:56.641258 21715 net.cpp:141] Setting up relu3
I1106 04:39:56.641283 21715 net.cpp:148] Top shape: 24 384 15 15 (2073600)
I1106 04:39:56.641302 21715 net.cpp:156] Memory required for data: 160829088
I1106 04:39:56.641324 21715 layer_factory.hpp:77] Creating layer conv4
I1106 04:39:56.641362 21715 net.cpp:91] Creating Layer conv4
I1106 04:39:56.641374 21715 net.cpp:425] conv4 <- conv3
I1106 04:39:56.641388 21715 net.cpp:399] conv4 -> conv4
I1106 04:39:56.645424 21715 net.cpp:141] Setting up conv4
I1106 04:39:56.645473 21715 net.cpp:148] Top shape: 24 256 15 15 (1382400)
I1106 04:39:56.645478 21715 net.cpp:156] Memory required for data: 166358688
I1106 04:39:56.645489 21715 layer_factory.hpp:77] Creating layer relu4
I1106 04:39:56.645500 21715 net.cpp:91] Creating Layer relu4
I1106 04:39:56.645506 21715 net.cpp:425] relu4 <- conv4
I1106 04:39:56.645514 21715 net.cpp:386] relu4 -> conv4 (in-place)
I1106 04:39:56.645524 21715 net.cpp:141] Setting up relu4
I1106 04:39:56.645529 21715 net.cpp:148] Top shape: 24 256 15 15 (1382400)
I1106 04:39:56.645534 21715 net.cpp:156] Memory required for data: 171888288
I1106 04:39:56.645537 21715 layer_factory.hpp:77] Creating layer pool5
I1106 04:39:56.645545 21715 net.cpp:91] Creating Layer pool5
I1106 04:39:56.645550 21715 net.cpp:425] pool5 <- conv4
I1106 04:39:56.645555 21715 net.cpp:399] pool5 -> pool5
I1106 04:39:56.645586 21715 net.cpp:141] Setting up pool5
I1106 04:39:56.645591 21715 net.cpp:148] Top shape: 24 256 7 7 (301056)
I1106 04:39:56.645596 21715 net.cpp:156] Memory required for data: 173092512
I1106 04:39:56.645601 21715 layer_factory.hpp:77] Creating layer fc6
I1106 04:39:56.645611 21715 net.cpp:91] Creating Layer fc6
I1106 04:39:56.645615 21715 net.cpp:425] fc6 <- pool5
I1106 04:39:56.645622 21715 net.cpp:399] fc6 -> fc6
I1106 04:39:56.963227 21715 net.cpp:141] Setting up fc6
I1106 04:39:56.963268 21715 net.cpp:148] Top shape: 24 4096 (98304)
I1106 04:39:56.963279 21715 net.cpp:156] Memory required for data: 173485728
I1106 04:39:56.963299 21715 layer_factory.hpp:77] Creating layer relu6
I1106 04:39:56.963315 21715 net.cpp:91] Creating Layer relu6
I1106 04:39:56.963325 21715 net.cpp:425] relu6 <- fc6
I1106 04:39:56.963335 21715 net.cpp:386] relu6 -> fc6 (in-place)
I1106 04:39:56.963348 21715 net.cpp:141] Setting up relu6
I1106 04:39:56.963357 21715 net.cpp:148] Top shape: 24 4096 (98304)
I1106 04:39:56.963364 21715 net.cpp:156] Memory required for data: 173878944
I1106 04:39:56.963371 21715 layer_factory.hpp:77] Creating layer drop6
I1106 04:39:56.963381 21715 net.cpp:91] Creating Layer drop6
I1106 04:39:56.963387 21715 net.cpp:425] drop6 <- fc6
I1106 04:39:56.963394 21715 net.cpp:386] drop6 -> fc6 (in-place)
I1106 04:39:56.963418 21715 net.cpp:141] Setting up drop6
I1106 04:39:56.963425 21715 net.cpp:148] Top shape: 24 4096 (98304)
I1106 04:39:56.963431 21715 net.cpp:156] Memory required for data: 174272160
I1106 04:39:56.963438 21715 layer_factory.hpp:77] Creating layer fc7
I1106 04:39:56.963449 21715 net.cpp:91] Creating Layer fc7
I1106 04:39:56.963455 21715 net.cpp:425] fc7 <- fc6
I1106 04:39:56.963464 21715 net.cpp:399] fc7 -> fc7
I1106 04:39:57.073972 21715 net.cpp:141] Setting up fc7
I1106 04:39:57.074009 21715 net.cpp:148] Top shape: 24 4096 (98304)
I1106 04:39:57.074019 21715 net.cpp:156] Memory required for data: 174665376
I1106 04:39:57.074038 21715 layer_factory.hpp:77] Creating layer relu7
I1106 04:39:57.074054 21715 net.cpp:91] Creating Layer relu7
I1106 04:39:57.074064 21715 net.cpp:425] relu7 <- fc7
I1106 04:39:57.074077 21715 net.cpp:386] relu7 -> fc7 (in-place)
I1106 04:39:57.074091 21715 net.cpp:141] Setting up relu7
I1106 04:39:57.074100 21715 net.cpp:148] Top shape: 24 4096 (98304)
I1106 04:39:57.074107 21715 net.cpp:156] Memory required for data: 175058592
I1106 04:39:57.074115 21715 layer_factory.hpp:77] Creating layer drop7
I1106 04:39:57.074126 21715 net.cpp:91] Creating Layer drop7
I1106 04:39:57.074133 21715 net.cpp:425] drop7 <- fc7
I1106 04:39:57.074142 21715 net.cpp:386] drop7 -> fc7 (in-place)
I1106 04:39:57.074183 21715 net.cpp:141] Setting up drop7
I1106 04:39:57.074196 21715 net.cpp:148] Top shape: 24 4096 (98304)
I1106 04:39:57.074203 21715 net.cpp:156] Memory required for data: 175451808
I1106 04:39:57.074211 21715 layer_factory.hpp:77] Creating layer fc8
I1106 04:39:57.074223 21715 net.cpp:91] Creating Layer fc8
I1106 04:39:57.074231 21715 net.cpp:425] fc8 <- fc7
I1106 04:39:57.074241 21715 net.cpp:399] fc8 -> fc8
I1106 04:39:57.075884 21715 net.cpp:141] Setting up fc8
I1106 04:39:57.075904 21715 net.cpp:148] Top shape: 24 40 (960)
I1106 04:39:57.075912 21715 net.cpp:156] Memory required for data: 175455648
I1106 04:39:57.075924 21715 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I1106 04:39:57.075935 21715 net.cpp:91] Creating Layer fc8_fc8_0_split
I1106 04:39:57.075943 21715 net.cpp:425] fc8_fc8_0_split <- fc8
I1106 04:39:57.075953 21715 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1106 04:39:57.075968 21715 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1106 04:39:57.076004 21715 net.cpp:141] Setting up fc8_fc8_0_split
I1106 04:39:57.076020 21715 net.cpp:148] Top shape: 24 40 (960)
I1106 04:39:57.076030 21715 net.cpp:148] Top shape: 24 40 (960)
I1106 04:39:57.076038 21715 net.cpp:156] Memory required for data: 175463328
I1106 04:39:57.076046 21715 layer_factory.hpp:77] Creating layer accuracy
I1106 04:39:57.076057 21715 net.cpp:91] Creating Layer accuracy
I1106 04:39:57.076066 21715 net.cpp:425] accuracy <- fc8_fc8_0_split_0
I1106 04:39:57.076074 21715 net.cpp:425] accuracy <- label_img_1_split_0
I1106 04:39:57.076084 21715 net.cpp:399] accuracy -> accuracy
I1106 04:39:57.076097 21715 net.cpp:141] Setting up accuracy
I1106 04:39:57.076107 21715 net.cpp:148] Top shape: (1)
I1106 04:39:57.076114 21715 net.cpp:156] Memory required for data: 175463332
I1106 04:39:57.076122 21715 layer_factory.hpp:77] Creating layer loss
I1106 04:39:57.076131 21715 net.cpp:91] Creating Layer loss
I1106 04:39:57.076139 21715 net.cpp:425] loss <- fc8_fc8_0_split_1
I1106 04:39:57.076148 21715 net.cpp:425] loss <- label_img_1_split_1
I1106 04:39:57.076158 21715 net.cpp:399] loss -> loss
I1106 04:39:57.076169 21715 layer_factory.hpp:77] Creating layer loss
I1106 04:39:57.076262 21715 net.cpp:141] Setting up loss
I1106 04:39:57.076273 21715 net.cpp:148] Top shape: (1)
I1106 04:39:57.076282 21715 net.cpp:151]     with loss weight 1
I1106 04:39:57.076300 21715 net.cpp:156] Memory required for data: 175463336
I1106 04:39:57.076310 21715 net.cpp:217] loss needs backward computation.
I1106 04:39:57.076318 21715 net.cpp:219] accuracy does not need backward computation.
I1106 04:39:57.076328 21715 net.cpp:217] fc8_fc8_0_split needs backward computation.
I1106 04:39:57.076334 21715 net.cpp:217] fc8 needs backward computation.
I1106 04:39:57.076340 21715 net.cpp:217] drop7 needs backward computation.
I1106 04:39:57.076349 21715 net.cpp:217] relu7 needs backward computation.
I1106 04:39:57.076355 21715 net.cpp:217] fc7 needs backward computation.
I1106 04:39:57.076364 21715 net.cpp:217] drop6 needs backward computation.
I1106 04:39:57.076370 21715 net.cpp:217] relu6 needs backward computation.
I1106 04:39:57.076378 21715 net.cpp:217] fc6 needs backward computation.
I1106 04:39:57.076386 21715 net.cpp:217] pool5 needs backward computation.
I1106 04:39:57.076395 21715 net.cpp:217] relu4 needs backward computation.
I1106 04:39:57.076401 21715 net.cpp:217] conv4 needs backward computation.
I1106 04:39:57.076409 21715 net.cpp:217] relu3 needs backward computation.
I1106 04:39:57.076417 21715 net.cpp:217] conv3 needs backward computation.
I1106 04:39:57.076426 21715 net.cpp:217] norm2 needs backward computation.
I1106 04:39:57.076433 21715 net.cpp:217] pool2 needs backward computation.
I1106 04:39:57.076441 21715 net.cpp:217] relu2 needs backward computation.
I1106 04:39:57.076448 21715 net.cpp:217] conv2 needs backward computation.
I1106 04:39:57.076457 21715 net.cpp:217] norm1 needs backward computation.
I1106 04:39:57.076465 21715 net.cpp:217] pool1 needs backward computation.
I1106 04:39:57.076474 21715 net.cpp:217] relu1 needs backward computation.
I1106 04:39:57.076480 21715 net.cpp:217] conv1 needs backward computation.
I1106 04:39:57.076488 21715 net.cpp:219] label_img_1_split does not need backward computation.
I1106 04:39:57.076496 21715 net.cpp:219] img does not need backward computation.
I1106 04:39:57.076504 21715 net.cpp:261] This network produces output accuracy
I1106 04:39:57.076513 21715 net.cpp:261] This network produces output loss
I1106 04:39:57.076532 21715 net.cpp:274] Network initialization done.
I1106 04:39:57.076628 21715 solver.cpp:60] Solver scaffolding done.
I1106 04:39:57.087519 21715 solver.cpp:337] Iteration 0, Testing net (#0)
I1106 04:40:04.873761 21715 solver.cpp:228] Iteration 0, loss = 3.74254
I1106 04:40:04.873806 21715 solver.cpp:244]     Train net output #0: accuracy = 0.0234375
I1106 04:40:04.873824 21715 solver.cpp:244]     Train net output #1: loss = 3.74254 (* 1 = 3.74254 loss)
I1106 04:40:04.873837 21715 sgd_solver.cpp:106] Iteration 0, lr = 0.01
>>> 2016-11-06 04:50:49.438245 Begin model classification tests
/home/kevin/catkin_ws/src/romans_stack/model_net/caffe_net/score.py:93: RuntimeWarning: invalid value encountered in true_divide
  return accuracy_max/len(dataset), accuracy_ave/len(dataset), loss/len(dataset), confusion_mat/(np.tile(confusion_mat.sum(1),(40,1))).T
>>> 2016-11-06 04:52:10.965884 Iteration 100 mean classification accuracy (max)  0.412955465587
>>> 2016-11-06 04:52:10.965912 Iteration 100 mean classification accuracy (ave) 0.417004048583
>>> 2016-11-06 04:52:10.965925 Iteration 100 mean testing loss 2.00056749531
>>> 2016-11-06 04:52:10.965943 Iteration 100 mean confusion matrix [ 1.          0.          0.03092784         nan         nan         nan
         nan         nan         nan         nan         nan         nan
         nan         nan         nan         nan         nan         nan
         nan         nan         nan         nan         nan         nan
         nan         nan         nan         nan         nan         nan
         nan         nan         nan         nan         nan         nan
         nan         nan         nan         nan]
I1106 04:52:19.482324 21715 solver.cpp:228] Iteration 100, loss = 2.38508
I1106 04:52:19.482398 21715 solver.cpp:244]     Train net output #0: accuracy = 0.34375
I1106 04:52:19.482425 21715 solver.cpp:244]     Train net output #1: loss = 2.38508 (* 1 = 2.38508 loss)
I1106 04:52:19.482452 21715 sgd_solver.cpp:106] Iteration 100, lr = 0.01
>>> 2016-11-06 05:03:21.644098 Begin model classification tests
>>> 2016-11-06 05:04:59.618226 Iteration 200 mean classification accuracy (max)  0.441295546559
>>> 2016-11-06 05:04:59.618274 Iteration 200 mean classification accuracy (ave) 0.647773279352
>>> 2016-11-06 05:04:59.618290 Iteration 200 mean testing loss 1.85658779246
>>> 2016-11-06 05:04:59.618309 Iteration 200 mean confusion matrix [        nan         nan  0.66666667  0.          0.9         0.64        0.
  1.                 nan         nan         nan         nan         nan
         nan         nan         nan         nan         nan         nan
         nan         nan         nan         nan         nan         nan
         nan         nan         nan         nan         nan         nan
         nan         nan         nan         nan         nan         nan
         nan         nan         nan]
I1106 05:05:10.806756 21715 solver.cpp:228] Iteration 200, loss = 1.98602
I1106 05:05:10.806805 21715 solver.cpp:244]     Train net output #0: accuracy = 0.445312
I1106 05:05:10.806824 21715 solver.cpp:244]     Train net output #1: loss = 1.98602 (* 1 = 1.98602 loss)
I1106 05:05:10.806859 21715 sgd_solver.cpp:106] Iteration 200, lr = 0.01
>>> 2016-11-06 05:17:24.218178 Begin model classification tests
>>> 2016-11-06 05:19:24.276625 Iteration 300 mean classification accuracy (max)  0.858299595142
>>> 2016-11-06 05:19:24.276671 Iteration 300 mean classification accuracy (ave) 0.898785425101
>>> 2016-11-06 05:19:24.276686 Iteration 300 mean testing loss 0.962067064422
>>> 2016-11-06 05:19:24.276703 Iteration 300 mean confusion matrix [        nan         nan         nan         nan         nan         nan
         nan  0.95833333  0.93        0.95        0.8         0.18181818
         nan         nan         nan         nan         nan         nan
         nan         nan         nan         nan         nan         nan
         nan         nan         nan         nan         nan         nan
         nan         nan         nan         nan         nan         nan
         nan         nan         nan         nan]
I1106 05:19:35.236996 21715 solver.cpp:228] Iteration 300, loss = 1.55452
I1106 05:19:35.237037 21715 solver.cpp:244]     Train net output #0: accuracy = 0.578125
I1106 05:19:35.237058 21715 solver.cpp:244]     Train net output #1: loss = 1.55452 (* 1 = 1.55452 loss)
I1106 05:19:35.237074 21715 sgd_solver.cpp:106] Iteration 300, lr = 0.01
>>> 2016-11-06 05:31:10.749930 Begin model classification tests
>>> 2016-11-06 05:33:01.266143 Iteration 400 mean classification accuracy (max)  0.226720647773
>>> 2016-11-06 05:33:01.266190 Iteration 400 mean classification accuracy (ave) 0.392712550607
>>> 2016-11-06 05:33:01.266205 Iteration 400 mean testing loss 2.22551593269
>>> 2016-11-06 05:33:01.266225 Iteration 400 mean confusion matrix [        nan         nan         nan         nan         nan         nan
         nan         nan         nan         nan         nan  0.55555556
  0.25581395  0.          0.47674419  0.55        0.69230769         nan
         nan         nan         nan         nan         nan         nan
         nan         nan         nan         nan         nan         nan
         nan         nan         nan         nan         nan         nan
         nan         nan         nan         nan]
I1106 05:33:08.450754 21715 solver.cpp:228] Iteration 400, loss = 1.44147
I1106 05:33:08.450824 21715 solver.cpp:244]     Train net output #0: accuracy = 0.5625
I1106 05:33:08.450855 21715 solver.cpp:244]     Train net output #1: loss = 1.44147 (* 1 = 1.44147 loss)
I1106 05:33:08.450875 21715 sgd_solver.cpp:106] Iteration 400, lr = 0.01
^CTraceback (most recent call last):
  File "./solve.py", line 27, in <module>
    solver.step(100)
  File "/home/kevin/caffeplus/python_layer/data_layers/model_net_layer.py", line 69, in reshape
    self.data, self.label = self.load_data(self.indices, self.idx)
  File "/home/kevin/caffeplus/python_layer/data_layers/model_net_layer.py", line 112, in load_data
    mat = scipy.io.loadmat('{}/{}.mat'.format(self.dataset_dir, filei))
  File "/usr/lib/python2.7/dist-packages/scipy/io/matlab/mio.py", line 125, in loadmat
    MR = mat_reader_factory(file_name, appendmat, **kwargs)
  File "/usr/lib/python2.7/dist-packages/scipy/io/matlab/mio.py", line 55, in mat_reader_factory
    mjv, mnv = get_matfile_version(byte_stream)
  File "/usr/lib/python2.7/dist-packages/scipy/io/matlab/miobase.py", line 217, in get_matfile_version
    buffer = fileobj.read(4))
KeyboardInterrupt
]0;kevin@kevin-desktop: ~/catkin_ws/src/romans_stack/model_net/caffe_netkevin@kevin-desktop:~/catkin_ws/src/romans_stack/model_net/caffe_net$ ./solve.py
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Net<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Blob<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Solver<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1106 05:34:40.582921 26139 solver.cpp:48] Initializing solver from parameters: 
train_net: "train.prototxt"
test_net: "test.prototxt"
test_iter: 0
test_interval: 9999999
base_lr: 0.01
display: 100
max_iter: 450000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 5000
snapshot: 1000
snapshot_prefix: "/home/kevin/snapshot/"
solver_mode: GPU
I1106 05:34:40.583050 26139 solver.cpp:81] Creating training net from train_net file: train.prototxt
I1106 05:34:40.583454 26139 net.cpp:49] Initializing net from parameters: 
state {
  phase: TRAIN
}
layer {
  name: "img"
  type: "Python"
  top: "img"
  top: "label"
  python_param {
    module: "data_layers.model_net_layer"
    layer: "ModelNetDataLayer"
    param_str: "{\'img_size\': (250, 250), \'dtype\': \'frame\', \'batch_size\': 256, \'seed\': 1337, \'split\': \'train\', \'dataset_dir\': \'/home/kevin/dataset/processed_data\', \'mean\': 2}"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "img"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 0
    kernel_size: 11
    group: 1
    stride: 4
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    name: "conv3_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv3_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    name: "conv4_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv4_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv4"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 40
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1106 05:34:40.584112 26139 layer_factory.hpp:77] Creating layer img
I1106 05:34:40.698846 26139 net.cpp:91] Creating Layer img
I1106 05:34:40.698875 26139 net.cpp:399] img -> img
I1106 05:34:40.698889 26139 net.cpp:399] img -> label
{'img_size': (250, 250), 'dtype': 'frame', 'batch_size': 256, 'seed': 1337, 'split': 'train', 'dataset_dir': '/home/kevin/dataset/processed_data', 'mean': 2}
I1106 05:35:08.550139 26139 net.cpp:141] Setting up img
I1106 05:35:08.550173 26139 net.cpp:148] Top shape: 256 1 250 250 (16000000)
I1106 05:35:08.550180 26139 net.cpp:148] Top shape: 256 1 (256)
I1106 05:35:08.550185 26139 net.cpp:156] Memory required for data: 64001024
I1106 05:35:08.550194 26139 layer_factory.hpp:77] Creating layer label_img_1_split
I1106 05:35:08.550215 26139 net.cpp:91] Creating Layer label_img_1_split
I1106 05:35:08.550220 26139 net.cpp:425] label_img_1_split <- label
I1106 05:35:08.550230 26139 net.cpp:399] label_img_1_split -> label_img_1_split_0
I1106 05:35:08.550241 26139 net.cpp:399] label_img_1_split -> label_img_1_split_1
I1106 05:35:08.550284 26139 net.cpp:141] Setting up label_img_1_split
I1106 05:35:08.550292 26139 net.cpp:148] Top shape: 256 1 (256)
I1106 05:35:08.550297 26139 net.cpp:148] Top shape: 256 1 (256)
I1106 05:35:08.550302 26139 net.cpp:156] Memory required for data: 64003072
I1106 05:35:08.550307 26139 layer_factory.hpp:77] Creating layer conv1
I1106 05:35:08.550321 26139 net.cpp:91] Creating Layer conv1
I1106 05:35:08.550326 26139 net.cpp:425] conv1 <- img
I1106 05:35:08.550333 26139 net.cpp:399] conv1 -> conv1
I1106 05:35:08.552058 26139 net.cpp:141] Setting up conv1
I1106 05:35:08.552073 26139 net.cpp:148] Top shape: 256 96 60 60 (88473600)
I1106 05:35:08.552078 26139 net.cpp:156] Memory required for data: 417897472
I1106 05:35:08.552090 26139 layer_factory.hpp:77] Creating layer relu1
I1106 05:35:08.552098 26139 net.cpp:91] Creating Layer relu1
I1106 05:35:08.552103 26139 net.cpp:425] relu1 <- conv1
I1106 05:35:08.552110 26139 net.cpp:386] relu1 -> conv1 (in-place)
I1106 05:35:08.552119 26139 net.cpp:141] Setting up relu1
I1106 05:35:08.552124 26139 net.cpp:148] Top shape: 256 96 60 60 (88473600)
I1106 05:35:08.552129 26139 net.cpp:156] Memory required for data: 771791872
I1106 05:35:08.552134 26139 layer_factory.hpp:77] Creating layer pool1
I1106 05:35:08.552142 26139 net.cpp:91] Creating Layer pool1
I1106 05:35:08.552146 26139 net.cpp:425] pool1 <- conv1
I1106 05:35:08.552153 26139 net.cpp:399] pool1 -> pool1
I1106 05:35:08.552199 26139 net.cpp:141] Setting up pool1
I1106 05:35:08.552207 26139 net.cpp:148] Top shape: 256 96 30 30 (22118400)
I1106 05:35:08.552211 26139 net.cpp:156] Memory required for data: 860265472
I1106 05:35:08.552217 26139 layer_factory.hpp:77] Creating layer norm1
I1106 05:35:08.552224 26139 net.cpp:91] Creating Layer norm1
I1106 05:35:08.552228 26139 net.cpp:425] norm1 <- pool1
I1106 05:35:08.552235 26139 net.cpp:399] norm1 -> norm1
I1106 05:35:08.552270 26139 net.cpp:141] Setting up norm1
I1106 05:35:08.552278 26139 net.cpp:148] Top shape: 256 96 30 30 (22118400)
I1106 05:35:08.552281 26139 net.cpp:156] Memory required for data: 948739072
I1106 05:35:08.552285 26139 layer_factory.hpp:77] Creating layer conv2
I1106 05:35:08.552296 26139 net.cpp:91] Creating Layer conv2
I1106 05:35:08.552301 26139 net.cpp:425] conv2 <- norm1
I1106 05:35:08.552309 26139 net.cpp:399] conv2 -> conv2
I1106 05:35:08.555835 26139 net.cpp:141] Setting up conv2
I1106 05:35:08.555857 26139 net.cpp:148] Top shape: 256 256 30 30 (58982400)
I1106 05:35:08.555868 26139 net.cpp:156] Memory required for data: 1184668672
I1106 05:35:08.555884 26139 layer_factory.hpp:77] Creating layer relu2
I1106 05:35:08.555897 26139 net.cpp:91] Creating Layer relu2
I1106 05:35:08.555907 26139 net.cpp:425] relu2 <- conv2
I1106 05:35:08.555918 26139 net.cpp:386] relu2 -> conv2 (in-place)
I1106 05:35:08.555932 26139 net.cpp:141] Setting up relu2
I1106 05:35:08.555943 26139 net.cpp:148] Top shape: 256 256 30 30 (58982400)
I1106 05:35:08.555953 26139 net.cpp:156] Memory required for data: 1420598272
I1106 05:35:08.555963 26139 layer_factory.hpp:77] Creating layer pool2
I1106 05:35:08.555975 26139 net.cpp:91] Creating Layer pool2
I1106 05:35:08.555985 26139 net.cpp:425] pool2 <- conv2
I1106 05:35:08.555997 26139 net.cpp:399] pool2 -> pool2
I1106 05:35:08.556046 26139 net.cpp:141] Setting up pool2
I1106 05:35:08.556061 26139 net.cpp:148] Top shape: 256 256 15 15 (14745600)
I1106 05:35:08.556071 26139 net.cpp:156] Memory required for data: 1479580672
I1106 05:35:08.556080 26139 layer_factory.hpp:77] Creating layer norm2
I1106 05:35:08.556093 26139 net.cpp:91] Creating Layer norm2
I1106 05:35:08.556102 26139 net.cpp:425] norm2 <- pool2
I1106 05:35:08.556113 26139 net.cpp:399] norm2 -> norm2
I1106 05:35:08.556154 26139 net.cpp:141] Setting up norm2
I1106 05:35:08.556169 26139 net.cpp:148] Top shape: 256 256 15 15 (14745600)
I1106 05:35:08.556177 26139 net.cpp:156] Memory required for data: 1538563072
I1106 05:35:08.556187 26139 layer_factory.hpp:77] Creating layer conv3
I1106 05:35:08.556205 26139 net.cpp:91] Creating Layer conv3
I1106 05:35:08.556215 26139 net.cpp:425] conv3 <- norm2
I1106 05:35:08.556226 26139 net.cpp:399] conv3 -> conv3
I1106 05:35:08.560420 26139 net.cpp:141] Setting up conv3
I1106 05:35:08.563501 26139 net.cpp:148] Top shape: 256 384 15 15 (22118400)
I1106 05:35:08.563516 26139 net.cpp:156] Memory required for data: 1627036672
I1106 05:35:08.563534 26139 layer_factory.hpp:77] Creating layer relu3
I1106 05:35:08.563547 26139 net.cpp:91] Creating Layer relu3
I1106 05:35:08.563557 26139 net.cpp:425] relu3 <- conv3
I1106 05:35:08.563570 26139 net.cpp:386] relu3 -> conv3 (in-place)
I1106 05:35:08.563582 26139 net.cpp:141] Setting up relu3
I1106 05:35:08.563594 26139 net.cpp:148] Top shape: 256 384 15 15 (22118400)
I1106 05:35:08.563603 26139 net.cpp:156] Memory required for data: 1715510272
I1106 05:35:08.563611 26139 layer_factory.hpp:77] Creating layer conv4
I1106 05:35:08.563627 26139 net.cpp:91] Creating Layer conv4
I1106 05:35:08.563637 26139 net.cpp:425] conv4 <- conv3
I1106 05:35:08.563649 26139 net.cpp:399] conv4 -> conv4
I1106 05:35:08.567914 26139 net.cpp:141] Setting up conv4
I1106 05:35:08.567947 26139 net.cpp:148] Top shape: 256 256 15 15 (14745600)
I1106 05:35:08.567960 26139 net.cpp:156] Memory required for data: 1774492672
I1106 05:35:08.567975 26139 layer_factory.hpp:77] Creating layer relu4
I1106 05:35:08.567989 26139 net.cpp:91] Creating Layer relu4
I1106 05:35:08.567999 26139 net.cpp:425] relu4 <- conv4
I1106 05:35:08.568013 26139 net.cpp:386] relu4 -> conv4 (in-place)
I1106 05:35:08.568027 26139 net.cpp:141] Setting up relu4
I1106 05:35:08.568039 26139 net.cpp:148] Top shape: 256 256 15 15 (14745600)
I1106 05:35:08.568049 26139 net.cpp:156] Memory required for data: 1833475072
I1106 05:35:08.568058 26139 layer_factory.hpp:77] Creating layer pool5
I1106 05:35:08.568071 26139 net.cpp:91] Creating Layer pool5
I1106 05:35:08.568080 26139 net.cpp:425] pool5 <- conv4
I1106 05:35:08.568094 26139 net.cpp:399] pool5 -> pool5
I1106 05:35:08.568142 26139 net.cpp:141] Setting up pool5
I1106 05:35:08.568156 26139 net.cpp:148] Top shape: 256 256 7 7 (3211264)
I1106 05:35:08.568166 26139 net.cpp:156] Memory required for data: 1846320128
I1106 05:35:08.568176 26139 layer_factory.hpp:77] Creating layer fc6
I1106 05:35:08.568197 26139 net.cpp:91] Creating Layer fc6
I1106 05:35:08.568207 26139 net.cpp:425] fc6 <- pool5
I1106 05:35:08.568219 26139 net.cpp:399] fc6 -> fc6
I1106 05:35:09.010442 26139 net.cpp:141] Setting up fc6
I1106 05:35:09.010486 26139 net.cpp:148] Top shape: 256 4096 (1048576)
I1106 05:35:09.010499 26139 net.cpp:156] Memory required for data: 1850514432
I1106 05:35:09.010522 26139 layer_factory.hpp:77] Creating layer relu6
I1106 05:35:09.010545 26139 net.cpp:91] Creating Layer relu6
I1106 05:35:09.010556 26139 net.cpp:425] relu6 <- fc6
I1106 05:35:09.010570 26139 net.cpp:386] relu6 -> fc6 (in-place)
I1106 05:35:09.010586 26139 net.cpp:141] Setting up relu6
I1106 05:35:09.010596 26139 net.cpp:148] Top shape: 256 4096 (1048576)
I1106 05:35:09.010606 26139 net.cpp:156] Memory required for data: 1854708736
I1106 05:35:09.010615 26139 layer_factory.hpp:77] Creating layer drop6
I1106 05:35:09.010637 26139 net.cpp:91] Creating Layer drop6
I1106 05:35:09.010648 26139 net.cpp:425] drop6 <- fc6
I1106 05:35:09.010658 26139 net.cpp:386] drop6 -> fc6 (in-place)
I1106 05:35:09.010689 26139 net.cpp:141] Setting up drop6
I1106 05:35:09.010702 26139 net.cpp:148] Top shape: 256 4096 (1048576)
I1106 05:35:09.010712 26139 net.cpp:156] Memory required for data: 1858903040
I1106 05:35:09.010721 26139 layer_factory.hpp:77] Creating layer fc7
I1106 05:35:09.010735 26139 net.cpp:91] Creating Layer fc7
I1106 05:35:09.010745 26139 net.cpp:425] fc7 <- fc6
I1106 05:35:09.010756 26139 net.cpp:399] fc7 -> fc7
I1106 05:35:09.113914 26139 net.cpp:141] Setting up fc7
I1106 05:35:09.113950 26139 net.cpp:148] Top shape: 256 4096 (1048576)
I1106 05:35:09.113956 26139 net.cpp:156] Memory required for data: 1863097344
I1106 05:35:09.113973 26139 layer_factory.hpp:77] Creating layer relu7
I1106 05:35:09.113986 26139 net.cpp:91] Creating Layer relu7
I1106 05:35:09.113996 26139 net.cpp:425] relu7 <- fc7
I1106 05:35:09.114008 26139 net.cpp:386] relu7 -> fc7 (in-place)
I1106 05:35:09.114022 26139 net.cpp:141] Setting up relu7
I1106 05:35:09.114029 26139 net.cpp:148] Top shape: 256 4096 (1048576)
I1106 05:35:09.114037 26139 net.cpp:156] Memory required for data: 1867291648
I1106 05:35:09.114044 26139 layer_factory.hpp:77] Creating layer drop7
I1106 05:35:09.114055 26139 net.cpp:91] Creating Layer drop7
I1106 05:35:09.114063 26139 net.cpp:425] drop7 <- fc7
I1106 05:35:09.114070 26139 net.cpp:386] drop7 -> fc7 (in-place)
I1106 05:35:09.114090 26139 net.cpp:141] Setting up drop7
I1106 05:35:09.114099 26139 net.cpp:148] Top shape: 256 4096 (1048576)
I1106 05:35:09.114105 26139 net.cpp:156] Memory required for data: 1871485952
I1106 05:35:09.114112 26139 layer_factory.hpp:77] Creating layer fc8
I1106 05:35:09.114123 26139 net.cpp:91] Creating Layer fc8
I1106 05:35:09.114130 26139 net.cpp:425] fc8 <- fc7
I1106 05:35:09.114140 26139 net.cpp:399] fc8 -> fc8
I1106 05:35:09.115612 26139 net.cpp:141] Setting up fc8
I1106 05:35:09.115629 26139 net.cpp:148] Top shape: 256 40 (10240)
I1106 05:35:09.115638 26139 net.cpp:156] Memory required for data: 1871526912
I1106 05:35:09.115649 26139 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I1106 05:35:09.115659 26139 net.cpp:91] Creating Layer fc8_fc8_0_split
I1106 05:35:09.115665 26139 net.cpp:425] fc8_fc8_0_split <- fc8
I1106 05:35:09.115675 26139 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1106 05:35:09.115689 26139 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1106 05:35:09.115718 26139 net.cpp:141] Setting up fc8_fc8_0_split
I1106 05:35:09.115730 26139 net.cpp:148] Top shape: 256 40 (10240)
I1106 05:35:09.115737 26139 net.cpp:148] Top shape: 256 40 (10240)
I1106 05:35:09.115744 26139 net.cpp:156] Memory required for data: 1871608832
I1106 05:35:09.115752 26139 layer_factory.hpp:77] Creating layer accuracy
I1106 05:35:09.115767 26139 net.cpp:91] Creating Layer accuracy
I1106 05:35:09.115774 26139 net.cpp:425] accuracy <- fc8_fc8_0_split_0
I1106 05:35:09.115782 26139 net.cpp:425] accuracy <- label_img_1_split_0
I1106 05:35:09.115792 26139 net.cpp:399] accuracy -> accuracy
I1106 05:35:09.115803 26139 net.cpp:141] Setting up accuracy
I1106 05:35:09.115810 26139 net.cpp:148] Top shape: (1)
I1106 05:35:09.115818 26139 net.cpp:156] Memory required for data: 1871608836
I1106 05:35:09.115824 26139 layer_factory.hpp:77] Creating layer loss
I1106 05:35:09.115836 26139 net.cpp:91] Creating Layer loss
I1106 05:35:09.115844 26139 net.cpp:425] loss <- fc8_fc8_0_split_1
I1106 05:35:09.115851 26139 net.cpp:425] loss <- label_img_1_split_1
I1106 05:35:09.115859 26139 net.cpp:399] loss -> loss
I1106 05:35:09.115870 26139 layer_factory.hpp:77] Creating layer loss
I1106 05:35:09.115962 26139 net.cpp:141] Setting up loss
I1106 05:35:09.115972 26139 net.cpp:148] Top shape: (1)
I1106 05:35:09.115978 26139 net.cpp:151]     with loss weight 1
I1106 05:35:09.115993 26139 net.cpp:156] Memory required for data: 1871608840
I1106 05:35:09.116000 26139 net.cpp:217] loss needs backward computation.
I1106 05:35:09.116008 26139 net.cpp:219] accuracy does not need backward computation.
I1106 05:35:09.116016 26139 net.cpp:217] fc8_fc8_0_split needs backward computation.
I1106 05:35:09.116024 26139 net.cpp:217] fc8 needs backward computation.
I1106 05:35:09.116030 26139 net.cpp:217] drop7 needs backward computation.
I1106 05:35:09.116037 26139 net.cpp:217] relu7 needs backward computation.
I1106 05:35:09.116044 26139 net.cpp:217] fc7 needs backward computation.
I1106 05:35:09.116051 26139 net.cpp:217] drop6 needs backward computation.
I1106 05:35:09.116058 26139 net.cpp:217] relu6 needs backward computation.
I1106 05:35:09.116065 26139 net.cpp:217] fc6 needs backward computation.
I1106 05:35:09.116072 26139 net.cpp:217] pool5 needs backward computation.
I1106 05:35:09.116080 26139 net.cpp:217] relu4 needs backward computation.
I1106 05:35:09.116086 26139 net.cpp:217] conv4 needs backward computation.
I1106 05:35:09.116094 26139 net.cpp:217] relu3 needs backward computation.
I1106 05:35:09.116101 26139 net.cpp:217] conv3 needs backward computation.
I1106 05:35:09.116108 26139 net.cpp:217] norm2 needs backward computation.
I1106 05:35:09.116116 26139 net.cpp:217] pool2 needs backward computation.
I1106 05:35:09.116122 26139 net.cpp:217] relu2 needs backward computation.
I1106 05:35:09.116127 26139 net.cpp:217] conv2 needs backward computation.
I1106 05:35:09.116135 26139 net.cpp:217] norm1 needs backward computation.
I1106 05:35:09.116142 26139 net.cpp:217] pool1 needs backward computation.
I1106 05:35:09.116150 26139 net.cpp:217] relu1 needs backward computation.
I1106 05:35:09.116158 26139 net.cpp:217] conv1 needs backward computation.
I1106 05:35:09.116166 26139 net.cpp:219] label_img_1_split does not need backward computation.
I1106 05:35:09.116174 26139 net.cpp:219] img does not need backward computation.
I1106 05:35:09.116180 26139 net.cpp:261] This network produces output accuracy
I1106 05:35:09.116189 26139 net.cpp:261] This network produces output loss
I1106 05:35:09.116205 26139 net.cpp:274] Network initialization done.
I1106 05:35:09.116719 26139 solver.cpp:181] Creating test net (#0) specified by test_net file: test.prototxt
I1106 05:35:09.116907 26139 net.cpp:49] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "img"
  type: "Python"
  top: "img"
  top: "label"
  python_param {
    module: "data_layers.model_net_layer"
    layer: "ModelNetDataLayer"
    param_str: "{\'img_size\': (250, 250), \'dtype\': \'object\', \'batch_size\': 256, \'seed\': 1337, \'split\': \'test\', \'dataset_dir\': \'/home/kevin/dataset/processed_data\', \'mean\': 2}"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "img"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 96
    pad: 0
    kernel_size: 11
    group: 1
    stride: 4
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    name: "conv3_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv3_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 384
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "conv3"
  top: "conv4"
  param {
    name: "conv4_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv4_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv4"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 40
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1106 05:35:09.118029 26139 layer_factory.hpp:77] Creating layer img
I1106 05:35:09.118077 26139 net.cpp:91] Creating Layer img
I1106 05:35:09.118088 26139 net.cpp:399] img -> img
I1106 05:35:09.118098 26139 net.cpp:399] img -> label
{'img_size': (250, 250), 'dtype': 'object', 'batch_size': 256, 'seed': 1337, 'split': 'test', 'dataset_dir': '/home/kevin/dataset/processed_data', 'mean': 2}
I1106 05:35:09.770267 26139 net.cpp:141] Setting up img
I1106 05:35:09.770304 26139 net.cpp:148] Top shape: 24 1 250 250 (1500000)
I1106 05:35:09.770313 26139 net.cpp:148] Top shape: 24 1 (24)
I1106 05:35:09.770318 26139 net.cpp:156] Memory required for data: 6000096
I1106 05:35:09.770326 26139 layer_factory.hpp:77] Creating layer label_img_1_split
I1106 05:35:09.770341 26139 net.cpp:91] Creating Layer label_img_1_split
I1106 05:35:09.770349 26139 net.cpp:425] label_img_1_split <- label
I1106 05:35:09.770357 26139 net.cpp:399] label_img_1_split -> label_img_1_split_0
I1106 05:35:09.770368 26139 net.cpp:399] label_img_1_split -> label_img_1_split_1
I1106 05:35:09.770419 26139 net.cpp:141] Setting up label_img_1_split
I1106 05:35:09.770426 26139 net.cpp:148] Top shape: 24 1 (24)
I1106 05:35:09.770432 26139 net.cpp:148] Top shape: 24 1 (24)
I1106 05:35:09.770436 26139 net.cpp:156] Memory required for data: 6000288
I1106 05:35:09.770439 26139 layer_factory.hpp:77] Creating layer conv1
I1106 05:35:09.770454 26139 net.cpp:91] Creating Layer conv1
I1106 05:35:09.770459 26139 net.cpp:425] conv1 <- img
I1106 05:35:09.770467 26139 net.cpp:399] conv1 -> conv1
I1106 05:35:09.770822 26139 net.cpp:141] Setting up conv1
I1106 05:35:09.770833 26139 net.cpp:148] Top shape: 24 96 60 60 (8294400)
I1106 05:35:09.770836 26139 net.cpp:156] Memory required for data: 39177888
I1106 05:35:09.770848 26139 layer_factory.hpp:77] Creating layer relu1
I1106 05:35:09.770856 26139 net.cpp:91] Creating Layer relu1
I1106 05:35:09.770860 26139 net.cpp:425] relu1 <- conv1
I1106 05:35:09.770866 26139 net.cpp:386] relu1 -> conv1 (in-place)
I1106 05:35:09.770874 26139 net.cpp:141] Setting up relu1
I1106 05:35:09.770880 26139 net.cpp:148] Top shape: 24 96 60 60 (8294400)
I1106 05:35:09.770884 26139 net.cpp:156] Memory required for data: 72355488
I1106 05:35:09.770889 26139 layer_factory.hpp:77] Creating layer pool1
I1106 05:35:09.770897 26139 net.cpp:91] Creating Layer pool1
I1106 05:35:09.770902 26139 net.cpp:425] pool1 <- conv1
I1106 05:35:09.770908 26139 net.cpp:399] pool1 -> pool1
I1106 05:35:09.770948 26139 net.cpp:141] Setting up pool1
I1106 05:35:09.770956 26139 net.cpp:148] Top shape: 24 96 30 30 (2073600)
I1106 05:35:09.770961 26139 net.cpp:156] Memory required for data: 80649888
I1106 05:35:09.770964 26139 layer_factory.hpp:77] Creating layer norm1
I1106 05:35:09.770973 26139 net.cpp:91] Creating Layer norm1
I1106 05:35:09.770977 26139 net.cpp:425] norm1 <- pool1
I1106 05:35:09.770983 26139 net.cpp:399] norm1 -> norm1
I1106 05:35:09.771018 26139 net.cpp:141] Setting up norm1
I1106 05:35:09.771024 26139 net.cpp:148] Top shape: 24 96 30 30 (2073600)
I1106 05:35:09.771028 26139 net.cpp:156] Memory required for data: 88944288
I1106 05:35:09.771034 26139 layer_factory.hpp:77] Creating layer conv2
I1106 05:35:09.771044 26139 net.cpp:91] Creating Layer conv2
I1106 05:35:09.771047 26139 net.cpp:425] conv2 <- norm1
I1106 05:35:09.771054 26139 net.cpp:399] conv2 -> conv2
I1106 05:35:09.774169 26139 net.cpp:141] Setting up conv2
I1106 05:35:09.774194 26139 net.cpp:148] Top shape: 24 256 30 30 (5529600)
I1106 05:35:09.774200 26139 net.cpp:156] Memory required for data: 111062688
I1106 05:35:09.774214 26139 layer_factory.hpp:77] Creating layer relu2
I1106 05:35:09.774224 26139 net.cpp:91] Creating Layer relu2
I1106 05:35:09.774229 26139 net.cpp:425] relu2 <- conv2
I1106 05:35:09.774235 26139 net.cpp:386] relu2 -> conv2 (in-place)
I1106 05:35:09.774245 26139 net.cpp:141] Setting up relu2
I1106 05:35:09.774250 26139 net.cpp:148] Top shape: 24 256 30 30 (5529600)
I1106 05:35:09.774255 26139 net.cpp:156] Memory required for data: 133181088
I1106 05:35:09.774260 26139 layer_factory.hpp:77] Creating layer pool2
I1106 05:35:09.774268 26139 net.cpp:91] Creating Layer pool2
I1106 05:35:09.774272 26139 net.cpp:425] pool2 <- conv2
I1106 05:35:09.774279 26139 net.cpp:399] pool2 -> pool2
I1106 05:35:09.774320 26139 net.cpp:141] Setting up pool2
I1106 05:35:09.774328 26139 net.cpp:148] Top shape: 24 256 15 15 (1382400)
I1106 05:35:09.774333 26139 net.cpp:156] Memory required for data: 138710688
I1106 05:35:09.774338 26139 layer_factory.hpp:77] Creating layer norm2
I1106 05:35:09.774345 26139 net.cpp:91] Creating Layer norm2
I1106 05:35:09.774350 26139 net.cpp:425] norm2 <- pool2
I1106 05:35:09.774355 26139 net.cpp:399] norm2 -> norm2
I1106 05:35:09.774390 26139 net.cpp:141] Setting up norm2
I1106 05:35:09.774397 26139 net.cpp:148] Top shape: 24 256 15 15 (1382400)
I1106 05:35:09.774401 26139 net.cpp:156] Memory required for data: 144240288
I1106 05:35:09.774406 26139 layer_factory.hpp:77] Creating layer conv3
I1106 05:35:09.774416 26139 net.cpp:91] Creating Layer conv3
I1106 05:35:09.774421 26139 net.cpp:425] conv3 <- norm2
I1106 05:35:09.774428 26139 net.cpp:399] conv3 -> conv3
I1106 05:35:09.778512 26139 net.cpp:141] Setting up conv3
I1106 05:35:09.778527 26139 net.cpp:148] Top shape: 24 384 15 15 (2073600)
I1106 05:35:09.778532 26139 net.cpp:156] Memory required for data: 152534688
I1106 05:35:09.778542 26139 layer_factory.hpp:77] Creating layer relu3
I1106 05:35:09.778549 26139 net.cpp:91] Creating Layer relu3
I1106 05:35:09.778553 26139 net.cpp:425] relu3 <- conv3
I1106 05:35:09.778560 26139 net.cpp:386] relu3 -> conv3 (in-place)
I1106 05:35:09.778568 26139 net.cpp:141] Setting up relu3
I1106 05:35:09.778573 26139 net.cpp:148] Top shape: 24 384 15 15 (2073600)
I1106 05:35:09.778578 26139 net.cpp:156] Memory required for data: 160829088
I1106 05:35:09.778583 26139 layer_factory.hpp:77] Creating layer conv4
I1106 05:35:09.778592 26139 net.cpp:91] Creating Layer conv4
I1106 05:35:09.778597 26139 net.cpp:425] conv4 <- conv3
I1106 05:35:09.778604 26139 net.cpp:399] conv4 -> conv4
I1106 05:35:09.782691 26139 net.cpp:141] Setting up conv4
I1106 05:35:09.782716 26139 net.cpp:148] Top shape: 24 256 15 15 (1382400)
I1106 05:35:09.782726 26139 net.cpp:156] Memory required for data: 166358688
I1106 05:35:09.782738 26139 layer_factory.hpp:77] Creating layer relu4
I1106 05:35:09.782752 26139 net.cpp:91] Creating Layer relu4
I1106 05:35:09.782762 26139 net.cpp:425] relu4 <- conv4
I1106 05:35:09.782773 26139 net.cpp:386] relu4 -> conv4 (in-place)
I1106 05:35:09.782786 26139 net.cpp:141] Setting up relu4
I1106 05:35:09.782798 26139 net.cpp:148] Top shape: 24 256 15 15 (1382400)
I1106 05:35:09.782807 26139 net.cpp:156] Memory required for data: 171888288
I1106 05:35:09.782816 26139 layer_factory.hpp:77] Creating layer pool5
I1106 05:35:09.782827 26139 net.cpp:91] Creating Layer pool5
I1106 05:35:09.782836 26139 net.cpp:425] pool5 <- conv4
I1106 05:35:09.782848 26139 net.cpp:399] pool5 -> pool5
I1106 05:35:09.782896 26139 net.cpp:141] Setting up pool5
I1106 05:35:09.782908 26139 net.cpp:148] Top shape: 24 256 7 7 (301056)
I1106 05:35:09.782918 26139 net.cpp:156] Memory required for data: 173092512
I1106 05:35:09.782927 26139 layer_factory.hpp:77] Creating layer fc6
I1106 05:35:09.782941 26139 net.cpp:91] Creating Layer fc6
I1106 05:35:09.782951 26139 net.cpp:425] fc6 <- pool5
I1106 05:35:09.782963 26139 net.cpp:399] fc6 -> fc6
I1106 05:35:10.142127 26139 net.cpp:141] Setting up fc6
I1106 05:35:10.142176 26139 net.cpp:148] Top shape: 24 4096 (98304)
I1106 05:35:10.142200 26139 net.cpp:156] Memory required for data: 173485728
I1106 05:35:10.142225 26139 layer_factory.hpp:77] Creating layer relu6
I1106 05:35:10.142247 26139 net.cpp:91] Creating Layer relu6
I1106 05:35:10.142259 26139 net.cpp:425] relu6 <- fc6
I1106 05:35:10.142273 26139 net.cpp:386] relu6 -> fc6 (in-place)
I1106 05:35:10.142290 26139 net.cpp:141] Setting up relu6
I1106 05:35:10.142303 26139 net.cpp:148] Top shape: 24 4096 (98304)
I1106 05:35:10.142313 26139 net.cpp:156] Memory required for data: 173878944
I1106 05:35:10.142323 26139 layer_factory.hpp:77] Creating layer drop6
I1106 05:35:10.142336 26139 net.cpp:91] Creating Layer drop6
I1106 05:35:10.142345 26139 net.cpp:425] drop6 <- fc6
I1106 05:35:10.142357 26139 net.cpp:386] drop6 -> fc6 (in-place)
I1106 05:35:10.142397 26139 net.cpp:141] Setting up drop6
I1106 05:35:10.142413 26139 net.cpp:148] Top shape: 24 4096 (98304)
I1106 05:35:10.142423 26139 net.cpp:156] Memory required for data: 174272160
I1106 05:35:10.142433 26139 layer_factory.hpp:77] Creating layer fc7
I1106 05:35:10.142448 26139 net.cpp:91] Creating Layer fc7
I1106 05:35:10.142458 26139 net.cpp:425] fc7 <- fc6
I1106 05:35:10.142470 26139 net.cpp:399] fc7 -> fc7
I1106 05:35:10.286674 26139 net.cpp:141] Setting up fc7
I1106 05:35:10.286720 26139 net.cpp:148] Top shape: 24 4096 (98304)
I1106 05:35:10.286731 26139 net.cpp:156] Memory required for data: 174665376
I1106 05:35:10.286752 26139 layer_factory.hpp:77] Creating layer relu7
I1106 05:35:10.286772 26139 net.cpp:91] Creating Layer relu7
I1106 05:35:10.286784 26139 net.cpp:425] relu7 <- fc7
I1106 05:35:10.286798 26139 net.cpp:386] relu7 -> fc7 (in-place)
I1106 05:35:10.286814 26139 net.cpp:141] Setting up relu7
I1106 05:35:10.286825 26139 net.cpp:148] Top shape: 24 4096 (98304)
I1106 05:35:10.286835 26139 net.cpp:156] Memory required for data: 175058592
I1106 05:35:10.286844 26139 layer_factory.hpp:77] Creating layer drop7
I1106 05:35:10.286857 26139 net.cpp:91] Creating Layer drop7
I1106 05:35:10.286866 26139 net.cpp:425] drop7 <- fc7
I1106 05:35:10.286877 26139 net.cpp:386] drop7 -> fc7 (in-place)
I1106 05:35:10.286916 26139 net.cpp:141] Setting up drop7
I1106 05:35:10.286929 26139 net.cpp:148] Top shape: 24 4096 (98304)
I1106 05:35:10.286938 26139 net.cpp:156] Memory required for data: 175451808
I1106 05:35:10.286947 26139 layer_factory.hpp:77] Creating layer fc8
I1106 05:35:10.286962 26139 net.cpp:91] Creating Layer fc8
I1106 05:35:10.286970 26139 net.cpp:425] fc8 <- fc7
I1106 05:35:10.286983 26139 net.cpp:399] fc8 -> fc8
I1106 05:35:10.288949 26139 net.cpp:141] Setting up fc8
I1106 05:35:10.288972 26139 net.cpp:148] Top shape: 24 40 (960)
I1106 05:35:10.288983 26139 net.cpp:156] Memory required for data: 175455648
I1106 05:35:10.289064 26139 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I1106 05:35:10.289079 26139 net.cpp:91] Creating Layer fc8_fc8_0_split
I1106 05:35:10.289089 26139 net.cpp:425] fc8_fc8_0_split <- fc8
I1106 05:35:10.289101 26139 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1106 05:35:10.289119 26139 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1106 05:35:10.289170 26139 net.cpp:141] Setting up fc8_fc8_0_split
I1106 05:35:10.289185 26139 net.cpp:148] Top shape: 24 40 (960)
I1106 05:35:10.289193 26139 net.cpp:148] Top shape: 24 40 (960)
I1106 05:35:10.289203 26139 net.cpp:156] Memory required for data: 175463328
I1106 05:35:10.289212 26139 layer_factory.hpp:77] Creating layer accuracy
I1106 05:35:10.289224 26139 net.cpp:91] Creating Layer accuracy
I1106 05:35:10.289234 26139 net.cpp:425] accuracy <- fc8_fc8_0_split_0
I1106 05:35:10.289245 26139 net.cpp:425] accuracy <- label_img_1_split_0
I1106 05:35:10.289257 26139 net.cpp:399] accuracy -> accuracy
I1106 05:35:10.289271 26139 net.cpp:141] Setting up accuracy
I1106 05:35:10.289283 26139 net.cpp:148] Top shape: (1)
I1106 05:35:10.289293 26139 net.cpp:156] Memory required for data: 175463332
I1106 05:35:10.289301 26139 layer_factory.hpp:77] Creating layer loss
I1106 05:35:10.289312 26139 net.cpp:91] Creating Layer loss
I1106 05:35:10.289322 26139 net.cpp:425] loss <- fc8_fc8_0_split_1
I1106 05:35:10.289333 26139 net.cpp:425] loss <- label_img_1_split_1
I1106 05:35:10.289343 26139 net.cpp:399] loss -> loss
I1106 05:35:10.289358 26139 layer_factory.hpp:77] Creating layer loss
I1106 05:35:10.289465 26139 net.cpp:141] Setting up loss
I1106 05:35:10.289479 26139 net.cpp:148] Top shape: (1)
I1106 05:35:10.289487 26139 net.cpp:151]     with loss weight 1
I1106 05:35:10.289506 26139 net.cpp:156] Memory required for data: 175463336
I1106 05:35:10.289515 26139 net.cpp:217] loss needs backward computation.
I1106 05:35:10.289525 26139 net.cpp:219] accuracy does not need backward computation.
I1106 05:35:10.289535 26139 net.cpp:217] fc8_fc8_0_split needs backward computation.
I1106 05:35:10.289546 26139 net.cpp:217] fc8 needs backward computation.
I1106 05:35:10.289554 26139 net.cpp:217] drop7 needs backward computation.
I1106 05:35:10.289563 26139 net.cpp:217] relu7 needs backward computation.
I1106 05:35:10.289573 26139 net.cpp:217] fc7 needs backward computation.
I1106 05:35:10.289582 26139 net.cpp:217] drop6 needs backward computation.
I1106 05:35:10.289592 26139 net.cpp:217] relu6 needs backward computation.
I1106 05:35:10.289600 26139 net.cpp:217] fc6 needs backward computation.
I1106 05:35:10.289610 26139 net.cpp:217] pool5 needs backward computation.
I1106 05:35:10.289619 26139 net.cpp:217] relu4 needs backward computation.
I1106 05:35:10.289629 26139 net.cpp:217] conv4 needs backward computation.
I1106 05:35:10.289639 26139 net.cpp:217] relu3 needs backward computation.
I1106 05:35:10.289649 26139 net.cpp:217] conv3 needs backward computation.
I1106 05:35:10.289659 26139 net.cpp:217] norm2 needs backward computation.
I1106 05:35:10.289669 26139 net.cpp:217] pool2 needs backward computation.
I1106 05:35:10.289677 26139 net.cpp:217] relu2 needs backward computation.
I1106 05:35:10.289686 26139 net.cpp:217] conv2 needs backward computation.
I1106 05:35:10.289696 26139 net.cpp:217] norm1 needs backward computation.
I1106 05:35:10.289706 26139 net.cpp:217] pool1 needs backward computation.
I1106 05:35:10.289716 26139 net.cpp:217] relu1 needs backward computation.
I1106 05:35:10.289726 26139 net.cpp:217] conv1 needs backward computation.
I1106 05:35:10.289736 26139 net.cpp:219] label_img_1_split does not need backward computation.
I1106 05:35:10.289746 26139 net.cpp:219] img does not need backward computation.
I1106 05:35:10.289754 26139 net.cpp:261] This network produces output accuracy
I1106 05:35:10.289764 26139 net.cpp:261] This network produces output loss
I1106 05:35:10.289788 26139 net.cpp:274] Network initialization done.
I1106 05:35:10.289891 26139 solver.cpp:60] Solver scaffolding done.
I1106 05:35:10.308521 26139 solver.cpp:337] Iteration 0, Testing net (#0)
I1106 05:35:13.677397 26139 solver.cpp:228] Iteration 0, loss = 3.73399
I1106 05:35:13.677439 26139 solver.cpp:244]     Train net output #0: accuracy = 0.03125
I1106 05:35:13.677454 26139 solver.cpp:244]     Train net output #1: loss = 3.73399 (* 1 = 3.73399 loss)
I1106 05:35:13.677465 26139 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1106 05:46:09.740970 26139 solver.cpp:228] Iteration 100, loss = 2.35562
I1106 05:46:09.741014 26139 solver.cpp:244]     Train net output #0: accuracy = 0.359375
I1106 05:46:09.741034 26139 solver.cpp:244]     Train net output #1: loss = 2.35562 (* 1 = 2.35562 loss)
I1106 05:46:09.741051 26139 sgd_solver.cpp:106] Iteration 100, lr = 0.01
I1106 05:54:19.447374 26139 solver.cpp:228] Iteration 200, loss = 1.87605
I1106 05:54:19.447418 26139 solver.cpp:244]     Train net output #0: accuracy = 0.484375
I1106 05:54:19.447427 26139 solver.cpp:244]     Train net output #1: loss = 1.87605 (* 1 = 1.87605 loss)
I1106 05:54:19.447446 26139 sgd_solver.cpp:106] Iteration 200, lr = 0.01
I1106 06:02:43.352154 26139 solver.cpp:228] Iteration 300, loss = 1.61854
I1106 06:02:43.352185 26139 solver.cpp:244]     Train net output #0: accuracy = 0.5625
I1106 06:02:43.352200 26139 solver.cpp:244]     Train net output #1: loss = 1.61854 (* 1 = 1.61854 loss)
I1106 06:02:43.352227 26139 sgd_solver.cpp:106] Iteration 300, lr = 0.01
I1106 06:11:00.735790 26139 solver.cpp:228] Iteration 400, loss = 1.4611
I1106 06:11:00.735821 26139 solver.cpp:244]     Train net output #0: accuracy = 0.574219
I1106 06:11:00.735829 26139 solver.cpp:244]     Train net output #1: loss = 1.4611 (* 1 = 1.4611 loss)
I1106 06:11:00.735836 26139 sgd_solver.cpp:106] Iteration 400, lr = 0.01
I1106 06:20:08.469281 26139 solver.cpp:228] Iteration 500, loss = 1.35493
I1106 06:20:08.469319 26139 solver.cpp:244]     Train net output #0: accuracy = 0.609375
I1106 06:20:08.469331 26139 solver.cpp:244]     Train net output #1: loss = 1.35493 (* 1 = 1.35493 loss)
I1106 06:20:08.469337 26139 sgd_solver.cpp:106] Iteration 500, lr = 0.01
I1106 06:29:23.054868 26139 solver.cpp:228] Iteration 600, loss = 1.17549
I1106 06:29:23.054900 26139 solver.cpp:244]     Train net output #0: accuracy = 0.683594
I1106 06:29:23.054910 26139 solver.cpp:244]     Train net output #1: loss = 1.17549 (* 1 = 1.17549 loss)
I1106 06:29:23.054918 26139 sgd_solver.cpp:106] Iteration 600, lr = 0.01
I1106 06:38:45.901090 26139 solver.cpp:228] Iteration 700, loss = 1.21621
I1106 06:38:45.901123 26139 solver.cpp:244]     Train net output #0: accuracy = 0.652344
I1106 06:38:45.901131 26139 solver.cpp:244]     Train net output #1: loss = 1.21621 (* 1 = 1.21621 loss)
I1106 06:38:45.901139 26139 sgd_solver.cpp:106] Iteration 700, lr = 0.01
I1106 06:49:19.846101 26139 solver.cpp:228] Iteration 800, loss = 0.962862
I1106 06:49:19.846140 26139 solver.cpp:244]     Train net output #0: accuracy = 0.722656
I1106 06:49:19.846150 26139 solver.cpp:244]     Train net output #1: loss = 0.962862 (* 1 = 0.962862 loss)
I1106 06:49:19.846158 26139 sgd_solver.cpp:106] Iteration 800, lr = 0.01
I1106 07:00:29.159888 26139 solver.cpp:228] Iteration 900, loss = 1.00518
I1106 07:00:29.159917 26139 solver.cpp:244]     Train net output #0: accuracy = 0.703125
I1106 07:00:29.159943 26139 solver.cpp:244]     Train net output #1: loss = 1.00518 (* 1 = 1.00518 loss)
I1106 07:00:29.159950 26139 sgd_solver.cpp:106] Iteration 900, lr = 0.01
I1106 07:12:41.996057 26139 solver.cpp:454] Snapshotting to binary proto file /home/kevin/snapshot/_iter_1000.caffemodel
I1106 07:13:00.663759 26139 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/kevin/snapshot/_iter_1000.solverstate
>>> 2016-11-06 07:13:00.899447 Begin model classification tests
>>> 2016-11-06 07:25:32.533053 Iteration 1000 mean classification accuracy (max)  0.763488843813
>>> 2016-11-06 07:25:32.533087 Iteration 1000 mean classification accuracy (ave) 0.802839756592
>>> 2016-11-06 07:25:32.533095 Iteration 1000 mean testing loss 0.989658864253
>>> 2016-11-06 07:25:32.533107 Iteration 1000 mean confusion matrix
[ 1.          0.54        0.92        0.3         0.89        0.94        0.5
  0.98        0.81        0.95        0.85        0.55        0.29069767
  0.35        0.51162791  0.1         0.9         0.96        0.7         0.75
  1.          0.84        0.92        0.45348837  0.89473684  0.88
  0.96938776  0.45        0.94        0.85        0.99        0.7         0.55
  0.99        0.85        0.98        0.46        0.87        0.25        0.55      ]
I1106 07:25:43.640029 26139 solver.cpp:228] Iteration 1000, loss = 0.963968
I1106 07:25:43.640059 26139 solver.cpp:244]     Train net output #0: accuracy = 0.714844
I1106 07:25:43.640069 26139 solver.cpp:244]     Train net output #1: loss = 0.963968 (* 1 = 0.963968 loss)
I1106 07:25:43.640075 26139 sgd_solver.cpp:106] Iteration 1000, lr = 0.01
I1106 07:39:53.299666 26139 solver.cpp:228] Iteration 1100, loss = 0.819367
I1106 07:39:53.299697 26139 solver.cpp:244]     Train net output #0: accuracy = 0.761719
I1106 07:39:53.299712 26139 solver.cpp:244]     Train net output #1: loss = 0.819367 (* 1 = 0.819367 loss)
I1106 07:39:53.299721 26139 sgd_solver.cpp:106] Iteration 1100, lr = 0.01
I1106 07:49:33.473187 26139 solver.cpp:228] Iteration 1200, loss = 0.959171
I1106 07:49:33.473219 26139 solver.cpp:244]     Train net output #0: accuracy = 0.710938
I1106 07:49:33.473237 26139 solver.cpp:244]     Train net output #1: loss = 0.959171 (* 1 = 0.959171 loss)
I1106 07:49:33.473244 26139 sgd_solver.cpp:106] Iteration 1200, lr = 0.01
I1106 07:59:27.907939 26139 solver.cpp:228] Iteration 1300, loss = 0.91464
I1106 07:59:27.907970 26139 solver.cpp:244]     Train net output #0: accuracy = 0.722656
I1106 07:59:27.907980 26139 solver.cpp:244]     Train net output #1: loss = 0.91464 (* 1 = 0.91464 loss)
I1106 07:59:27.907987 26139 sgd_solver.cpp:106] Iteration 1300, lr = 0.01
I1106 08:10:38.707655 26139 solver.cpp:228] Iteration 1400, loss = 0.872367
I1106 08:10:38.707684 26139 solver.cpp:244]     Train net output #0: accuracy = 0.730469
I1106 08:10:38.707693 26139 solver.cpp:244]     Train net output #1: loss = 0.872367 (* 1 = 0.872367 loss)
I1106 08:10:38.707700 26139 sgd_solver.cpp:106] Iteration 1400, lr = 0.01
I1106 08:27:49.676422 26139 solver.cpp:228] Iteration 1500, loss = 0.782936
I1106 08:27:49.676455 26139 solver.cpp:244]     Train net output #0: accuracy = 0.769531
I1106 08:27:49.676481 26139 solver.cpp:244]     Train net output #1: loss = 0.782936 (* 1 = 0.782936 loss)
I1106 08:27:49.676488 26139 sgd_solver.cpp:106] Iteration 1500, lr = 0.01
I1106 08:39:49.567529 26139 solver.cpp:228] Iteration 1600, loss = 0.787232
I1106 08:39:49.567564 26139 solver.cpp:244]     Train net output #0: accuracy = 0.761719
I1106 08:39:49.567572 26139 solver.cpp:244]     Train net output #1: loss = 0.787232 (* 1 = 0.787232 loss)
I1106 08:39:49.567579 26139 sgd_solver.cpp:106] Iteration 1600, lr = 0.01
I1106 08:50:28.648866 26139 solver.cpp:228] Iteration 1700, loss = 0.801344
I1106 08:50:28.648896 26139 solver.cpp:244]     Train net output #0: accuracy = 0.730469
I1106 08:50:28.648905 26139 solver.cpp:244]     Train net output #1: loss = 0.801344 (* 1 = 0.801344 loss)
I1106 08:50:28.648911 26139 sgd_solver.cpp:106] Iteration 1700, lr = 0.01
I1106 09:04:17.437528 26139 solver.cpp:228] Iteration 1800, loss = 0.705915
I1106 09:04:17.437561 26139 solver.cpp:244]     Train net output #0: accuracy = 0.777344
I1106 09:04:17.437587 26139 solver.cpp:244]     Train net output #1: loss = 0.705915 (* 1 = 0.705915 loss)
I1106 09:04:17.437607 26139 sgd_solver.cpp:106] Iteration 1800, lr = 0.01
I1106 09:16:29.994756 26139 solver.cpp:228] Iteration 1900, loss = 0.812441
I1106 09:16:29.994786 26139 solver.cpp:244]     Train net output #0: accuracy = 0.738281
I1106 09:16:29.994794 26139 solver.cpp:244]     Train net output #1: loss = 0.812441 (* 1 = 0.812441 loss)
I1106 09:16:29.994801 26139 sgd_solver.cpp:106] Iteration 1900, lr = 0.01
I1106 09:27:26.074899 26139 solver.cpp:454] Snapshotting to binary proto file /home/kevin/snapshot/_iter_2000.caffemodel
I1106 09:27:49.788234 26139 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/kevin/snapshot/_iter_2000.solverstate
>>> 2016-11-06 09:27:50.018678 Begin model classification tests
>>> 2016-11-06 09:40:30.253759 Iteration 2000 mean classification accuracy (max)  0.818661257606
>>> 2016-11-06 09:40:30.253793 Iteration 2000 mean classification accuracy (ave) 0.865314401623
>>> 2016-11-06 09:40:30.253802 Iteration 2000 mean testing loss 0.755021959627
>>> 2016-11-06 09:40:30.253814 Iteration 2000 mean confusion matrix
[ 1.          0.88        0.95        0.3         0.92        0.9         0.55
  0.98        0.87        0.95        0.95        0.8         0.70930233
  0.8         0.69767442  0.45        0.89        0.94        0.85        0.8
  1.          0.92        0.95        0.44186047  1.          0.89
  0.95918367  0.65        0.95        0.85        0.99        0.95        0.75
  0.84        0.9         1.          0.72        1.          0.15        0.6       ]
I1106 09:40:40.098589 26139 solver.cpp:228] Iteration 2000, loss = 0.641258
I1106 09:40:40.098630 26139 solver.cpp:244]     Train net output #0: accuracy = 0.816406
I1106 09:40:40.098639 26139 solver.cpp:244]     Train net output #1: loss = 0.641258 (* 1 = 0.641258 loss)
I1106 09:40:40.098649 26139 sgd_solver.cpp:106] Iteration 2000, lr = 0.01
I1106 09:51:44.593588 26139 solver.cpp:228] Iteration 2100, loss = 0.692037
I1106 09:51:44.593628 26139 solver.cpp:244]     Train net output #0: accuracy = 0.792969
I1106 09:51:44.593646 26139 solver.cpp:244]     Train net output #1: loss = 0.692037 (* 1 = 0.692037 loss)
I1106 09:51:44.593667 26139 sgd_solver.cpp:106] Iteration 2100, lr = 0.01
I1106 10:08:53.361049 26139 solver.cpp:228] Iteration 2200, loss = 0.787959
I1106 10:08:53.361079 26139 solver.cpp:244]     Train net output #0: accuracy = 0.761719
I1106 10:08:53.361088 26139 solver.cpp:244]     Train net output #1: loss = 0.787959 (* 1 = 0.787959 loss)
I1106 10:08:53.361094 26139 sgd_solver.cpp:106] Iteration 2200, lr = 0.01
I1106 10:25:56.596369 26139 solver.cpp:228] Iteration 2300, loss = 0.590398
I1106 10:25:56.596400 26139 solver.cpp:244]     Train net output #0: accuracy = 0.824219
I1106 10:25:56.596407 26139 solver.cpp:244]     Train net output #1: loss = 0.590398 (* 1 = 0.590398 loss)
I1106 10:25:56.596426 26139 sgd_solver.cpp:106] Iteration 2300, lr = 0.01
I1106 10:43:11.858021 26139 solver.cpp:228] Iteration 2400, loss = 0.603829
I1106 10:43:11.858050 26139 solver.cpp:244]     Train net output #0: accuracy = 0.832031
I1106 10:43:11.858059 26139 solver.cpp:244]     Train net output #1: loss = 0.603829 (* 1 = 0.603829 loss)
I1106 10:43:11.858067 26139 sgd_solver.cpp:106] Iteration 2400, lr = 0.01
I1106 11:00:29.561789 26139 solver.cpp:228] Iteration 2500, loss = 0.428728
I1106 11:00:29.561818 26139 solver.cpp:244]     Train net output #0: accuracy = 0.882812
I1106 11:00:29.561827 26139 solver.cpp:244]     Train net output #1: loss = 0.428728 (* 1 = 0.428728 loss)
I1106 11:00:29.561835 26139 sgd_solver.cpp:106] Iteration 2500, lr = 0.01
I1106 11:17:35.990988 26139 solver.cpp:228] Iteration 2600, loss = 0.694645
I1106 11:17:35.991024 26139 solver.cpp:244]     Train net output #0: accuracy = 0.773438
I1106 11:17:35.991034 26139 solver.cpp:244]     Train net output #1: loss = 0.694645 (* 1 = 0.694645 loss)
I1106 11:17:35.991041 26139 sgd_solver.cpp:106] Iteration 2600, lr = 0.01
I1106 11:34:38.260572 26139 solver.cpp:228] Iteration 2700, loss = 0.527185
I1106 11:34:38.260604 26139 solver.cpp:244]     Train net output #0: accuracy = 0.867188
I1106 11:34:38.260613 26139 solver.cpp:244]     Train net output #1: loss = 0.527185 (* 1 = 0.527185 loss)
I1106 11:34:38.260620 26139 sgd_solver.cpp:106] Iteration 2700, lr = 0.01
I1106 11:51:19.584769 26139 solver.cpp:228] Iteration 2800, loss = 0.621738
I1106 11:51:19.584801 26139 solver.cpp:244]     Train net output #0: accuracy = 0.804688
I1106 11:51:19.584818 26139 solver.cpp:244]     Train net output #1: loss = 0.621738 (* 1 = 0.621738 loss)
I1106 11:51:19.584825 26139 sgd_solver.cpp:106] Iteration 2800, lr = 0.01
I1106 12:08:04.081614 26139 solver.cpp:228] Iteration 2900, loss = 0.562767
I1106 12:08:04.081646 26139 solver.cpp:244]     Train net output #0: accuracy = 0.851562
I1106 12:08:04.081655 26139 solver.cpp:244]     Train net output #1: loss = 0.562767 (* 1 = 0.562767 loss)
I1106 12:08:04.081662 26139 sgd_solver.cpp:106] Iteration 2900, lr = 0.01
I1106 12:24:58.867847 26139 solver.cpp:454] Snapshotting to binary proto file /home/kevin/snapshot/_iter_3000.caffemodel
I1106 12:25:24.037035 26139 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/kevin/snapshot/_iter_3000.solverstate
>>> 2016-11-06 12:25:24.266934 Begin model classification tests
>>> 2016-11-06 12:37:59.034791 Iteration 3000 mean classification accuracy (max)  0.841784989858
>>> 2016-11-06 12:37:59.034829 Iteration 3000 mean classification accuracy (ave) 0.871805273834
>>> 2016-11-06 12:37:59.034845 Iteration 3000 mean testing loss 0.693684092049
>>> 2016-11-06 12:37:59.034860 Iteration 3000 mean confusion matrix
[ 1.          0.82        0.93        0.4         0.92        0.96        0.7
  0.98        0.87        0.95        0.9         0.8         0.70930233
  0.75        0.68604651  0.4         0.9         0.96        0.85        0.75
  1.          0.9         0.96        0.41860465  0.89473684  0.92
  0.97959184  0.6         0.94        0.8         0.99        0.9         0.8
  0.85        0.9         1.          0.78        1.          0.4         0.75      ]
I1106 12:38:08.426300 26139 solver.cpp:228] Iteration 3000, loss = 0.569819
I1106 12:38:08.426345 26139 solver.cpp:244]     Train net output #0: accuracy = 0.804688
I1106 12:38:08.426354 26139 solver.cpp:244]     Train net output #1: loss = 0.569819 (* 1 = 0.569819 loss)
I1106 12:38:08.426369 26139 sgd_solver.cpp:106] Iteration 3000, lr = 0.01
I1106 12:50:15.125771 26139 solver.cpp:228] Iteration 3100, loss = 0.509507
I1106 12:50:15.125802 26139 solver.cpp:244]     Train net output #0: accuracy = 0.828125
I1106 12:50:15.125810 26139 solver.cpp:244]     Train net output #1: loss = 0.509507 (* 1 = 0.509507 loss)
I1106 12:50:15.125818 26139 sgd_solver.cpp:106] Iteration 3100, lr = 0.01
I1106 13:07:22.213759 26139 solver.cpp:228] Iteration 3200, loss = 0.563382
I1106 13:07:22.213790 26139 solver.cpp:244]     Train net output #0: accuracy = 0.824219
I1106 13:07:22.213799 26139 solver.cpp:244]     Train net output #1: loss = 0.563382 (* 1 = 0.563382 loss)
I1106 13:07:22.213805 26139 sgd_solver.cpp:106] Iteration 3200, lr = 0.01
I1106 13:22:55.252539 26139 solver.cpp:228] Iteration 3300, loss = 0.552267
I1106 13:22:55.252568 26139 solver.cpp:244]     Train net output #0: accuracy = 0.824219
I1106 13:22:55.252578 26139 solver.cpp:244]     Train net output #1: loss = 0.552267 (* 1 = 0.552267 loss)
I1106 13:22:55.252584 26139 sgd_solver.cpp:106] Iteration 3300, lr = 0.01
I1106 13:32:56.547564 26139 solver.cpp:228] Iteration 3400, loss = 0.490833
I1106 13:32:56.547622 26139 solver.cpp:244]     Train net output #0: accuracy = 0.820312
I1106 13:32:56.547642 26139 solver.cpp:244]     Train net output #1: loss = 0.490833 (* 1 = 0.490833 loss)
I1106 13:32:56.547663 26139 sgd_solver.cpp:106] Iteration 3400, lr = 0.01
I1106 13:42:54.299140 26139 solver.cpp:228] Iteration 3500, loss = 0.511538
I1106 13:42:54.299171 26139 solver.cpp:244]     Train net output #0: accuracy = 0.847656
I1106 13:42:54.299180 26139 solver.cpp:244]     Train net output #1: loss = 0.511538 (* 1 = 0.511538 loss)
I1106 13:42:54.299186 26139 sgd_solver.cpp:106] Iteration 3500, lr = 0.01
I1106 13:52:44.215071 26139 solver.cpp:228] Iteration 3600, loss = 0.474808
I1106 13:52:44.215109 26139 solver.cpp:244]     Train net output #0: accuracy = 0.851562
I1106 13:52:44.215119 26139 solver.cpp:244]     Train net output #1: loss = 0.474808 (* 1 = 0.474808 loss)
I1106 13:52:44.215126 26139 sgd_solver.cpp:106] Iteration 3600, lr = 0.01
I1106 14:02:31.724594 26139 solver.cpp:228] Iteration 3700, loss = 0.537587
I1106 14:02:31.724634 26139 solver.cpp:244]     Train net output #0: accuracy = 0.835938
I1106 14:02:31.724663 26139 solver.cpp:244]     Train net output #1: loss = 0.537587 (* 1 = 0.537587 loss)
I1106 14:02:31.724694 26139 sgd_solver.cpp:106] Iteration 3700, lr = 0.01
I1106 14:15:20.485144 26139 solver.cpp:228] Iteration 3800, loss = 0.616508
I1106 14:15:20.485175 26139 solver.cpp:244]     Train net output #0: accuracy = 0.8125
I1106 14:15:20.485184 26139 solver.cpp:244]     Train net output #1: loss = 0.616508 (* 1 = 0.616508 loss)
I1106 14:15:20.485190 26139 sgd_solver.cpp:106] Iteration 3800, lr = 0.01
I1106 14:31:18.874143 26139 solver.cpp:228] Iteration 3900, loss = 0.587943
I1106 14:31:18.874191 26139 solver.cpp:244]     Train net output #0: accuracy = 0.832031
I1106 14:31:18.874222 26139 solver.cpp:244]     Train net output #1: loss = 0.587943 (* 1 = 0.587943 loss)
I1106 14:31:18.874246 26139 sgd_solver.cpp:106] Iteration 3900, lr = 0.01
I1106 14:41:40.309629 26139 solver.cpp:454] Snapshotting to binary proto file /home/kevin/snapshot/_iter_4000.caffemodel
I1106 14:41:53.518573 26139 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/kevin/snapshot/_iter_4000.solverstate
>>> 2016-11-06 14:41:53.759285 Begin model classification tests
>>> 2016-11-06 14:54:36.403397 Iteration 4000 mean classification accuracy (max)  0.851115618661
>>> 2016-11-06 14:54:36.403434 Iteration 4000 mean classification accuracy (ave) 0.8738336714
>>> 2016-11-06 14:54:36.403443 Iteration 4000 mean testing loss 0.658655438382
>>> 2016-11-06 14:54:36.403459 Iteration 4000 mean confusion matrix
[ 1.          0.92        0.97        0.4         0.92        0.94        0.85
  0.98        0.87        0.95        0.9         0.8         0.73255814
  0.8         0.68604651  0.6         0.9         0.96        0.8         0.75
  1.          0.96        0.95        0.3255814   1.          0.94
  0.96938776  0.55        0.95        0.85        0.98        0.95        0.75
  0.81        0.9         0.98        0.87        0.97        0.15        0.45      ]
I1106 14:54:48.272670 26139 solver.cpp:228] Iteration 4000, loss = 0.418197
I1106 14:54:48.272707 26139 solver.cpp:244]     Train net output #0: accuracy = 0.878906
I1106 14:54:48.272719 26139 solver.cpp:244]     Train net output #1: loss = 0.418197 (* 1 = 0.418197 loss)
I1106 14:54:48.272740 26139 sgd_solver.cpp:106] Iteration 4000, lr = 0.01
I1106 15:05:02.832242 26139 solver.cpp:228] Iteration 4100, loss = 0.503153
I1106 15:05:02.832274 26139 solver.cpp:244]     Train net output #0: accuracy = 0.847656
I1106 15:05:02.832284 26139 solver.cpp:244]     Train net output #1: loss = 0.503153 (* 1 = 0.503153 loss)
I1106 15:05:02.832293 26139 sgd_solver.cpp:106] Iteration 4100, lr = 0.01
I1106 15:15:13.171876 26139 solver.cpp:228] Iteration 4200, loss = 0.420756
I1106 15:15:13.171910 26139 solver.cpp:244]     Train net output #0: accuracy = 0.867188
I1106 15:15:13.171919 26139 solver.cpp:244]     Train net output #1: loss = 0.420756 (* 1 = 0.420756 loss)
I1106 15:15:13.171927 26139 sgd_solver.cpp:106] Iteration 4200, lr = 0.01
I1106 15:25:48.105885 26139 solver.cpp:228] Iteration 4300, loss = 0.412235
I1106 15:25:48.105918 26139 solver.cpp:244]     Train net output #0: accuracy = 0.875
I1106 15:25:48.105928 26139 solver.cpp:244]     Train net output #1: loss = 0.412235 (* 1 = 0.412235 loss)
I1106 15:25:48.105937 26139 sgd_solver.cpp:106] Iteration 4300, lr = 0.01
I1106 15:35:36.607892 26139 solver.cpp:228] Iteration 4400, loss = 0.316915
I1106 15:35:36.607928 26139 solver.cpp:244]     Train net output #0: accuracy = 0.90625
I1106 15:35:36.607939 26139 solver.cpp:244]     Train net output #1: loss = 0.316915 (* 1 = 0.316915 loss)
I1106 15:35:36.607946 26139 sgd_solver.cpp:106] Iteration 4400, lr = 0.01
I1106 15:45:40.616595 26139 solver.cpp:228] Iteration 4500, loss = 0.425683
I1106 15:45:40.616636 26139 solver.cpp:244]     Train net output #0: accuracy = 0.871094
I1106 15:45:40.616658 26139 solver.cpp:244]     Train net output #1: loss = 0.425683 (* 1 = 0.425683 loss)
I1106 15:45:40.616681 26139 sgd_solver.cpp:106] Iteration 4500, lr = 0.01
I1106 15:56:50.604710 26139 solver.cpp:228] Iteration 4600, loss = 0.529179
I1106 15:56:50.604750 26139 solver.cpp:244]     Train net output #0: accuracy = 0.855469
I1106 15:56:50.604761 26139 solver.cpp:244]     Train net output #1: loss = 0.529179 (* 1 = 0.529179 loss)
I1106 15:56:50.604769 26139 sgd_solver.cpp:106] Iteration 4600, lr = 0.01
I1106 16:09:37.932389 26139 solver.cpp:228] Iteration 4700, loss = 0.441478
I1106 16:09:37.932420 26139 solver.cpp:244]     Train net output #0: accuracy = 0.847656
I1106 16:09:37.932430 26139 solver.cpp:244]     Train net output #1: loss = 0.441478 (* 1 = 0.441478 loss)
I1106 16:09:37.932437 26139 sgd_solver.cpp:106] Iteration 4700, lr = 0.01
I1106 16:23:26.074110 26139 solver.cpp:228] Iteration 4800, loss = 0.310174
I1106 16:23:26.074141 26139 solver.cpp:244]     Train net output #0: accuracy = 0.910156
I1106 16:23:26.074149 26139 solver.cpp:244]     Train net output #1: loss = 0.310174 (* 1 = 0.310174 loss)
I1106 16:23:26.074156 26139 sgd_solver.cpp:106] Iteration 4800, lr = 0.01
I1106 16:35:57.943157 26139 solver.cpp:228] Iteration 4900, loss = 0.415451
I1106 16:35:57.943188 26139 solver.cpp:244]     Train net output #0: accuracy = 0.890625
I1106 16:35:57.943197 26139 solver.cpp:244]     Train net output #1: loss = 0.415451 (* 1 = 0.415451 loss)
I1106 16:35:57.943204 26139 sgd_solver.cpp:106] Iteration 4900, lr = 0.01
I1106 16:46:24.282934 26139 solver.cpp:454] Snapshotting to binary proto file /home/kevin/snapshot/_iter_5000.caffemodel
I1106 16:46:40.066745 26139 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/kevin/snapshot/_iter_5000.solverstate
>>> 2016-11-06 16:46:40.296588 Begin model classification tests
>>> 2016-11-06 16:59:22.616821 Iteration 5000 mean classification accuracy (max)  0.85476673428
>>> 2016-11-06 16:59:22.616891 Iteration 5000 mean classification accuracy (ave) 0.879513184584
>>> 2016-11-06 16:59:22.616911 Iteration 5000 mean testing loss 0.643455794566
>>> 2016-11-06 16:59:22.616935 Iteration 5000 mean confusion matrix
[ 1.          0.9         0.95        0.65        0.95        0.94        0.8
  1.          0.85        0.95        0.8         0.8         0.81395349
  0.8         0.60465116  0.65        0.88        0.96        0.85        0.75
  1.          0.96        0.96        0.44186047  0.94736842  0.9
  0.95918367  0.6         0.95        0.85        0.98        0.9         0.8
  0.81        0.85        1.          0.75        1.          0.6         0.7       ]
I1106 16:59:31.327862 26139 solver.cpp:228] Iteration 5000, loss = 0.436447
I1106 16:59:31.327891 26139 solver.cpp:244]     Train net output #0: accuracy = 0.855469
I1106 16:59:31.327900 26139 solver.cpp:244]     Train net output #1: loss = 0.436447 (* 1 = 0.436447 loss)
I1106 16:59:31.327908 26139 sgd_solver.cpp:106] Iteration 5000, lr = 0.001
I1106 17:09:46.570066 26139 solver.cpp:228] Iteration 5100, loss = 0.357683
I1106 17:09:46.570097 26139 solver.cpp:244]     Train net output #0: accuracy = 0.898438
I1106 17:09:46.570106 26139 solver.cpp:244]     Train net output #1: loss = 0.357683 (* 1 = 0.357683 loss)
I1106 17:09:46.570112 26139 sgd_solver.cpp:106] Iteration 5100, lr = 0.001
I1106 17:23:15.858815 26139 solver.cpp:228] Iteration 5200, loss = 0.366785
I1106 17:23:15.858849 26139 solver.cpp:244]     Train net output #0: accuracy = 0.878906
I1106 17:23:15.858858 26139 solver.cpp:244]     Train net output #1: loss = 0.366785 (* 1 = 0.366785 loss)
I1106 17:23:15.858865 26139 sgd_solver.cpp:106] Iteration 5200, lr = 0.001
I1106 17:34:09.009109 26139 solver.cpp:228] Iteration 5300, loss = 0.3438
I1106 17:34:09.009142 26139 solver.cpp:244]     Train net output #0: accuracy = 0.886719
I1106 17:34:09.009152 26139 solver.cpp:244]     Train net output #1: loss = 0.3438 (* 1 = 0.3438 loss)
I1106 17:34:09.009160 26139 sgd_solver.cpp:106] Iteration 5300, lr = 0.001
I1106 17:47:46.485461 26139 solver.cpp:228] Iteration 5400, loss = 0.302006
I1106 17:47:46.485494 26139 solver.cpp:244]     Train net output #0: accuracy = 0.914062
I1106 17:47:46.485514 26139 solver.cpp:244]     Train net output #1: loss = 0.302006 (* 1 = 0.302006 loss)
I1106 17:47:46.485527 26139 sgd_solver.cpp:106] Iteration 5400, lr = 0.001
I1106 18:03:36.548380 26139 solver.cpp:228] Iteration 5500, loss = 0.259855
I1106 18:03:36.548413 26139 solver.cpp:244]     Train net output #0: accuracy = 0.910156
I1106 18:03:36.548424 26139 solver.cpp:244]     Train net output #1: loss = 0.259855 (* 1 = 0.259855 loss)
I1106 18:03:36.548430 26139 sgd_solver.cpp:106] Iteration 5500, lr = 0.001
I1106 18:19:24.060380 26139 solver.cpp:228] Iteration 5600, loss = 0.291276
I1106 18:19:24.060420 26139 solver.cpp:244]     Train net output #0: accuracy = 0.878906
I1106 18:19:24.060446 26139 solver.cpp:244]     Train net output #1: loss = 0.291276 (* 1 = 0.291276 loss)
I1106 18:19:24.060472 26139 sgd_solver.cpp:106] Iteration 5600, lr = 0.001
I1106 18:34:21.848644 26139 solver.cpp:228] Iteration 5700, loss = 0.390718
I1106 18:34:21.848669 26139 solver.cpp:244]     Train net output #0: accuracy = 0.882812
I1106 18:34:21.848675 26139 solver.cpp:244]     Train net output #1: loss = 0.390718 (* 1 = 0.390718 loss)
I1106 18:34:21.848680 26139 sgd_solver.cpp:106] Iteration 5700, lr = 0.001
I1106 18:46:53.031651 26139 solver.cpp:228] Iteration 5800, loss = 0.358295
I1106 18:46:53.031682 26139 solver.cpp:244]     Train net output #0: accuracy = 0.859375
I1106 18:46:53.031690 26139 solver.cpp:244]     Train net output #1: loss = 0.358295 (* 1 = 0.358295 loss)
I1106 18:46:53.031708 26139 sgd_solver.cpp:106] Iteration 5800, lr = 0.001
I1106 19:01:12.696323 26139 solver.cpp:228] Iteration 5900, loss = 0.277888
I1106 19:01:12.696357 26139 solver.cpp:244]     Train net output #0: accuracy = 0.90625
I1106 19:01:12.696365 26139 solver.cpp:244]     Train net output #1: loss = 0.277888 (* 1 = 0.277888 loss)
I1106 19:01:12.696372 26139 sgd_solver.cpp:106] Iteration 5900, lr = 0.001
I1106 19:44:01.091377 26139 solver.cpp:454] Snapshotting to binary proto file /home/kevin/snapshot/_iter_6000.caffemodel
I1106 19:44:09.430691 26139 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/kevin/snapshot/_iter_6000.solverstate
>>> 2016-11-06 19:44:09.671007 Begin model classification tests
>>> 2016-11-06 20:46:52.857640 Iteration 6000 mean classification accuracy (max)  0.856389452333
>>> 2016-11-06 20:46:52.857682 Iteration 6000 mean classification accuracy (ave) 0.884787018256
>>> 2016-11-06 20:46:52.857692 Iteration 6000 mean testing loss 0.619669006989
>>> 2016-11-06 20:46:52.857714 Iteration 6000 mean confusion matrix
[ 1.          0.92        0.96        0.7         0.93        0.94        0.85
  0.99        0.81        0.95        0.9         0.8         0.84883721
  0.8         0.6744186   0.7         0.88        0.96        0.85        0.75
  1.          0.96        0.96        0.43023256  0.94736842  0.95
  0.97959184  0.65        0.95        0.85        0.98        0.95        0.8
  0.82        0.9         0.99        0.76        0.95        0.5         0.75      ]
I1106 20:47:20.700232 26139 solver.cpp:228] Iteration 6000, loss = 0.412623
I1106 20:47:20.700283 26139 solver.cpp:244]     Train net output #0: accuracy = 0.859375
I1106 20:47:20.700307 26139 solver.cpp:244]     Train net output #1: loss = 0.412623 (* 1 = 0.412623 loss)
I1106 20:47:20.700321 26139 sgd_solver.cpp:106] Iteration 6000, lr = 0.001
I1106 21:17:30.850284 26139 solver.cpp:228] Iteration 6100, loss = 0.399044
I1106 21:17:30.850317 26139 solver.cpp:244]     Train net output #0: accuracy = 0.84375
I1106 21:17:30.850328 26139 solver.cpp:244]     Train net output #1: loss = 0.399044 (* 1 = 0.399044 loss)
I1106 21:17:30.850350 26139 sgd_solver.cpp:106] Iteration 6100, lr = 0.001
I1106 21:51:14.578997 26139 solver.cpp:228] Iteration 6200, loss = 0.253666
I1106 21:51:14.579040 26139 solver.cpp:244]     Train net output #0: accuracy = 0.910156
I1106 21:51:14.579051 26139 solver.cpp:244]     Train net output #1: loss = 0.253666 (* 1 = 0.253666 loss)
I1106 21:51:14.579059 26139 sgd_solver.cpp:106] Iteration 6200, lr = 0.001
I1106 22:15:28.198222 26139 solver.cpp:228] Iteration 6300, loss = 0.259068
I1106 22:15:28.198264 26139 solver.cpp:244]     Train net output #0: accuracy = 0.90625
I1106 22:15:28.198281 26139 solver.cpp:244]     Train net output #1: loss = 0.259068 (* 1 = 0.259068 loss)
I1106 22:15:28.198304 26139 sgd_solver.cpp:106] Iteration 6300, lr = 0.001
I1106 22:52:00.973476 26139 solver.cpp:228] Iteration 6400, loss = 0.345243
I1106 22:52:00.973527 26139 solver.cpp:244]     Train net output #0: accuracy = 0.890625
I1106 22:52:00.973546 26139 solver.cpp:244]     Train net output #1: loss = 0.345243 (* 1 = 0.345243 loss)
I1106 22:52:00.973558 26139 sgd_solver.cpp:106] Iteration 6400, lr = 0.001
I1106 23:22:59.354315 26139 solver.cpp:228] Iteration 6500, loss = 0.371834
I1106 23:22:59.354380 26139 solver.cpp:244]     Train net output #0: accuracy = 0.871094
I1106 23:22:59.354393 26139 solver.cpp:244]     Train net output #1: loss = 0.371834 (* 1 = 0.371834 loss)
I1106 23:22:59.354414 26139 sgd_solver.cpp:106] Iteration 6500, lr = 0.001
I1106 23:45:02.313146 26139 solver.cpp:228] Iteration 6600, loss = 0.282771
I1106 23:45:02.313194 26139 solver.cpp:244]     Train net output #0: accuracy = 0.882812
I1106 23:45:02.313211 26139 solver.cpp:244]     Train net output #1: loss = 0.282771 (* 1 = 0.282771 loss)
I1106 23:45:02.313222 26139 sgd_solver.cpp:106] Iteration 6600, lr = 0.001
I1107 00:00:33.325732 26139 solver.cpp:228] Iteration 6700, loss = 0.396069
I1107 00:00:33.325803 26139 solver.cpp:244]     Train net output #0: accuracy = 0.863281
I1107 00:00:33.325855 26139 solver.cpp:244]     Train net output #1: loss = 0.396069 (* 1 = 0.396069 loss)
I1107 00:00:33.325912 26139 sgd_solver.cpp:106] Iteration 6700, lr = 0.001
I1107 00:19:45.496932 26139 solver.cpp:228] Iteration 6800, loss = 0.300742
I1107 00:19:45.496992 26139 solver.cpp:244]     Train net output #0: accuracy = 0.894531
I1107 00:19:45.497017 26139 solver.cpp:244]     Train net output #1: loss = 0.300742 (* 1 = 0.300742 loss)
I1107 00:19:45.497028 26139 sgd_solver.cpp:106] Iteration 6800, lr = 0.001
I1107 00:45:22.961279 26139 solver.cpp:228] Iteration 6900, loss = 0.361948
I1107 00:45:22.961339 26139 solver.cpp:244]     Train net output #0: accuracy = 0.875
I1107 00:45:22.961362 26139 solver.cpp:244]     Train net output #1: loss = 0.361948 (* 1 = 0.361948 loss)
I1107 00:45:22.961396 26139 sgd_solver.cpp:106] Iteration 6900, lr = 0.001
I1107 01:12:30.533190 26139 solver.cpp:454] Snapshotting to binary proto file /home/kevin/snapshot/_iter_7000.caffemodel
I1107 01:12:35.591378 26139 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/kevin/snapshot/_iter_7000.solverstate
>>> 2016-11-07 01:12:35.856740 Begin model classification tests
>>> 2016-11-07 01:41:09.100779 Iteration 7000 mean classification accuracy (max)  0.868154158215
>>> 2016-11-07 01:41:09.100821 Iteration 7000 mean classification accuracy (ave) 0.890872210953
>>> 2016-11-07 01:41:09.100835 Iteration 7000 mean testing loss 0.618454911305
>>> 2016-11-07 01:41:09.100855 Iteration 7000 mean confusion matrix
[ 1.          0.92        0.97        0.7         0.93        0.95        0.85
  1.          0.79        0.95        0.9         0.8         0.84883721
  0.85        0.68604651  0.7         0.88        0.96        0.85        0.75
  1.          0.96        0.98        0.40697674  0.94736842  0.95
  0.97959184  0.75        0.95        0.85        0.98        0.95        0.8
  0.82        0.9         0.99        0.85        0.98        0.35        0.8       ]
I1107 01:41:24.169615 26139 solver.cpp:228] Iteration 7000, loss = 0.310321
I1107 01:41:24.169653 26139 solver.cpp:244]     Train net output #0: accuracy = 0.902344
I1107 01:41:24.169669 26139 solver.cpp:244]     Train net output #1: loss = 0.310321 (* 1 = 0.310321 loss)
I1107 01:41:24.169697 26139 sgd_solver.cpp:106] Iteration 7000, lr = 0.001
I1107 02:10:07.571035 26139 solver.cpp:228] Iteration 7100, loss = 0.28069
I1107 02:10:07.571085 26139 solver.cpp:244]     Train net output #0: accuracy = 0.902344
I1107 02:10:07.571096 26139 solver.cpp:244]     Train net output #1: loss = 0.28069 (* 1 = 0.28069 loss)
I1107 02:10:07.571106 26139 sgd_solver.cpp:106] Iteration 7100, lr = 0.001
I1107 02:49:06.652770 26139 solver.cpp:228] Iteration 7200, loss = 0.332286
I1107 02:49:06.652799 26139 solver.cpp:244]     Train net output #0: accuracy = 0.898438
I1107 02:49:06.652808 26139 solver.cpp:244]     Train net output #1: loss = 0.332286 (* 1 = 0.332286 loss)
I1107 02:49:06.652815 26139 sgd_solver.cpp:106] Iteration 7200, lr = 0.001
I1107 03:27:37.707876 26139 solver.cpp:228] Iteration 7300, loss = 0.267892
I1107 03:27:37.707926 26139 solver.cpp:244]     Train net output #0: accuracy = 0.925781
I1107 03:27:37.707942 26139 solver.cpp:244]     Train net output #1: loss = 0.267892 (* 1 = 0.267892 loss)
I1107 03:27:37.707967 26139 sgd_solver.cpp:106] Iteration 7300, lr = 0.001
I1107 04:05:39.613850 26139 solver.cpp:228] Iteration 7400, loss = 0.333277
I1107 04:05:39.613898 26139 solver.cpp:244]     Train net output #0: accuracy = 0.910156
I1107 04:05:39.613914 26139 solver.cpp:244]     Train net output #1: loss = 0.333277 (* 1 = 0.333277 loss)
I1107 04:05:39.613950 26139 sgd_solver.cpp:106] Iteration 7400, lr = 0.001
I1107 04:44:14.605432 26139 solver.cpp:228] Iteration 7500, loss = 0.416316
I1107 04:44:14.605468 26139 solver.cpp:244]     Train net output #0: accuracy = 0.859375
I1107 04:44:14.605489 26139 solver.cpp:244]     Train net output #1: loss = 0.416316 (* 1 = 0.416316 loss)
I1107 04:44:14.605523 26139 sgd_solver.cpp:106] Iteration 7500, lr = 0.001
I1107 05:22:40.316571 26139 solver.cpp:228] Iteration 7600, loss = 0.308264
I1107 05:22:40.316617 26139 solver.cpp:244]     Train net output #0: accuracy = 0.894531
I1107 05:22:40.316638 26139 solver.cpp:244]     Train net output #1: loss = 0.308264 (* 1 = 0.308264 loss)
I1107 05:22:40.316658 26139 sgd_solver.cpp:106] Iteration 7600, lr = 0.001
I1107 06:01:09.783617 26139 solver.cpp:228] Iteration 7700, loss = 0.268758
I1107 06:01:09.783648 26139 solver.cpp:244]     Train net output #0: accuracy = 0.925781
I1107 06:01:09.783659 26139 solver.cpp:244]     Train net output #1: loss = 0.268758 (* 1 = 0.268758 loss)
I1107 06:01:09.783670 26139 sgd_solver.cpp:106] Iteration 7700, lr = 0.001
I1107 06:39:04.943174 26139 solver.cpp:228] Iteration 7800, loss = 0.246396
I1107 06:39:04.943236 26139 solver.cpp:244]     Train net output #0: accuracy = 0.929688
I1107 06:39:04.943271 26139 solver.cpp:244]     Train net output #1: loss = 0.246396 (* 1 = 0.246396 loss)
I1107 06:39:04.943287 26139 sgd_solver.cpp:106] Iteration 7800, lr = 0.001
I1107 07:17:41.796929 26139 solver.cpp:228] Iteration 7900, loss = 0.311692
I1107 07:17:41.796975 26139 solver.cpp:244]     Train net output #0: accuracy = 0.894531
I1107 07:17:41.796986 26139 solver.cpp:244]     Train net output #1: loss = 0.311692 (* 1 = 0.311692 loss)
I1107 07:17:41.797008 26139 sgd_solver.cpp:106] Iteration 7900, lr = 0.001
I1107 07:55:18.841236 26139 solver.cpp:454] Snapshotting to binary proto file /home/kevin/snapshot/_iter_8000.caffemodel
I1107 07:55:52.408953 26139 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/kevin/snapshot/_iter_8000.solverstate
>>> 2016-11-07 07:55:52.706878 Begin model classification tests
>>> 2016-11-07 08:24:42.447473 Iteration 8000 mean classification accuracy (max)  0.868559837728
>>> 2016-11-07 08:24:42.447598 Iteration 8000 mean classification accuracy (ave) 0.888438133874
>>> 2016-11-07 08:24:42.447657 Iteration 8000 mean testing loss 0.619918205457
>>> 2016-11-07 08:24:42.447708 Iteration 8000 mean confusion matrix
[ 1.          0.92        0.97        0.7         0.92        0.96        0.85
  1.          0.82        0.95        0.8         0.8         0.8372093
  0.8         0.69767442  0.7         0.88        0.97        0.9         0.75
  1.          0.95        0.97        0.45348837  0.94736842  0.95
  0.95918367  0.7         0.95        0.85        0.99        0.95        0.8
  0.81        0.9         0.99        0.81        0.95        0.35        0.8       ]
I1107 08:25:01.983777 26139 solver.cpp:228] Iteration 8000, loss = 0.324225
I1107 08:25:01.983834 26139 solver.cpp:244]     Train net output #0: accuracy = 0.882812
I1107 08:25:01.983850 26139 solver.cpp:244]     Train net output #1: loss = 0.324225 (* 1 = 0.324225 loss)
I1107 08:25:01.983861 26139 sgd_solver.cpp:106] Iteration 8000, lr = 0.001
I1107 08:54:21.538290 26139 solver.cpp:228] Iteration 8100, loss = 0.339846
I1107 08:54:21.538359 26139 solver.cpp:244]     Train net output #0: accuracy = 0.90625
I1107 08:54:21.538411 26139 solver.cpp:244]     Train net output #1: loss = 0.339846 (* 1 = 0.339846 loss)
I1107 08:54:21.538455 26139 sgd_solver.cpp:106] Iteration 8100, lr = 0.001
I1107 09:22:56.112402 26139 solver.cpp:228] Iteration 8200, loss = 0.282546
I1107 09:22:56.112488 26139 solver.cpp:244]     Train net output #0: accuracy = 0.902344
I1107 09:22:56.112536 26139 solver.cpp:244]     Train net output #1: loss = 0.282546 (* 1 = 0.282546 loss)
I1107 09:22:56.112581 26139 sgd_solver.cpp:106] Iteration 8200, lr = 0.001
I1107 09:51:58.701781 26139 solver.cpp:228] Iteration 8300, loss = 0.306814
I1107 09:51:58.701845 26139 solver.cpp:244]     Train net output #0: accuracy = 0.898438
I1107 09:51:58.701897 26139 solver.cpp:244]     Train net output #1: loss = 0.306814 (* 1 = 0.306814 loss)
I1107 09:51:58.701941 26139 sgd_solver.cpp:106] Iteration 8300, lr = 0.001
I1107 10:22:34.494051 26139 solver.cpp:228] Iteration 8400, loss = 0.291279
I1107 10:22:34.494084 26139 solver.cpp:244]     Train net output #0: accuracy = 0.898438
I1107 10:22:34.494094 26139 solver.cpp:244]     Train net output #1: loss = 0.291279 (* 1 = 0.291279 loss)
I1107 10:22:34.494102 26139 sgd_solver.cpp:106] Iteration 8400, lr = 0.001
I1107 11:00:17.932119 26139 solver.cpp:228] Iteration 8500, loss = 0.384783
I1107 11:00:17.932183 26139 solver.cpp:244]     Train net output #0: accuracy = 0.867188
I1107 11:00:17.932200 26139 solver.cpp:244]     Train net output #1: loss = 0.384783 (* 1 = 0.384783 loss)
I1107 11:00:17.932225 26139 sgd_solver.cpp:106] Iteration 8500, lr = 0.001
I1107 11:39:40.040982 26139 solver.cpp:228] Iteration 8600, loss = 0.309707
I1107 11:39:40.041040 26139 solver.cpp:244]     Train net output #0: accuracy = 0.886719
I1107 11:39:40.041062 26139 solver.cpp:244]     Train net output #1: loss = 0.309707 (* 1 = 0.309707 loss)
I1107 11:39:40.041090 26139 sgd_solver.cpp:106] Iteration 8600, lr = 0.001
I1107 12:33:53.709115 26139 solver.cpp:228] Iteration 8700, loss = 0.287853
I1107 12:33:53.709172 26139 solver.cpp:244]     Train net output #0: accuracy = 0.90625
I1107 12:33:53.709218 26139 solver.cpp:244]     Train net output #1: loss = 0.287853 (* 1 = 0.287853 loss)
I1107 12:33:53.709234 26139 sgd_solver.cpp:106] Iteration 8700, lr = 0.001
I1107 12:51:14.834517 26139 solver.cpp:228] Iteration 8800, loss = 0.358085
I1107 12:51:14.834547 26139 solver.cpp:244]     Train net output #0: accuracy = 0.882812
I1107 12:51:14.834563 26139 solver.cpp:244]     Train net output #1: loss = 0.358085 (* 1 = 0.358085 loss)
I1107 12:51:14.834570 26139 sgd_solver.cpp:106] Iteration 8800, lr = 0.001
I1107 13:01:08.431493 26139 solver.cpp:228] Iteration 8900, loss = 0.319544
I1107 13:01:08.431522 26139 solver.cpp:244]     Train net output #0: accuracy = 0.882812
I1107 13:01:08.431532 26139 solver.cpp:244]     Train net output #1: loss = 0.319544 (* 1 = 0.319544 loss)
I1107 13:01:08.431540 26139 sgd_solver.cpp:106] Iteration 8900, lr = 0.001
I1107 13:10:32.745018 26139 solver.cpp:454] Snapshotting to binary proto file /home/kevin/snapshot/_iter_9000.caffemodel
I1107 13:10:40.964706 26139 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/kevin/snapshot/_iter_9000.solverstate
>>> 2016-11-07 13:10:41.194562 Begin model classification tests
>>> 2016-11-07 13:23:12.571438 Iteration 9000 mean classification accuracy (max)  0.87139959432
>>> 2016-11-07 13:23:12.571475 Iteration 9000 mean classification accuracy (ave) 0.886409736308
>>> 2016-11-07 13:23:12.571491 Iteration 9000 mean testing loss 0.628839597226
>>> 2016-11-07 13:23:12.571521 Iteration 9000 mean confusion matrix
[ 1.          0.92        0.97        0.7         0.93        0.95        0.85
  0.99        0.79        0.95        0.9         0.8         0.86046512
  0.9         0.6744186   0.7         0.88        0.97        0.9         0.75
  1.          0.96        0.97        0.46511628  0.94736842  0.95
  0.97959184  0.7         0.94        0.85        0.98        0.95        0.8
  0.79        0.9         0.99        0.72        0.95        0.55        0.8       ]
I1107 13:23:19.542387 26139 solver.cpp:228] Iteration 9000, loss = 0.25593
I1107 13:23:19.542418 26139 solver.cpp:244]     Train net output #0: accuracy = 0.898438
I1107 13:23:19.542426 26139 solver.cpp:244]     Train net output #1: loss = 0.25593 (* 1 = 0.25593 loss)
I1107 13:23:19.542433 26139 sgd_solver.cpp:106] Iteration 9000, lr = 0.001
I1107 13:33:40.732964 26139 solver.cpp:228] Iteration 9100, loss = 0.349494
I1107 13:33:40.732995 26139 solver.cpp:244]     Train net output #0: accuracy = 0.878906
I1107 13:33:40.733003 26139 solver.cpp:244]     Train net output #1: loss = 0.349494 (* 1 = 0.349494 loss)
I1107 13:33:40.733009 26139 sgd_solver.cpp:106] Iteration 9100, lr = 0.001
I1107 13:43:30.772125 26139 solver.cpp:228] Iteration 9200, loss = 0.313728
I1107 13:43:30.772153 26139 solver.cpp:244]     Train net output #0: accuracy = 0.894531
I1107 13:43:30.772162 26139 solver.cpp:244]     Train net output #1: loss = 0.313728 (* 1 = 0.313728 loss)
I1107 13:43:30.772168 26139 sgd_solver.cpp:106] Iteration 9200, lr = 0.001
I1107 13:52:57.456599 26139 solver.cpp:228] Iteration 9300, loss = 0.307535
I1107 13:52:57.456630 26139 solver.cpp:244]     Train net output #0: accuracy = 0.898438
I1107 13:52:57.456640 26139 solver.cpp:244]     Train net output #1: loss = 0.307535 (* 1 = 0.307535 loss)
I1107 13:52:57.456647 26139 sgd_solver.cpp:106] Iteration 9300, lr = 0.001
I1107 14:04:53.964859 26139 solver.cpp:228] Iteration 9400, loss = 0.314623
I1107 14:04:53.964905 26139 solver.cpp:244]     Train net output #0: accuracy = 0.902344
I1107 14:04:53.964915 26139 solver.cpp:244]     Train net output #1: loss = 0.314623 (* 1 = 0.314623 loss)
I1107 14:04:53.964931 26139 sgd_solver.cpp:106] Iteration 9400, lr = 0.001
I1107 14:23:27.659876 26139 solver.cpp:228] Iteration 9500, loss = 0.320816
I1107 14:23:27.659906 26139 solver.cpp:244]     Train net output #0: accuracy = 0.894531
I1107 14:23:27.659916 26139 solver.cpp:244]     Train net output #1: loss = 0.320816 (* 1 = 0.320816 loss)
I1107 14:23:27.659922 26139 sgd_solver.cpp:106] Iteration 9500, lr = 0.001
I1107 14:43:21.146554 26139 solver.cpp:228] Iteration 9600, loss = 0.360549
I1107 14:43:21.146606 26139 solver.cpp:244]     Train net output #0: accuracy = 0.878906
I1107 14:43:21.146616 26139 solver.cpp:244]     Train net output #1: loss = 0.360549 (* 1 = 0.360549 loss)
I1107 14:43:21.146633 26139 sgd_solver.cpp:106] Iteration 9600, lr = 0.001
I1107 15:01:44.935489 26139 solver.cpp:228] Iteration 9700, loss = 0.267947
I1107 15:01:44.935521 26139 solver.cpp:244]     Train net output #0: accuracy = 0.894531
I1107 15:01:44.935537 26139 solver.cpp:244]     Train net output #1: loss = 0.267947 (* 1 = 0.267947 loss)
I1107 15:01:44.935562 26139 sgd_solver.cpp:106] Iteration 9700, lr = 0.001
I1107 15:13:07.340376 26139 solver.cpp:228] Iteration 9800, loss = 0.251769
I1107 15:13:07.340405 26139 solver.cpp:244]     Train net output #0: accuracy = 0.933594
I1107 15:13:07.340414 26139 solver.cpp:244]     Train net output #1: loss = 0.251769 (* 1 = 0.251769 loss)
I1107 15:13:07.340420 26139 sgd_solver.cpp:106] Iteration 9800, lr = 0.001
I1107 15:22:28.574342 26139 solver.cpp:228] Iteration 9900, loss = 0.362593
I1107 15:22:28.574371 26139 solver.cpp:244]     Train net output #0: accuracy = 0.894531
I1107 15:22:28.574381 26139 solver.cpp:244]     Train net output #1: loss = 0.362593 (* 1 = 0.362593 loss)
I1107 15:22:28.574388 26139 sgd_solver.cpp:106] Iteration 9900, lr = 0.001
I1107 15:31:14.076258 26139 solver.cpp:454] Snapshotting to binary proto file /home/kevin/snapshot/_iter_10000.caffemodel
I1107 15:31:20.563354 26139 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/kevin/snapshot/_iter_10000.solverstate
>>> 2016-11-07 15:31:20.800767 Begin model classification tests
>>> 2016-11-07 15:44:17.566499 Iteration 10000 mean classification accuracy (max)  0.871805273834
>>> 2016-11-07 15:44:17.566536 Iteration 10000 mean classification accuracy (ave) 0.892089249493
>>> 2016-11-07 15:44:17.566545 Iteration 10000 mean testing loss 0.617715795657
>>> 2016-11-07 15:44:17.566557 Iteration 10000 mean confusion matrix
[ 1.          0.92        0.97        0.7         0.93        0.96        0.85
  1.          0.81        0.95        0.85        0.8         0.84883721
  0.85        0.69767442  0.7         0.88        0.97        0.9         0.75
  1.          0.96        0.97        0.34883721  0.94736842  0.95
  0.97959184  0.7         0.95        0.85        0.98        0.95        0.8
  0.85        0.9         1.          0.86        0.98        0.35        0.8       ]
I1107 15:44:25.564069 26139 solver.cpp:228] Iteration 10000, loss = 0.291767
I1107 15:44:25.564100 26139 solver.cpp:244]     Train net output #0: accuracy = 0.894531
I1107 15:44:25.564108 26139 solver.cpp:244]     Train net output #1: loss = 0.291767 (* 1 = 0.291767 loss)
I1107 15:44:25.564116 26139 sgd_solver.cpp:106] Iteration 10000, lr = 0.0001
I1107 15:54:40.393085 26139 solver.cpp:228] Iteration 10100, loss = 0.30229
I1107 15:54:40.393115 26139 solver.cpp:244]     Train net output #0: accuracy = 0.894531
I1107 15:54:40.393124 26139 solver.cpp:244]     Train net output #1: loss = 0.30229 (* 1 = 0.30229 loss)
I1107 15:54:40.393131 26139 sgd_solver.cpp:106] Iteration 10100, lr = 0.0001
I1107 16:04:07.406209 26139 solver.cpp:228] Iteration 10200, loss = 0.343645
I1107 16:04:07.406239 26139 solver.cpp:244]     Train net output #0: accuracy = 0.886719
I1107 16:04:07.406247 26139 solver.cpp:244]     Train net output #1: loss = 0.343645 (* 1 = 0.343645 loss)
I1107 16:04:07.406255 26139 sgd_solver.cpp:106] Iteration 10200, lr = 0.0001
I1107 16:14:10.088554 26139 solver.cpp:228] Iteration 10300, loss = 0.196193
I1107 16:14:10.088608 26139 solver.cpp:244]     Train net output #0: accuracy = 0.921875
I1107 16:14:10.088624 26139 solver.cpp:244]     Train net output #1: loss = 0.196193 (* 1 = 0.196193 loss)
I1107 16:14:10.088646 26139 sgd_solver.cpp:106] Iteration 10300, lr = 0.0001
I1107 16:23:41.009322 26139 solver.cpp:228] Iteration 10400, loss = 0.261092
I1107 16:23:41.009353 26139 solver.cpp:244]     Train net output #0: accuracy = 0.921875
I1107 16:23:41.009361 26139 solver.cpp:244]     Train net output #1: loss = 0.261092 (* 1 = 0.261092 loss)
I1107 16:23:41.009367 26139 sgd_solver.cpp:106] Iteration 10400, lr = 0.0001
I1107 16:35:48.442116 26139 solver.cpp:228] Iteration 10500, loss = 0.275496
I1107 16:35:48.442147 26139 solver.cpp:244]     Train net output #0: accuracy = 0.917969
I1107 16:35:48.442157 26139 solver.cpp:244]     Train net output #1: loss = 0.275496 (* 1 = 0.275496 loss)
I1107 16:35:48.442163 26139 sgd_solver.cpp:106] Iteration 10500, lr = 0.0001
I1107 16:47:49.739269 26139 solver.cpp:228] Iteration 10600, loss = 0.266337
I1107 16:47:49.739305 26139 solver.cpp:244]     Train net output #0: accuracy = 0.925781
I1107 16:47:49.739322 26139 solver.cpp:244]     Train net output #1: loss = 0.266337 (* 1 = 0.266337 loss)
I1107 16:47:49.739349 26139 sgd_solver.cpp:106] Iteration 10600, lr = 0.0001
I1107 17:07:43.953311 26139 solver.cpp:228] Iteration 10700, loss = 0.291816
I1107 17:07:43.953342 26139 solver.cpp:244]     Train net output #0: accuracy = 0.878906
I1107 17:07:43.953351 26139 solver.cpp:244]     Train net output #1: loss = 0.291816 (* 1 = 0.291816 loss)
I1107 17:07:43.953358 26139 sgd_solver.cpp:106] Iteration 10700, lr = 0.0001
I1107 17:23:49.084934 26139 solver.cpp:228] Iteration 10800, loss = 0.315664
I1107 17:23:49.084974 26139 solver.cpp:244]     Train net output #0: accuracy = 0.882812
I1107 17:23:49.084983 26139 solver.cpp:244]     Train net output #1: loss = 0.315664 (* 1 = 0.315664 loss)
I1107 17:23:49.084993 26139 sgd_solver.cpp:106] Iteration 10800, lr = 0.0001
I1107 17:41:01.909600 26139 solver.cpp:228] Iteration 10900, loss = 0.295417
I1107 17:41:01.909647 26139 solver.cpp:244]     Train net output #0: accuracy = 0.894531
I1107 17:41:01.909657 26139 solver.cpp:244]     Train net output #1: loss = 0.295417 (* 1 = 0.295417 loss)
I1107 17:41:01.909675 26139 sgd_solver.cpp:106] Iteration 10900, lr = 0.0001
I1107 17:56:38.255235 26139 solver.cpp:454] Snapshotting to binary proto file /home/kevin/snapshot/_iter_11000.caffemodel
I1107 17:56:52.089517 26139 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/kevin/snapshot/_iter_11000.solverstate
>>> 2016-11-07 17:56:52.323070 Begin model classification tests
>>> 2016-11-07 18:09:30.002144 Iteration 11000 mean classification accuracy (max)  0.871805273834
>>> 2016-11-07 18:09:30.002180 Iteration 11000 mean classification accuracy (ave) 0.892900608519
>>> 2016-11-07 18:09:30.002189 Iteration 11000 mean testing loss 0.620554969117
>>> 2016-11-07 18:09:30.002204 Iteration 11000 mean confusion matrix
[ 1.          0.92        0.97        0.7         0.93        0.96        0.85
  1.          0.81        0.95        0.9         0.8         0.86046512
  0.85        0.70930233  0.7         0.88        0.97        0.9         0.75
  1.          0.96        0.97        0.43023256  0.94736842  0.95
  0.97959184  0.7         0.95        0.85        0.98        0.95        0.8
  0.84        0.9         1.          0.79        0.96        0.45        0.8       ]
I1107 18:09:40.392905 26139 solver.cpp:228] Iteration 11000, loss = 0.266998
I1107 18:09:40.392935 26139 solver.cpp:244]     Train net output #0: accuracy = 0.914062
I1107 18:09:40.392945 26139 solver.cpp:244]     Train net output #1: loss = 0.266998 (* 1 = 0.266998 loss)
I1107 18:09:40.392951 26139 sgd_solver.cpp:106] Iteration 11000, lr = 0.0001
I1107 18:20:19.913935 26139 solver.cpp:228] Iteration 11100, loss = 0.283625
I1107 18:20:19.913964 26139 solver.cpp:244]     Train net output #0: accuracy = 0.921875
I1107 18:20:19.913974 26139 solver.cpp:244]     Train net output #1: loss = 0.283625 (* 1 = 0.283625 loss)
I1107 18:20:19.913980 26139 sgd_solver.cpp:106] Iteration 11100, lr = 0.0001
I1107 18:30:03.981784 26139 solver.cpp:228] Iteration 11200, loss = 0.26086
I1107 18:30:03.981815 26139 solver.cpp:244]     Train net output #0: accuracy = 0.90625
I1107 18:30:03.981824 26139 solver.cpp:244]     Train net output #1: loss = 0.26086 (* 1 = 0.26086 loss)
I1107 18:30:03.981832 26139 sgd_solver.cpp:106] Iteration 11200, lr = 0.0001
I1107 18:39:14.617709 26139 solver.cpp:228] Iteration 11300, loss = 0.270245
I1107 18:39:14.617763 26139 solver.cpp:244]     Train net output #0: accuracy = 0.914062
I1107 18:39:14.617774 26139 solver.cpp:244]     Train net output #1: loss = 0.270245 (* 1 = 0.270245 loss)
I1107 18:39:14.617792 26139 sgd_solver.cpp:106] Iteration 11300, lr = 0.0001
I1107 18:51:35.489809 26139 solver.cpp:228] Iteration 11400, loss = 0.316799
I1107 18:51:35.489840 26139 solver.cpp:244]     Train net output #0: accuracy = 0.898438
I1107 18:51:35.489848 26139 solver.cpp:244]     Train net output #1: loss = 0.316799 (* 1 = 0.316799 loss)
I1107 18:51:35.489855 26139 sgd_solver.cpp:106] Iteration 11400, lr = 0.0001
I1107 19:09:59.253758 26139 solver.cpp:228] Iteration 11500, loss = 0.24136
I1107 19:09:59.253790 26139 solver.cpp:244]     Train net output #0: accuracy = 0.933594
I1107 19:09:59.253800 26139 solver.cpp:244]     Train net output #1: loss = 0.24136 (* 1 = 0.24136 loss)
I1107 19:09:59.253808 26139 sgd_solver.cpp:106] Iteration 11500, lr = 0.0001
I1107 19:29:24.099285 26139 solver.cpp:228] Iteration 11600, loss = 0.253352
I1107 19:29:24.099315 26139 solver.cpp:244]     Train net output #0: accuracy = 0.914062
I1107 19:29:24.099324 26139 solver.cpp:244]     Train net output #1: loss = 0.253352 (* 1 = 0.253352 loss)
I1107 19:29:24.099330 26139 sgd_solver.cpp:106] Iteration 11600, lr = 0.0001
I1107 19:45:29.894779 26139 solver.cpp:228] Iteration 11700, loss = 0.274092
I1107 19:45:29.894820 26139 solver.cpp:244]     Train net output #0: accuracy = 0.917969
I1107 19:45:29.894830 26139 solver.cpp:244]     Train net output #1: loss = 0.274092 (* 1 = 0.274092 loss)
I1107 19:45:29.894839 26139 sgd_solver.cpp:106] Iteration 11700, lr = 0.0001
I1107 19:57:58.541668 26139 solver.cpp:228] Iteration 11800, loss = 0.240678
I1107 19:57:58.541703 26139 solver.cpp:244]     Train net output #0: accuracy = 0.902344
I1107 19:57:58.541712 26139 solver.cpp:244]     Train net output #1: loss = 0.240678 (* 1 = 0.240678 loss)
I1107 19:57:58.541719 26139 sgd_solver.cpp:106] Iteration 11800, lr = 0.0001
I1107 20:08:42.054006 26139 solver.cpp:228] Iteration 11900, loss = 0.410892
I1107 20:08:42.054039 26139 solver.cpp:244]     Train net output #0: accuracy = 0.855469
I1107 20:08:42.054047 26139 solver.cpp:244]     Train net output #1: loss = 0.410892 (* 1 = 0.410892 loss)
I1107 20:08:42.054054 26139 sgd_solver.cpp:106] Iteration 11900, lr = 0.0001
I1107 20:18:01.732466 26139 solver.cpp:454] Snapshotting to binary proto file /home/kevin/snapshot/_iter_12000.caffemodel
I1107 20:18:09.155642 26139 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/kevin/snapshot/_iter_12000.solverstate
>>> 2016-11-07 20:18:09.414089 Begin model classification tests
>>> 2016-11-07 20:30:43.076811 Iteration 12000 mean classification accuracy (max)  0.874239350913
>>> 2016-11-07 20:30:43.082047 Iteration 12000 mean classification accuracy (ave) 0.89168356998
>>> 2016-11-07 20:30:43.082063 Iteration 12000 mean testing loss 0.621220896173
>>> 2016-11-07 20:30:43.090539 Iteration 12000 mean confusion matrix
[ 1.          0.92        0.97        0.7         0.93        0.95        0.85
  1.          0.81        0.95        0.9         0.8         0.86046512
  0.9         0.70930233  0.7         0.88        0.97        0.9         0.75
  1.          0.96        0.97        0.45348837  0.94736842  0.95
  0.97959184  0.7         0.95        0.85        0.98        0.95        0.8
  0.8         0.9         1.          0.79        0.95        0.45        0.8       ]
I1107 20:30:50.349746 26139 solver.cpp:228] Iteration 12000, loss = 0.293018
I1107 20:30:50.349786 26139 solver.cpp:244]     Train net output #0: accuracy = 0.898438
I1107 20:30:50.349794 26139 solver.cpp:244]     Train net output #1: loss = 0.293018 (* 1 = 0.293018 loss)
I1107 20:30:50.349812 26139 sgd_solver.cpp:106] Iteration 12000, lr = 0.0001
I1107 20:41:34.173959 26139 solver.cpp:228] Iteration 12100, loss = 0.277861
I1107 20:41:34.173990 26139 solver.cpp:244]     Train net output #0: accuracy = 0.910156
I1107 20:41:34.173998 26139 solver.cpp:244]     Train net output #1: loss = 0.277861 (* 1 = 0.277861 loss)
I1107 20:41:34.174005 26139 sgd_solver.cpp:106] Iteration 12100, lr = 0.0001
I1107 20:50:58.087561 26139 solver.cpp:228] Iteration 12200, loss = 0.245573
I1107 20:50:58.087591 26139 solver.cpp:244]     Train net output #0: accuracy = 0.917969
I1107 20:50:58.087599 26139 solver.cpp:244]     Train net output #1: loss = 0.245573 (* 1 = 0.245573 loss)
I1107 20:50:58.087606 26139 sgd_solver.cpp:106] Iteration 12200, lr = 0.0001
I1107 20:59:53.634337 26139 solver.cpp:228] Iteration 12300, loss = 0.226867
I1107 20:59:53.634368 26139 solver.cpp:244]     Train net output #0: accuracy = 0.929688
I1107 20:59:53.634377 26139 solver.cpp:244]     Train net output #1: loss = 0.226867 (* 1 = 0.226867 loss)
I1107 20:59:53.634385 26139 sgd_solver.cpp:106] Iteration 12300, lr = 0.0001
I1107 21:08:58.980299 26139 solver.cpp:228] Iteration 12400, loss = 0.189262
I1107 21:08:58.980330 26139 solver.cpp:244]     Train net output #0: accuracy = 0.945312
I1107 21:08:58.980339 26139 solver.cpp:244]     Train net output #1: loss = 0.189262 (* 1 = 0.189262 loss)
I1107 21:08:58.980345 26139 sgd_solver.cpp:106] Iteration 12400, lr = 0.0001
I1107 21:18:00.292421 26139 solver.cpp:228] Iteration 12500, loss = 0.219223
I1107 21:18:00.292453 26139 solver.cpp:244]     Train net output #0: accuracy = 0.9375
I1107 21:18:00.292476 26139 solver.cpp:244]     Train net output #1: loss = 0.219223 (* 1 = 0.219223 loss)
I1107 21:18:00.292497 26139 sgd_solver.cpp:106] Iteration 12500, lr = 0.0001
I1107 21:26:50.779834 26139 solver.cpp:228] Iteration 12600, loss = 0.274969
I1107 21:26:50.779870 26139 solver.cpp:244]     Train net output #0: accuracy = 0.90625
I1107 21:26:50.779880 26139 solver.cpp:244]     Train net output #1: loss = 0.274969 (* 1 = 0.274969 loss)
I1107 21:26:50.779886 26139 sgd_solver.cpp:106] Iteration 12600, lr = 0.0001
I1107 21:36:25.850929 26139 solver.cpp:228] Iteration 12700, loss = 0.29334
I1107 21:36:25.850973 26139 solver.cpp:244]     Train net output #0: accuracy = 0.894531
I1107 21:36:25.850987 26139 solver.cpp:244]     Train net output #1: loss = 0.29334 (* 1 = 0.29334 loss)
I1107 21:36:25.851011 26139 sgd_solver.cpp:106] Iteration 12700, lr = 0.0001
I1107 21:47:46.236070 26139 solver.cpp:228] Iteration 12800, loss = 0.24878
I1107 21:47:46.236100 26139 solver.cpp:244]     Train net output #0: accuracy = 0.910156
I1107 21:47:46.236109 26139 solver.cpp:244]     Train net output #1: loss = 0.24878 (* 1 = 0.24878 loss)
I1107 21:47:46.236116 26139 sgd_solver.cpp:106] Iteration 12800, lr = 0.0001
I1107 21:59:41.099169 26139 solver.cpp:228] Iteration 12900, loss = 0.307208
I1107 21:59:41.099210 26139 solver.cpp:244]     Train net output #0: accuracy = 0.890625
I1107 21:59:41.099222 26139 solver.cpp:244]     Train net output #1: loss = 0.307208 (* 1 = 0.307208 loss)
I1107 21:59:41.099243 26139 sgd_solver.cpp:106] Iteration 12900, lr = 0.0001
I1107 22:09:19.060411 26139 solver.cpp:454] Snapshotting to binary proto file /home/kevin/snapshot/_iter_13000.caffemodel
I1107 22:09:24.421613 26139 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/kevin/snapshot/_iter_13000.solverstate
>>> 2016-11-07 22:09:24.685643 Begin model classification tests
>>> 2016-11-07 22:22:27.681347 Iteration 13000 mean classification accuracy (max)  0.873022312373
>>> 2016-11-07 22:22:27.697499 Iteration 13000 mean classification accuracy (ave) 0.892089249493
>>> 2016-11-07 22:22:27.697532 Iteration 13000 mean testing loss 0.624971243322
>>> 2016-11-07 22:22:27.697551 Iteration 13000 mean confusion matrix
[ 1.          0.92        0.97        0.7         0.93        0.95        0.85
  1.          0.81        0.95        0.9         0.8         0.86046512
  0.9         0.70930233  0.7         0.88        0.97        0.9         0.75
  1.          0.96        0.97        0.47674419  0.94736842  0.95
  0.97959184  0.7         0.95        0.85        0.98        0.95        0.8
  0.79        0.9         1.          0.77        0.96        0.5         0.8       ]
I1107 22:22:37.963515 26139 solver.cpp:228] Iteration 13000, loss = 0.259057
I1107 22:22:37.963556 26139 solver.cpp:244]     Train net output #0: accuracy = 0.917969
I1107 22:22:37.963574 26139 solver.cpp:244]     Train net output #1: loss = 0.259057 (* 1 = 0.259057 loss)
I1107 22:22:37.963604 26139 sgd_solver.cpp:106] Iteration 13000, lr = 0.0001
I1107 22:32:37.348139 26139 solver.cpp:228] Iteration 13100, loss = 0.295338
I1107 22:32:37.348193 26139 solver.cpp:244]     Train net output #0: accuracy = 0.882812
I1107 22:32:37.348206 26139 solver.cpp:244]     Train net output #1: loss = 0.295338 (* 1 = 0.295338 loss)
I1107 22:32:37.348227 26139 sgd_solver.cpp:106] Iteration 13100, lr = 0.0001
I1107 22:42:16.233847 26139 solver.cpp:228] Iteration 13200, loss = 0.171338
I1107 22:42:16.233891 26139 solver.cpp:244]     Train net output #0: accuracy = 0.949219
I1107 22:42:16.233901 26139 solver.cpp:244]     Train net output #1: loss = 0.171338 (* 1 = 0.171338 loss)
I1107 22:42:16.233909 26139 sgd_solver.cpp:106] Iteration 13200, lr = 0.0001
I1107 22:51:57.273958 26139 solver.cpp:228] Iteration 13300, loss = 0.232256
I1107 22:51:57.274001 26139 solver.cpp:244]     Train net output #0: accuracy = 0.914062
I1107 22:51:57.274010 26139 solver.cpp:244]     Train net output #1: loss = 0.232256 (* 1 = 0.232256 loss)
I1107 22:51:57.274029 26139 sgd_solver.cpp:106] Iteration 13300, lr = 0.0001
I1107 23:01:30.580016 26139 solver.cpp:228] Iteration 13400, loss = 0.317709
I1107 23:01:30.580054 26139 solver.cpp:244]     Train net output #0: accuracy = 0.902344
I1107 23:01:30.580075 26139 solver.cpp:244]     Train net output #1: loss = 0.317709 (* 1 = 0.317709 loss)
I1107 23:01:30.580082 26139 sgd_solver.cpp:106] Iteration 13400, lr = 0.0001
I1107 23:11:29.291355 26139 solver.cpp:228] Iteration 13500, loss = 0.247697
I1107 23:11:29.291394 26139 solver.cpp:244]     Train net output #0: accuracy = 0.921875
I1107 23:11:29.291419 26139 solver.cpp:244]     Train net output #1: loss = 0.247697 (* 1 = 0.247697 loss)
I1107 23:11:29.291429 26139 sgd_solver.cpp:106] Iteration 13500, lr = 0.0001
I1107 23:20:55.567704 26139 solver.cpp:228] Iteration 13600, loss = 0.328538
I1107 23:20:55.567754 26139 solver.cpp:244]     Train net output #0: accuracy = 0.90625
I1107 23:20:55.567766 26139 solver.cpp:244]     Train net output #1: loss = 0.328538 (* 1 = 0.328538 loss)
I1107 23:20:55.567787 26139 sgd_solver.cpp:106] Iteration 13600, lr = 0.0001
I1107 23:30:37.330574 26139 solver.cpp:228] Iteration 13700, loss = 0.331158
I1107 23:30:37.330620 26139 solver.cpp:244]     Train net output #0: accuracy = 0.90625
I1107 23:30:37.330628 26139 solver.cpp:244]     Train net output #1: loss = 0.331158 (* 1 = 0.331158 loss)
I1107 23:30:37.330646 26139 sgd_solver.cpp:106] Iteration 13700, lr = 0.0001
I1107 23:40:40.369565 26139 solver.cpp:228] Iteration 13800, loss = 0.286846
I1107 23:40:40.369603 26139 solver.cpp:244]     Train net output #0: accuracy = 0.894531
I1107 23:40:40.369628 26139 solver.cpp:244]     Train net output #1: loss = 0.286846 (* 1 = 0.286846 loss)
I1107 23:40:40.369648 26139 sgd_solver.cpp:106] Iteration 13800, lr = 0.0001
I1107 23:51:17.350484 26139 solver.cpp:228] Iteration 13900, loss = 0.243909
I1107 23:51:17.350529 26139 solver.cpp:244]     Train net output #0: accuracy = 0.914062
I1107 23:51:17.350539 26139 solver.cpp:244]     Train net output #1: loss = 0.243909 (* 1 = 0.243909 loss)
I1107 23:51:17.350558 26139 sgd_solver.cpp:106] Iteration 13900, lr = 0.0001
I1108 00:00:38.379714 26139 solver.cpp:454] Snapshotting to binary proto file /home/kevin/snapshot/_iter_14000.caffemodel
I1108 00:00:48.561390 26139 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/kevin/snapshot/_iter_14000.solverstate
>>> 2016-11-08 00:00:48.809559 Begin model classification tests
>>> 2016-11-08 00:20:15.786570 Iteration 14000 mean classification accuracy (max)  0.874239350913
>>> 2016-11-08 00:20:15.786617 Iteration 14000 mean classification accuracy (ave) 0.892089249493
>>> 2016-11-08 00:20:15.786626 Iteration 14000 mean testing loss 0.623912589872
>>> 2016-11-08 00:20:15.786649 Iteration 14000 mean confusion matrix
[ 1.          0.92        0.97        0.7         0.93        0.96        0.85
  1.          0.81        0.95        0.9         0.8         0.86046512
  0.9         0.69767442  0.7         0.88        0.97        0.9         0.75
  1.          0.96        0.98        0.46511628  0.94736842  0.95
  0.97959184  0.7         0.95        0.85        0.98        0.95        0.8
  0.8         0.9         1.          0.78        0.95        0.45        0.8       ]
I1108 00:20:31.880815 26139 solver.cpp:228] Iteration 14000, loss = 0.303078
I1108 00:20:31.880847 26139 solver.cpp:244]     Train net output #0: accuracy = 0.890625
I1108 00:20:31.880856 26139 solver.cpp:244]     Train net output #1: loss = 0.303078 (* 1 = 0.303078 loss)
I1108 00:20:31.880863 26139 sgd_solver.cpp:106] Iteration 14000, lr = 0.0001
I1108 00:44:19.085691 26139 solver.cpp:228] Iteration 14100, loss = 0.271525
I1108 00:44:19.085736 26139 solver.cpp:244]     Train net output #0: accuracy = 0.90625
I1108 00:44:19.085763 26139 solver.cpp:244]     Train net output #1: loss = 0.271525 (* 1 = 0.271525 loss)
I1108 00:44:19.085777 26139 sgd_solver.cpp:106] Iteration 14100, lr = 0.0001
I1108 01:10:18.899847 26139 solver.cpp:228] Iteration 14200, loss = 0.381695
I1108 01:10:18.899930 26139 solver.cpp:244]     Train net output #0: accuracy = 0.878906
I1108 01:10:18.899976 26139 solver.cpp:244]     Train net output #1: loss = 0.381695 (* 1 = 0.381695 loss)
I1108 01:10:18.900020 26139 sgd_solver.cpp:106] Iteration 14200, lr = 0.0001
I1108 01:36:08.974164 26139 solver.cpp:228] Iteration 14300, loss = 0.219857
I1108 01:36:08.974254 26139 solver.cpp:244]     Train net output #0: accuracy = 0.921875
I1108 01:36:08.974293 26139 solver.cpp:244]     Train net output #1: loss = 0.219857 (* 1 = 0.219857 loss)
I1108 01:36:08.974334 26139 sgd_solver.cpp:106] Iteration 14300, lr = 0.0001
I1108 02:00:45.583647 26139 solver.cpp:228] Iteration 14400, loss = 0.250223
I1108 02:00:45.583729 26139 solver.cpp:244]     Train net output #0: accuracy = 0.914062
I1108 02:00:45.583770 26139 solver.cpp:244]     Train net output #1: loss = 0.250223 (* 1 = 0.250223 loss)
I1108 02:00:45.583792 26139 sgd_solver.cpp:106] Iteration 14400, lr = 0.0001
I1108 02:33:52.516424 26139 solver.cpp:228] Iteration 14500, loss = 0.289519
I1108 02:33:52.516477 26139 solver.cpp:244]     Train net output #0: accuracy = 0.894531
I1108 02:33:52.516495 26139 solver.cpp:244]     Train net output #1: loss = 0.289519 (* 1 = 0.289519 loss)
I1108 02:33:52.516515 26139 sgd_solver.cpp:106] Iteration 14500, lr = 0.0001
I1108 03:04:00.340816 26139 solver.cpp:228] Iteration 14600, loss = 0.257897
I1108 03:04:00.340852 26139 solver.cpp:244]     Train net output #0: accuracy = 0.914062
I1108 03:04:00.340868 26139 solver.cpp:244]     Train net output #1: loss = 0.257897 (* 1 = 0.257897 loss)
I1108 03:04:00.340878 26139 sgd_solver.cpp:106] Iteration 14600, lr = 0.0001
I1108 03:25:19.704396 26139 solver.cpp:228] Iteration 14700, loss = 0.237454
I1108 03:25:19.704430 26139 solver.cpp:244]     Train net output #0: accuracy = 0.914062
I1108 03:25:19.704440 26139 solver.cpp:244]     Train net output #1: loss = 0.237454 (* 1 = 0.237454 loss)
I1108 03:25:19.704448 26139 sgd_solver.cpp:106] Iteration 14700, lr = 0.0001
I1108 03:46:57.754034 26139 solver.cpp:228] Iteration 14800, loss = 0.255536
I1108 03:46:57.754091 26139 solver.cpp:244]     Train net output #0: accuracy = 0.90625
I1108 03:46:57.754120 26139 solver.cpp:244]     Train net output #1: loss = 0.255536 (* 1 = 0.255536 loss)
I1108 03:46:57.754142 26139 sgd_solver.cpp:106] Iteration 14800, lr = 0.0001
I1108 04:08:24.315656 26139 solver.cpp:228] Iteration 14900, loss = 0.180435
I1108 04:08:24.315696 26139 solver.cpp:244]     Train net output #0: accuracy = 0.9375
I1108 04:08:24.315706 26139 solver.cpp:244]     Train net output #1: loss = 0.180435 (* 1 = 0.180435 loss)
I1108 04:08:24.315724 26139 sgd_solver.cpp:106] Iteration 14900, lr = 0.0001
I1108 04:33:14.539979 26139 solver.cpp:454] Snapshotting to binary proto file /home/kevin/snapshot/_iter_15000.caffemodel
I1108 04:34:02.424715 26139 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/kevin/snapshot/_iter_15000.solverstate
>>> 2016-11-08 04:34:02.724327 Begin model classification tests
>>> 2016-11-08 05:24:12.614868 Iteration 15000 mean classification accuracy (max)  0.875050709939
>>> 2016-11-08 05:24:12.614923 Iteration 15000 mean classification accuracy (ave) 0.89046653144
>>> 2016-11-08 05:24:12.614931 Iteration 15000 mean testing loss 0.623910552831
>>> 2016-11-08 05:24:12.614945 Iteration 15000 mean confusion matrix
[ 1.          0.92        0.97        0.7         0.93        0.96        0.85
  1.          0.8         0.95        0.85        0.8         0.87209302
  0.9         0.70930233  0.7         0.88        0.97        0.9         0.75
  1.          0.96        0.98        0.45348837  0.94736842  0.95
  0.97959184  0.7         0.95        0.85        0.98        0.95        0.8
  0.78        0.9         0.99        0.78        0.95        0.45        0.8       ]
]0;kevin@kevin-desktop: ~/catkin_ws/src/romans_stack/model_net/caffe_netkevin@kevin-desktop:~/catkin_ws/src/romans_stack/model_net/caffe_net$ cd ..
d]0;kevin@kevin-desktop: ~/catkin_ws/src/romans_stack/model_netkevin@kevin-desktop:~/catkin_ws/src/romans_stack/model_net$ cd heavy/
~/   tmp/ 
]0;kevin@kevin-desktop: ~/catkin_ws/src/romans_stack/model_netkevin@kevin-desktop:~/catkin_ws/src/romans_stack/model_net$ cd heavy/
]0;kevin@kevin-desktop: ~/catkin_ws/src/romans_stack/model_net/heavykevin@kevin-desktop:~/catkin_ws/src/romans_stack/model_net/heavy$ cd heavy/[K[K[K[K[K[K[K[K[K./create_net_heavy.py
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Net<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Blob<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Solver<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
]0;kevin@kevin-desktop: ~/catkin_ws/src/romans_stack/model_net/heavykevin@kevin-desktop:~/catkin_ws/src/romans_stack/model_net/heavy$ ./create_net_heavy.py[12Pcd heavy/./create_net_heavy.py[K./ao[K[Ksolve.py
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Net<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Blob<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Solver<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1108 11:58:18.016067 10077 solver.cpp:48] Initializing solver from parameters: 
train_net: "train.prototxt"
test_net: "test.prototxt"
test_iter: 0
test_interval: 9999999
base_lr: 0.01
display: 100
max_iter: 450000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 5000
snapshot: 1000
snapshot_prefix: "/home/kevin/snapshot/snapshot"
solver_mode: GPU
I1108 11:58:18.016142 10077 solver.cpp:81] Creating training net from train_net file: train.prototxt
I1108 11:58:18.059451 10077 net.cpp:49] Initializing net from parameters: 
state {
  phase: TRAIN
}
layer {
  name: "img"
  type: "Python"
  top: "img"
  top: "label"
  python_param {
    module: "data_layers.model_net_layer"
    layer: "ModelNetDataLayer"
    param_str: "{\'img_size\': (250, 250), \'split\': \'train\', \'dataset_dir\': \'/home/kevin/dataset/processed_data\', \'variable\': \'depth_map\', \'dtype\': \'frame\', \'seed\': 1337, \'batch_size\': 128, \'mean\': 2}"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "img"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    name: "conv3_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv3_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  param {
    name: "conv4_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv4_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    name: "conv5_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv5_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 40
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1108 11:58:18.059653 10077 layer_factory.hpp:77] Creating layer img
I1108 11:58:23.827811 10077 net.cpp:91] Creating Layer img
I1108 11:58:23.827832 10077 net.cpp:399] img -> img
I1108 11:58:23.827841 10077 net.cpp:399] img -> label
{'img_size': (250, 250), 'split': 'train', 'dataset_dir': '/home/kevin/dataset/processed_data', 'variable': 'depth_map', 'dtype': 'frame', 'seed': 1337, 'batch_size': 128, 'mean': 2}
I1108 11:59:14.539696 10077 net.cpp:141] Setting up img
I1108 11:59:14.539721 10077 net.cpp:148] Top shape: 128 1 250 250 (8000000)
I1108 11:59:14.539726 10077 net.cpp:148] Top shape: 128 1 (128)
I1108 11:59:14.539727 10077 net.cpp:156] Memory required for data: 32000512
I1108 11:59:14.539733 10077 layer_factory.hpp:77] Creating layer label_img_1_split
I1108 11:59:14.539747 10077 net.cpp:91] Creating Layer label_img_1_split
I1108 11:59:14.539750 10077 net.cpp:425] label_img_1_split <- label
I1108 11:59:14.539757 10077 net.cpp:399] label_img_1_split -> label_img_1_split_0
I1108 11:59:14.539763 10077 net.cpp:399] label_img_1_split -> label_img_1_split_1
I1108 11:59:14.576545 10077 net.cpp:141] Setting up label_img_1_split
I1108 11:59:14.576589 10077 net.cpp:148] Top shape: 128 1 (128)
I1108 11:59:14.576597 10077 net.cpp:148] Top shape: 128 1 (128)
I1108 11:59:14.576603 10077 net.cpp:156] Memory required for data: 32001536
I1108 11:59:14.576611 10077 layer_factory.hpp:77] Creating layer conv1
I1108 11:59:14.576639 10077 net.cpp:91] Creating Layer conv1
I1108 11:59:14.576647 10077 net.cpp:425] conv1 <- img
I1108 11:59:14.576656 10077 net.cpp:399] conv1 -> conv1
I1108 11:59:14.660717 10077 net.cpp:141] Setting up conv1
I1108 11:59:14.660748 10077 net.cpp:148] Top shape: 128 128 125 125 (256000000)
I1108 11:59:14.660753 10077 net.cpp:156] Memory required for data: 1056001536
I1108 11:59:14.660768 10077 layer_factory.hpp:77] Creating layer relu1
I1108 11:59:14.660779 10077 net.cpp:91] Creating Layer relu1
I1108 11:59:14.660784 10077 net.cpp:425] relu1 <- conv1
I1108 11:59:14.660789 10077 net.cpp:386] relu1 -> conv1 (in-place)
I1108 11:59:14.660796 10077 net.cpp:141] Setting up relu1
I1108 11:59:14.660800 10077 net.cpp:148] Top shape: 128 128 125 125 (256000000)
I1108 11:59:14.660802 10077 net.cpp:156] Memory required for data: 2080001536
I1108 11:59:14.660805 10077 layer_factory.hpp:77] Creating layer pool1
I1108 11:59:14.660812 10077 net.cpp:91] Creating Layer pool1
I1108 11:59:14.660815 10077 net.cpp:425] pool1 <- conv1
I1108 11:59:14.660818 10077 net.cpp:399] pool1 -> pool1
I1108 11:59:14.702098 10077 net.cpp:141] Setting up pool1
I1108 11:59:14.702122 10077 net.cpp:148] Top shape: 128 128 62 62 (62980096)
I1108 11:59:14.702126 10077 net.cpp:156] Memory required for data: 2331921920
I1108 11:59:14.702129 10077 layer_factory.hpp:77] Creating layer norm1
I1108 11:59:14.702149 10077 net.cpp:91] Creating Layer norm1
I1108 11:59:14.702153 10077 net.cpp:425] norm1 <- pool1
I1108 11:59:14.702167 10077 net.cpp:399] norm1 -> norm1
I1108 11:59:14.702199 10077 net.cpp:141] Setting up norm1
I1108 11:59:14.702203 10077 net.cpp:148] Top shape: 128 128 62 62 (62980096)
I1108 11:59:14.702205 10077 net.cpp:156] Memory required for data: 2583842304
I1108 11:59:14.702208 10077 layer_factory.hpp:77] Creating layer conv2
I1108 11:59:14.702216 10077 net.cpp:91] Creating Layer conv2
I1108 11:59:14.702219 10077 net.cpp:425] conv2 <- norm1
I1108 11:59:14.702222 10077 net.cpp:399] conv2 -> conv2
I1108 11:59:14.706794 10077 net.cpp:141] Setting up conv2
I1108 11:59:14.706804 10077 net.cpp:148] Top shape: 128 256 31 31 (31490048)
I1108 11:59:14.706806 10077 net.cpp:156] Memory required for data: 2709802496
I1108 11:59:14.706815 10077 layer_factory.hpp:77] Creating layer relu2
I1108 11:59:14.706820 10077 net.cpp:91] Creating Layer relu2
I1108 11:59:14.706822 10077 net.cpp:425] relu2 <- conv2
I1108 11:59:14.706826 10077 net.cpp:386] relu2 -> conv2 (in-place)
I1108 11:59:14.706831 10077 net.cpp:141] Setting up relu2
I1108 11:59:14.706835 10077 net.cpp:148] Top shape: 128 256 31 31 (31490048)
I1108 11:59:14.706836 10077 net.cpp:156] Memory required for data: 2835762688
I1108 11:59:14.706838 10077 layer_factory.hpp:77] Creating layer pool2
I1108 11:59:14.706843 10077 net.cpp:91] Creating Layer pool2
I1108 11:59:14.706845 10077 net.cpp:425] pool2 <- conv2
I1108 11:59:14.706848 10077 net.cpp:399] pool2 -> pool2
I1108 11:59:14.706872 10077 net.cpp:141] Setting up pool2
I1108 11:59:14.706876 10077 net.cpp:148] Top shape: 128 256 15 15 (7372800)
I1108 11:59:14.706878 10077 net.cpp:156] Memory required for data: 2865253888
I1108 11:59:14.706881 10077 layer_factory.hpp:77] Creating layer norm2
I1108 11:59:14.706884 10077 net.cpp:91] Creating Layer norm2
I1108 11:59:14.706887 10077 net.cpp:425] norm2 <- pool2
I1108 11:59:14.706889 10077 net.cpp:399] norm2 -> norm2
I1108 11:59:14.706907 10077 net.cpp:141] Setting up norm2
I1108 11:59:14.706909 10077 net.cpp:148] Top shape: 128 256 15 15 (7372800)
I1108 11:59:14.706912 10077 net.cpp:156] Memory required for data: 2894745088
I1108 11:59:14.706913 10077 layer_factory.hpp:77] Creating layer conv3
I1108 11:59:14.706919 10077 net.cpp:91] Creating Layer conv3
I1108 11:59:14.706921 10077 net.cpp:425] conv3 <- norm2
I1108 11:59:14.706924 10077 net.cpp:399] conv3 -> conv3
I1108 11:59:14.708567 10077 net.cpp:141] Setting up conv3
I1108 11:59:14.708575 10077 net.cpp:148] Top shape: 128 256 15 15 (7372800)
I1108 11:59:14.708577 10077 net.cpp:156] Memory required for data: 2924236288
I1108 11:59:14.708583 10077 layer_factory.hpp:77] Creating layer relu3
I1108 11:59:14.708587 10077 net.cpp:91] Creating Layer relu3
I1108 11:59:14.708590 10077 net.cpp:425] relu3 <- conv3
I1108 11:59:14.708592 10077 net.cpp:386] relu3 -> conv3 (in-place)
I1108 11:59:14.708596 10077 net.cpp:141] Setting up relu3
I1108 11:59:14.708600 10077 net.cpp:148] Top shape: 128 256 15 15 (7372800)
I1108 11:59:14.708601 10077 net.cpp:156] Memory required for data: 2953727488
I1108 11:59:14.708603 10077 layer_factory.hpp:77] Creating layer pool3
I1108 11:59:14.708607 10077 net.cpp:91] Creating Layer pool3
I1108 11:59:14.708609 10077 net.cpp:425] pool3 <- conv3
I1108 11:59:14.708612 10077 net.cpp:399] pool3 -> pool3
I1108 11:59:14.708633 10077 net.cpp:141] Setting up pool3
I1108 11:59:14.708636 10077 net.cpp:148] Top shape: 128 256 7 7 (1605632)
I1108 11:59:14.708638 10077 net.cpp:156] Memory required for data: 2960150016
I1108 11:59:14.708641 10077 layer_factory.hpp:77] Creating layer conv4
I1108 11:59:14.708647 10077 net.cpp:91] Creating Layer conv4
I1108 11:59:14.708648 10077 net.cpp:425] conv4 <- pool3
I1108 11:59:14.708652 10077 net.cpp:399] conv4 -> conv4
I1108 11:59:14.711649 10077 net.cpp:141] Setting up conv4
I1108 11:59:14.711657 10077 net.cpp:148] Top shape: 128 512 7 7 (3211264)
I1108 11:59:14.711658 10077 net.cpp:156] Memory required for data: 2972995072
I1108 11:59:14.711663 10077 layer_factory.hpp:77] Creating layer relu4
I1108 11:59:14.711666 10077 net.cpp:91] Creating Layer relu4
I1108 11:59:14.711669 10077 net.cpp:425] relu4 <- conv4
I1108 11:59:14.711673 10077 net.cpp:386] relu4 -> conv4 (in-place)
I1108 11:59:14.711676 10077 net.cpp:141] Setting up relu4
I1108 11:59:14.711678 10077 net.cpp:148] Top shape: 128 512 7 7 (3211264)
I1108 11:59:14.711680 10077 net.cpp:156] Memory required for data: 2985840128
I1108 11:59:14.711683 10077 layer_factory.hpp:77] Creating layer conv5
I1108 11:59:14.711688 10077 net.cpp:91] Creating Layer conv5
I1108 11:59:14.711690 10077 net.cpp:425] conv5 <- conv4
I1108 11:59:14.711694 10077 net.cpp:399] conv5 -> conv5
I1108 11:59:14.717362 10077 net.cpp:141] Setting up conv5
I1108 11:59:14.717380 10077 net.cpp:148] Top shape: 128 512 7 7 (3211264)
I1108 11:59:14.717384 10077 net.cpp:156] Memory required for data: 2998685184
I1108 11:59:14.717396 10077 layer_factory.hpp:77] Creating layer relu5
I1108 11:59:14.717409 10077 net.cpp:91] Creating Layer relu5
I1108 11:59:14.717417 10077 net.cpp:425] relu5 <- conv5
I1108 11:59:14.717423 10077 net.cpp:386] relu5 -> conv5 (in-place)
I1108 11:59:14.717430 10077 net.cpp:141] Setting up relu5
I1108 11:59:14.717435 10077 net.cpp:148] Top shape: 128 512 7 7 (3211264)
I1108 11:59:14.717442 10077 net.cpp:156] Memory required for data: 3011530240
I1108 11:59:14.717448 10077 layer_factory.hpp:77] Creating layer pool5
I1108 11:59:14.717455 10077 net.cpp:91] Creating Layer pool5
I1108 11:59:14.717459 10077 net.cpp:425] pool5 <- conv5
I1108 11:59:14.717468 10077 net.cpp:399] pool5 -> pool5
I1108 11:59:14.717547 10077 net.cpp:141] Setting up pool5
I1108 11:59:14.717555 10077 net.cpp:148] Top shape: 128 512 3 3 (589824)
I1108 11:59:14.717558 10077 net.cpp:156] Memory required for data: 3013889536
I1108 11:59:14.717562 10077 layer_factory.hpp:77] Creating layer fc6
I1108 11:59:14.777448 10077 net.cpp:91] Creating Layer fc6
I1108 11:59:14.777489 10077 net.cpp:425] fc6 <- pool5
I1108 11:59:14.777511 10077 net.cpp:399] fc6 -> fc6
I1108 11:59:14.893090 10077 net.cpp:141] Setting up fc6
I1108 11:59:14.893208 10077 net.cpp:148] Top shape: 128 4096 (524288)
I1108 11:59:14.893241 10077 net.cpp:156] Memory required for data: 3015986688
I1108 11:59:14.893280 10077 layer_factory.hpp:77] Creating layer relu6
I1108 11:59:14.893308 10077 net.cpp:91] Creating Layer relu6
I1108 11:59:14.893321 10077 net.cpp:425] relu6 <- fc6
I1108 11:59:14.893339 10077 net.cpp:386] relu6 -> fc6 (in-place)
I1108 11:59:14.893360 10077 net.cpp:141] Setting up relu6
I1108 11:59:14.893373 10077 net.cpp:148] Top shape: 128 4096 (524288)
I1108 11:59:14.893383 10077 net.cpp:156] Memory required for data: 3018083840
I1108 11:59:14.893391 10077 layer_factory.hpp:77] Creating layer drop6
I1108 11:59:14.935501 10077 net.cpp:91] Creating Layer drop6
I1108 11:59:14.935537 10077 net.cpp:425] drop6 <- fc6
I1108 11:59:14.935549 10077 net.cpp:386] drop6 -> fc6 (in-place)
I1108 11:59:14.935614 10077 net.cpp:141] Setting up drop6
I1108 11:59:14.935631 10077 net.cpp:148] Top shape: 128 4096 (524288)
I1108 11:59:14.935637 10077 net.cpp:156] Memory required for data: 3020180992
I1108 11:59:14.935643 10077 layer_factory.hpp:77] Creating layer fc7
I1108 11:59:14.935654 10077 net.cpp:91] Creating Layer fc7
I1108 11:59:14.935662 10077 net.cpp:425] fc7 <- fc6
I1108 11:59:14.935670 10077 net.cpp:399] fc7 -> fc7
I1108 11:59:15.038781 10077 net.cpp:141] Setting up fc7
I1108 11:59:15.038816 10077 net.cpp:148] Top shape: 128 4096 (524288)
I1108 11:59:15.038822 10077 net.cpp:156] Memory required for data: 3022278144
I1108 11:59:15.038846 10077 layer_factory.hpp:77] Creating layer relu7
I1108 11:59:15.038867 10077 net.cpp:91] Creating Layer relu7
I1108 11:59:15.038883 10077 net.cpp:425] relu7 <- fc7
I1108 11:59:15.038902 10077 net.cpp:386] relu7 -> fc7 (in-place)
I1108 11:59:15.038910 10077 net.cpp:141] Setting up relu7
I1108 11:59:15.038918 10077 net.cpp:148] Top shape: 128 4096 (524288)
I1108 11:59:15.038923 10077 net.cpp:156] Memory required for data: 3024375296
I1108 11:59:15.038928 10077 layer_factory.hpp:77] Creating layer drop7
I1108 11:59:15.038940 10077 net.cpp:91] Creating Layer drop7
I1108 11:59:15.038950 10077 net.cpp:425] drop7 <- fc7
I1108 11:59:15.038960 10077 net.cpp:386] drop7 -> fc7 (in-place)
I1108 11:59:15.039006 10077 net.cpp:141] Setting up drop7
I1108 11:59:15.039026 10077 net.cpp:148] Top shape: 128 4096 (524288)
I1108 11:59:15.039031 10077 net.cpp:156] Memory required for data: 3026472448
I1108 11:59:15.039049 10077 layer_factory.hpp:77] Creating layer fc8
I1108 11:59:15.039078 10077 net.cpp:91] Creating Layer fc8
I1108 11:59:15.039085 10077 net.cpp:425] fc8 <- fc7
I1108 11:59:15.039094 10077 net.cpp:399] fc8 -> fc8
I1108 11:59:15.040797 10077 net.cpp:141] Setting up fc8
I1108 11:59:15.040825 10077 net.cpp:148] Top shape: 128 40 (5120)
I1108 11:59:15.040835 10077 net.cpp:156] Memory required for data: 3026492928
I1108 11:59:15.040858 10077 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I1108 11:59:15.040868 10077 net.cpp:91] Creating Layer fc8_fc8_0_split
I1108 11:59:15.040875 10077 net.cpp:425] fc8_fc8_0_split <- fc8
I1108 11:59:15.040892 10077 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1108 11:59:15.040902 10077 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1108 11:59:15.040957 10077 net.cpp:141] Setting up fc8_fc8_0_split
I1108 11:59:15.040966 10077 net.cpp:148] Top shape: 128 40 (5120)
I1108 11:59:15.040985 10077 net.cpp:148] Top shape: 128 40 (5120)
I1108 11:59:15.040992 10077 net.cpp:156] Memory required for data: 3026533888
I1108 11:59:15.041007 10077 layer_factory.hpp:77] Creating layer accuracy
I1108 11:59:15.041031 10077 net.cpp:91] Creating Layer accuracy
I1108 11:59:15.041049 10077 net.cpp:425] accuracy <- fc8_fc8_0_split_0
I1108 11:59:15.041065 10077 net.cpp:425] accuracy <- label_img_1_split_0
I1108 11:59:15.041084 10077 net.cpp:399] accuracy -> accuracy
I1108 11:59:15.041103 10077 net.cpp:141] Setting up accuracy
I1108 11:59:15.041110 10077 net.cpp:148] Top shape: (1)
I1108 11:59:15.041117 10077 net.cpp:156] Memory required for data: 3026533892
I1108 11:59:15.041124 10077 layer_factory.hpp:77] Creating layer loss
I1108 11:59:15.041136 10077 net.cpp:91] Creating Layer loss
I1108 11:59:15.041152 10077 net.cpp:425] loss <- fc8_fc8_0_split_1
I1108 11:59:15.041159 10077 net.cpp:425] loss <- label_img_1_split_1
I1108 11:59:15.041177 10077 net.cpp:399] loss -> loss
I1108 11:59:15.041187 10077 layer_factory.hpp:77] Creating layer loss
I1108 11:59:15.041280 10077 net.cpp:141] Setting up loss
I1108 11:59:15.041290 10077 net.cpp:148] Top shape: (1)
I1108 11:59:15.041296 10077 net.cpp:151]     with loss weight 1
I1108 11:59:15.041312 10077 net.cpp:156] Memory required for data: 3026533896
I1108 11:59:15.041321 10077 net.cpp:217] loss needs backward computation.
I1108 11:59:15.041327 10077 net.cpp:219] accuracy does not need backward computation.
I1108 11:59:15.041335 10077 net.cpp:217] fc8_fc8_0_split needs backward computation.
I1108 11:59:15.041342 10077 net.cpp:217] fc8 needs backward computation.
I1108 11:59:15.041349 10077 net.cpp:217] drop7 needs backward computation.
I1108 11:59:15.041357 10077 net.cpp:217] relu7 needs backward computation.
I1108 11:59:15.041363 10077 net.cpp:217] fc7 needs backward computation.
I1108 11:59:15.041370 10077 net.cpp:217] drop6 needs backward computation.
I1108 11:59:15.041378 10077 net.cpp:217] relu6 needs backward computation.
I1108 11:59:15.041384 10077 net.cpp:217] fc6 needs backward computation.
I1108 11:59:15.041393 10077 net.cpp:217] pool5 needs backward computation.
I1108 11:59:15.041400 10077 net.cpp:217] relu5 needs backward computation.
I1108 11:59:15.041407 10077 net.cpp:217] conv5 needs backward computation.
I1108 11:59:15.041414 10077 net.cpp:217] relu4 needs backward computation.
I1108 11:59:15.041422 10077 net.cpp:217] conv4 needs backward computation.
I1108 11:59:15.041429 10077 net.cpp:217] pool3 needs backward computation.
I1108 11:59:15.041436 10077 net.cpp:217] relu3 needs backward computation.
I1108 11:59:15.041443 10077 net.cpp:217] conv3 needs backward computation.
I1108 11:59:15.041451 10077 net.cpp:217] norm2 needs backward computation.
I1108 11:59:15.041458 10077 net.cpp:217] pool2 needs backward computation.
I1108 11:59:15.041465 10077 net.cpp:217] relu2 needs backward computation.
I1108 11:59:15.041472 10077 net.cpp:217] conv2 needs backward computation.
I1108 11:59:15.041479 10077 net.cpp:217] norm1 needs backward computation.
I1108 11:59:15.041487 10077 net.cpp:217] pool1 needs backward computation.
I1108 11:59:15.041494 10077 net.cpp:217] relu1 needs backward computation.
I1108 11:59:15.041501 10077 net.cpp:217] conv1 needs backward computation.
I1108 11:59:15.041509 10077 net.cpp:219] label_img_1_split does not need backward computation.
I1108 11:59:15.041517 10077 net.cpp:219] img does not need backward computation.
I1108 11:59:15.041524 10077 net.cpp:261] This network produces output accuracy
I1108 11:59:15.041532 10077 net.cpp:261] This network produces output loss
I1108 11:59:15.041550 10077 net.cpp:274] Network initialization done.
I1108 11:59:15.042119 10077 solver.cpp:181] Creating test net (#0) specified by test_net file: test.prototxt
I1108 11:59:15.042330 10077 net.cpp:49] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "img"
  type: "Python"
  top: "img"
  top: "label"
  python_param {
    module: "data_layers.model_net_layer"
    layer: "ModelNetDataLayer"
    param_str: "{\'img_size\': (250, 250), \'split\': \'test\', \'dataset_dir\': \'/home/kevin/dataset/processed_data\', \'variable\': \'depth_map\', \'dtype\': \'object\', \'seed\': 1337, \'batch_size\': 128, \'mean\': 2}"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "img"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    name: "conv3_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv3_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  param {
    name: "conv4_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv4_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    name: "conv5_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv5_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 40
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1108 11:59:15.043661 10077 layer_factory.hpp:77] Creating layer img
I1108 11:59:15.043717 10077 net.cpp:91] Creating Layer img
I1108 11:59:15.043728 10077 net.cpp:399] img -> img
I1108 11:59:15.043740 10077 net.cpp:399] img -> label
{'img_size': (250, 250), 'split': 'test', 'dataset_dir': '/home/kevin/dataset/processed_data', 'variable': 'depth_map', 'dtype': 'object', 'seed': 1337, 'batch_size': 128, 'mean': 2}
I1108 11:59:15.268915 10077 net.cpp:141] Setting up img
I1108 11:59:15.268946 10077 net.cpp:148] Top shape: 24 1 250 250 (1500000)
I1108 11:59:15.268968 10077 net.cpp:148] Top shape: 24 1 (24)
I1108 11:59:15.268972 10077 net.cpp:156] Memory required for data: 6000096
I1108 11:59:15.268996 10077 layer_factory.hpp:77] Creating layer label_img_1_split
I1108 11:59:15.269011 10077 net.cpp:91] Creating Layer label_img_1_split
I1108 11:59:15.269016 10077 net.cpp:425] label_img_1_split <- label
I1108 11:59:15.269026 10077 net.cpp:399] label_img_1_split -> label_img_1_split_0
I1108 11:59:15.269037 10077 net.cpp:399] label_img_1_split -> label_img_1_split_1
I1108 11:59:15.269093 10077 net.cpp:141] Setting up label_img_1_split
I1108 11:59:15.269100 10077 net.cpp:148] Top shape: 24 1 (24)
I1108 11:59:15.269115 10077 net.cpp:148] Top shape: 24 1 (24)
I1108 11:59:15.269120 10077 net.cpp:156] Memory required for data: 6000288
I1108 11:59:15.269137 10077 layer_factory.hpp:77] Creating layer conv1
I1108 11:59:15.269150 10077 net.cpp:91] Creating Layer conv1
I1108 11:59:15.269156 10077 net.cpp:425] conv1 <- img
I1108 11:59:15.269163 10077 net.cpp:399] conv1 -> conv1
I1108 11:59:15.269384 10077 net.cpp:141] Setting up conv1
I1108 11:59:15.269399 10077 net.cpp:148] Top shape: 24 128 125 125 (48000000)
I1108 11:59:15.269415 10077 net.cpp:156] Memory required for data: 198000288
I1108 11:59:15.269428 10077 layer_factory.hpp:77] Creating layer relu1
I1108 11:59:15.269438 10077 net.cpp:91] Creating Layer relu1
I1108 11:59:15.269444 10077 net.cpp:425] relu1 <- conv1
I1108 11:59:15.269453 10077 net.cpp:386] relu1 -> conv1 (in-place)
I1108 11:59:15.269462 10077 net.cpp:141] Setting up relu1
I1108 11:59:15.269470 10077 net.cpp:148] Top shape: 24 128 125 125 (48000000)
I1108 11:59:15.269476 10077 net.cpp:156] Memory required for data: 390000288
I1108 11:59:15.269484 10077 layer_factory.hpp:77] Creating layer pool1
I1108 11:59:15.269493 10077 net.cpp:91] Creating Layer pool1
I1108 11:59:15.269501 10077 net.cpp:425] pool1 <- conv1
I1108 11:59:15.269510 10077 net.cpp:399] pool1 -> pool1
I1108 11:59:15.269546 10077 net.cpp:141] Setting up pool1
I1108 11:59:15.269556 10077 net.cpp:148] Top shape: 24 128 62 62 (11808768)
I1108 11:59:15.269562 10077 net.cpp:156] Memory required for data: 437235360
I1108 11:59:15.269568 10077 layer_factory.hpp:77] Creating layer norm1
I1108 11:59:15.269578 10077 net.cpp:91] Creating Layer norm1
I1108 11:59:15.269585 10077 net.cpp:425] norm1 <- pool1
I1108 11:59:15.269593 10077 net.cpp:399] norm1 -> norm1
I1108 11:59:15.269621 10077 net.cpp:141] Setting up norm1
I1108 11:59:15.269639 10077 net.cpp:148] Top shape: 24 128 62 62 (11808768)
I1108 11:59:15.269646 10077 net.cpp:156] Memory required for data: 484470432
I1108 11:59:15.269662 10077 layer_factory.hpp:77] Creating layer conv2
I1108 11:59:15.269672 10077 net.cpp:91] Creating Layer conv2
I1108 11:59:15.269680 10077 net.cpp:425] conv2 <- norm1
I1108 11:59:15.269690 10077 net.cpp:399] conv2 -> conv2
I1108 11:59:15.275609 10077 net.cpp:141] Setting up conv2
I1108 11:59:15.275645 10077 net.cpp:148] Top shape: 24 256 31 31 (5904384)
I1108 11:59:15.275667 10077 net.cpp:156] Memory required for data: 508087968
I1108 11:59:15.275684 10077 layer_factory.hpp:77] Creating layer relu2
I1108 11:59:15.275697 10077 net.cpp:91] Creating Layer relu2
I1108 11:59:15.275707 10077 net.cpp:425] relu2 <- conv2
I1108 11:59:15.275717 10077 net.cpp:386] relu2 -> conv2 (in-place)
I1108 11:59:15.275727 10077 net.cpp:141] Setting up relu2
I1108 11:59:15.275735 10077 net.cpp:148] Top shape: 24 256 31 31 (5904384)
I1108 11:59:15.275743 10077 net.cpp:156] Memory required for data: 531705504
I1108 11:59:15.275749 10077 layer_factory.hpp:77] Creating layer pool2
I1108 11:59:15.275760 10077 net.cpp:91] Creating Layer pool2
I1108 11:59:15.275768 10077 net.cpp:425] pool2 <- conv2
I1108 11:59:15.275775 10077 net.cpp:399] pool2 -> pool2
I1108 11:59:15.275813 10077 net.cpp:141] Setting up pool2
I1108 11:59:15.275823 10077 net.cpp:148] Top shape: 24 256 15 15 (1382400)
I1108 11:59:15.275831 10077 net.cpp:156] Memory required for data: 537235104
I1108 11:59:15.275838 10077 layer_factory.hpp:77] Creating layer norm2
I1108 11:59:15.275848 10077 net.cpp:91] Creating Layer norm2
I1108 11:59:15.275856 10077 net.cpp:425] norm2 <- pool2
I1108 11:59:15.275866 10077 net.cpp:399] norm2 -> norm2
I1108 11:59:15.275909 10077 net.cpp:141] Setting up norm2
I1108 11:59:15.275926 10077 net.cpp:148] Top shape: 24 256 15 15 (1382400)
I1108 11:59:15.275940 10077 net.cpp:156] Memory required for data: 542764704
I1108 11:59:15.275949 10077 layer_factory.hpp:77] Creating layer conv3
I1108 11:59:15.275964 10077 net.cpp:91] Creating Layer conv3
I1108 11:59:15.275972 10077 net.cpp:425] conv3 <- norm2
I1108 11:59:15.275982 10077 net.cpp:399] conv3 -> conv3
I1108 11:59:15.278424 10077 net.cpp:141] Setting up conv3
I1108 11:59:15.278465 10077 net.cpp:148] Top shape: 24 256 15 15 (1382400)
I1108 11:59:15.278479 10077 net.cpp:156] Memory required for data: 548294304
I1108 11:59:15.278499 10077 layer_factory.hpp:77] Creating layer relu3
I1108 11:59:15.278513 10077 net.cpp:91] Creating Layer relu3
I1108 11:59:15.278522 10077 net.cpp:425] relu3 <- conv3
I1108 11:59:15.278532 10077 net.cpp:386] relu3 -> conv3 (in-place)
I1108 11:59:15.278544 10077 net.cpp:141] Setting up relu3
I1108 11:59:15.278553 10077 net.cpp:148] Top shape: 24 256 15 15 (1382400)
I1108 11:59:15.278559 10077 net.cpp:156] Memory required for data: 553823904
I1108 11:59:15.278566 10077 layer_factory.hpp:77] Creating layer pool3
I1108 11:59:15.278576 10077 net.cpp:91] Creating Layer pool3
I1108 11:59:15.278584 10077 net.cpp:425] pool3 <- conv3
I1108 11:59:15.278592 10077 net.cpp:399] pool3 -> pool3
I1108 11:59:15.278647 10077 net.cpp:141] Setting up pool3
I1108 11:59:15.278658 10077 net.cpp:148] Top shape: 24 256 7 7 (301056)
I1108 11:59:15.278664 10077 net.cpp:156] Memory required for data: 555028128
I1108 11:59:15.278671 10077 layer_factory.hpp:77] Creating layer conv4
I1108 11:59:15.278686 10077 net.cpp:91] Creating Layer conv4
I1108 11:59:15.278692 10077 net.cpp:425] conv4 <- pool3
I1108 11:59:15.278702 10077 net.cpp:399] conv4 -> conv4
I1108 11:59:15.283146 10077 net.cpp:141] Setting up conv4
I1108 11:59:15.283202 10077 net.cpp:148] Top shape: 24 512 7 7 (602112)
I1108 11:59:15.283211 10077 net.cpp:156] Memory required for data: 557436576
I1108 11:59:15.283246 10077 layer_factory.hpp:77] Creating layer relu4
I1108 11:59:15.283260 10077 net.cpp:91] Creating Layer relu4
I1108 11:59:15.283278 10077 net.cpp:425] relu4 <- conv4
I1108 11:59:15.283288 10077 net.cpp:386] relu4 -> conv4 (in-place)
I1108 11:59:15.283318 10077 net.cpp:141] Setting up relu4
I1108 11:59:15.283326 10077 net.cpp:148] Top shape: 24 512 7 7 (602112)
I1108 11:59:15.283334 10077 net.cpp:156] Memory required for data: 559845024
I1108 11:59:15.283342 10077 layer_factory.hpp:77] Creating layer conv5
I1108 11:59:15.283357 10077 net.cpp:91] Creating Layer conv5
I1108 11:59:15.283365 10077 net.cpp:425] conv5 <- conv4
I1108 11:59:15.283375 10077 net.cpp:399] conv5 -> conv5
I1108 11:59:15.291007 10077 net.cpp:141] Setting up conv5
I1108 11:59:15.291045 10077 net.cpp:148] Top shape: 24 512 7 7 (602112)
I1108 11:59:15.291054 10077 net.cpp:156] Memory required for data: 562253472
I1108 11:59:15.291084 10077 layer_factory.hpp:77] Creating layer relu5
I1108 11:59:15.291111 10077 net.cpp:91] Creating Layer relu5
I1108 11:59:15.291129 10077 net.cpp:425] relu5 <- conv5
I1108 11:59:15.291149 10077 net.cpp:386] relu5 -> conv5 (in-place)
I1108 11:59:15.291160 10077 net.cpp:141] Setting up relu5
I1108 11:59:15.291169 10077 net.cpp:148] Top shape: 24 512 7 7 (602112)
I1108 11:59:15.291177 10077 net.cpp:156] Memory required for data: 564661920
I1108 11:59:15.291193 10077 layer_factory.hpp:77] Creating layer pool5
I1108 11:59:15.291213 10077 net.cpp:91] Creating Layer pool5
I1108 11:59:15.291229 10077 net.cpp:425] pool5 <- conv5
I1108 11:59:15.291247 10077 net.cpp:399] pool5 -> pool5
I1108 11:59:15.291330 10077 net.cpp:141] Setting up pool5
I1108 11:59:15.291340 10077 net.cpp:148] Top shape: 24 512 3 3 (110592)
I1108 11:59:15.291347 10077 net.cpp:156] Memory required for data: 565104288
I1108 11:59:15.291354 10077 layer_factory.hpp:77] Creating layer fc6
I1108 11:59:15.291366 10077 net.cpp:91] Creating Layer fc6
I1108 11:59:15.291373 10077 net.cpp:425] fc6 <- pool5
I1108 11:59:15.291383 10077 net.cpp:399] fc6 -> fc6
I1108 11:59:15.412942 10077 net.cpp:141] Setting up fc6
I1108 11:59:15.413041 10077 net.cpp:148] Top shape: 24 4096 (98304)
I1108 11:59:15.413059 10077 net.cpp:156] Memory required for data: 565497504
I1108 11:59:15.413075 10077 layer_factory.hpp:77] Creating layer relu6
I1108 11:59:15.413101 10077 net.cpp:91] Creating Layer relu6
I1108 11:59:15.413130 10077 net.cpp:425] relu6 <- fc6
I1108 11:59:15.413153 10077 net.cpp:386] relu6 -> fc6 (in-place)
I1108 11:59:15.413167 10077 net.cpp:141] Setting up relu6
I1108 11:59:15.413185 10077 net.cpp:148] Top shape: 24 4096 (98304)
I1108 11:59:15.413192 10077 net.cpp:156] Memory required for data: 565890720
I1108 11:59:15.413198 10077 layer_factory.hpp:77] Creating layer drop6
I1108 11:59:15.413228 10077 net.cpp:91] Creating Layer drop6
I1108 11:59:15.413244 10077 net.cpp:425] drop6 <- fc6
I1108 11:59:15.413264 10077 net.cpp:386] drop6 -> fc6 (in-place)
I1108 11:59:15.413326 10077 net.cpp:141] Setting up drop6
I1108 11:59:15.413347 10077 net.cpp:148] Top shape: 24 4096 (98304)
I1108 11:59:15.413363 10077 net.cpp:156] Memory required for data: 566283936
I1108 11:59:15.413372 10077 layer_factory.hpp:77] Creating layer fc7
I1108 11:59:15.413395 10077 net.cpp:91] Creating Layer fc7
I1108 11:59:15.413403 10077 net.cpp:425] fc7 <- fc6
I1108 11:59:15.413415 10077 net.cpp:399] fc7 -> fc7
I1108 11:59:15.526504 10077 net.cpp:141] Setting up fc7
I1108 11:59:15.526577 10077 net.cpp:148] Top shape: 24 4096 (98304)
I1108 11:59:15.526600 10077 net.cpp:156] Memory required for data: 566677152
I1108 11:59:15.526619 10077 layer_factory.hpp:77] Creating layer relu7
I1108 11:59:15.526659 10077 net.cpp:91] Creating Layer relu7
I1108 11:59:15.526669 10077 net.cpp:425] relu7 <- fc7
I1108 11:59:15.526684 10077 net.cpp:386] relu7 -> fc7 (in-place)
I1108 11:59:15.526700 10077 net.cpp:141] Setting up relu7
I1108 11:59:15.526718 10077 net.cpp:148] Top shape: 24 4096 (98304)
I1108 11:59:15.526724 10077 net.cpp:156] Memory required for data: 567070368
I1108 11:59:15.526741 10077 layer_factory.hpp:77] Creating layer drop7
I1108 11:59:15.526754 10077 net.cpp:91] Creating Layer drop7
I1108 11:59:15.526762 10077 net.cpp:425] drop7 <- fc7
I1108 11:59:15.526772 10077 net.cpp:386] drop7 -> fc7 (in-place)
I1108 11:59:15.526803 10077 net.cpp:141] Setting up drop7
I1108 11:59:15.526823 10077 net.cpp:148] Top shape: 24 4096 (98304)
I1108 11:59:15.526829 10077 net.cpp:156] Memory required for data: 567463584
I1108 11:59:15.526846 10077 layer_factory.hpp:77] Creating layer fc8
I1108 11:59:15.526859 10077 net.cpp:91] Creating Layer fc8
I1108 11:59:15.526867 10077 net.cpp:425] fc8 <- fc7
I1108 11:59:15.526877 10077 net.cpp:399] fc8 -> fc8
I1108 11:59:15.528642 10077 net.cpp:141] Setting up fc8
I1108 11:59:15.528707 10077 net.cpp:148] Top shape: 24 40 (960)
I1108 11:59:15.528715 10077 net.cpp:156] Memory required for data: 567467424
I1108 11:59:15.528745 10077 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I1108 11:59:15.528762 10077 net.cpp:91] Creating Layer fc8_fc8_0_split
I1108 11:59:15.528772 10077 net.cpp:425] fc8_fc8_0_split <- fc8
I1108 11:59:15.528786 10077 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1108 11:59:15.528801 10077 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1108 11:59:15.528844 10077 net.cpp:141] Setting up fc8_fc8_0_split
I1108 11:59:15.528857 10077 net.cpp:148] Top shape: 24 40 (960)
I1108 11:59:15.528874 10077 net.cpp:148] Top shape: 24 40 (960)
I1108 11:59:15.528887 10077 net.cpp:156] Memory required for data: 567475104
I1108 11:59:15.528898 10077 layer_factory.hpp:77] Creating layer accuracy
I1108 11:59:15.528914 10077 net.cpp:91] Creating Layer accuracy
I1108 11:59:15.528921 10077 net.cpp:425] accuracy <- fc8_fc8_0_split_0
I1108 11:59:15.528930 10077 net.cpp:425] accuracy <- label_img_1_split_0
I1108 11:59:15.528939 10077 net.cpp:399] accuracy -> accuracy
I1108 11:59:15.528951 10077 net.cpp:141] Setting up accuracy
I1108 11:59:15.528959 10077 net.cpp:148] Top shape: (1)
I1108 11:59:15.528966 10077 net.cpp:156] Memory required for data: 567475108
I1108 11:59:15.528973 10077 layer_factory.hpp:77] Creating layer loss
I1108 11:59:15.528983 10077 net.cpp:91] Creating Layer loss
I1108 11:59:15.528990 10077 net.cpp:425] loss <- fc8_fc8_0_split_1
I1108 11:59:15.528998 10077 net.cpp:425] loss <- label_img_1_split_1
I1108 11:59:15.529007 10077 net.cpp:399] loss -> loss
I1108 11:59:15.529018 10077 layer_factory.hpp:77] Creating layer loss
I1108 11:59:15.529100 10077 net.cpp:141] Setting up loss
I1108 11:59:15.529110 10077 net.cpp:148] Top shape: (1)
I1108 11:59:15.529117 10077 net.cpp:151]     with loss weight 1
I1108 11:59:15.529134 10077 net.cpp:156] Memory required for data: 567475112
I1108 11:59:15.529141 10077 net.cpp:217] loss needs backward computation.
I1108 11:59:15.529150 10077 net.cpp:219] accuracy does not need backward computation.
I1108 11:59:15.529156 10077 net.cpp:217] fc8_fc8_0_split needs backward computation.
I1108 11:59:15.529165 10077 net.cpp:217] fc8 needs backward computation.
I1108 11:59:15.529171 10077 net.cpp:217] drop7 needs backward computation.
I1108 11:59:15.529180 10077 net.cpp:217] relu7 needs backward computation.
I1108 11:59:15.529186 10077 net.cpp:217] fc7 needs backward computation.
I1108 11:59:15.529193 10077 net.cpp:217] drop6 needs backward computation.
I1108 11:59:15.529201 10077 net.cpp:217] relu6 needs backward computation.
I1108 11:59:15.529207 10077 net.cpp:217] fc6 needs backward computation.
I1108 11:59:15.529214 10077 net.cpp:217] pool5 needs backward computation.
I1108 11:59:15.529222 10077 net.cpp:217] relu5 needs backward computation.
I1108 11:59:15.529228 10077 net.cpp:217] conv5 needs backward computation.
I1108 11:59:15.529235 10077 net.cpp:217] relu4 needs backward computation.
I1108 11:59:15.529242 10077 net.cpp:217] conv4 needs backward computation.
I1108 11:59:15.529249 10077 net.cpp:217] pool3 needs backward computation.
I1108 11:59:15.529256 10077 net.cpp:217] relu3 needs backward computation.
I1108 11:59:15.529263 10077 net.cpp:217] conv3 needs backward computation.
I1108 11:59:15.529270 10077 net.cpp:217] norm2 needs backward computation.
I1108 11:59:15.529278 10077 net.cpp:217] pool2 needs backward computation.
I1108 11:59:15.529284 10077 net.cpp:217] relu2 needs backward computation.
I1108 11:59:15.529290 10077 net.cpp:217] conv2 needs backward computation.
I1108 11:59:15.529299 10077 net.cpp:217] norm1 needs backward computation.
I1108 11:59:15.529305 10077 net.cpp:217] pool1 needs backward computation.
I1108 11:59:15.529314 10077 net.cpp:217] relu1 needs backward computation.
I1108 11:59:15.529320 10077 net.cpp:217] conv1 needs backward computation.
I1108 11:59:15.529328 10077 net.cpp:219] label_img_1_split does not need backward computation.
I1108 11:59:15.529336 10077 net.cpp:219] img does not need backward computation.
I1108 11:59:15.529342 10077 net.cpp:261] This network produces output accuracy
I1108 11:59:15.529350 10077 net.cpp:261] This network produces output loss
I1108 11:59:15.529367 10077 net.cpp:274] Network initialization done.
I1108 11:59:15.529458 10077 solver.cpp:60] Solver scaffolding done.
I1108 11:59:15.558820 10077 solver.cpp:337] Iteration 0, Testing net (#0)
I1108 11:59:17.093662 10077 solver.cpp:228] Iteration 0, loss = 3.70453
I1108 11:59:17.093720 10077 solver.cpp:244]     Train net output #0: accuracy = 0.03125
I1108 11:59:17.093755 10077 solver.cpp:244]     Train net output #1: loss = 3.70453 (* 1 = 3.70453 loss)
I1108 11:59:17.093770 10077 sgd_solver.cpp:106] Iteration 0, lr = 0.01
^CTraceback (most recent call last):
  File "./solve.py", line 27, in <module>
    solver.step(1000)
  File "/home/kevin/caffeplus/python_layer/data_layers/model_net_layer.py", line 70, in reshape
    self.data, self.label = self.load_data(self.indices, self.idx)
  File "/home/kevin/caffeplus/python_layer/data_layers/model_net_layer.py", line 113, in load_data
    mat = scipy.io.loadmat('{}/{}.mat'.format(self.dataset_dir, filei))
  File "/usr/lib/python2.7/dist-packages/scipy/io/matlab/mio.py", line 125, in loadmat
    MR = mat_reader_factory(file_name, appendmat, **kwargs)
  File "/usr/lib/python2.7/dist-packages/scipy/io/matlab/mio.py", line 55, in mat_reader_factory
    mjv, mnv = get_matfile_version(byte_stream)
  File "/usr/lib/python2.7/dist-packages/scipy/io/matlab/miobase.py", line 217, in get_matfile_version
    buffer = fileobj.read(4))
KeyboardInterrupt
]0;kevin@kevin-desktop: ~/catkin_ws/src/romans_stack/model_net/heavykevin@kevin-desktop:~/catkin_ws/src/romans_stack/model_net/heavy$ ./solve.pycreate_net_heavy.py
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Net<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Blob<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Solver<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
]0;kevin@kevin-desktop: ~/catkin_ws/src/romans_stack/model_net/heavykevin@kevin-desktop:~/catkin_ws/src/romans_stack/model_net/heavy$ ./create_net_heavy.py[11Psolve.py
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Net<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Blob<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Solver<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1108 12:00:43.182003 10107 solver.cpp:48] Initializing solver from parameters: 
train_net: "train.prototxt"
test_net: "test.prototxt"
test_iter: 0
test_interval: 9999999
base_lr: 0.01
display: 100
max_iter: 450000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 5000
snapshot: 1000
snapshot_prefix: "/home/kevin/snapshot/snapshot"
solver_mode: GPU
I1108 12:00:43.182170 10107 solver.cpp:81] Creating training net from train_net file: train.prototxt
I1108 12:00:43.182584 10107 net.cpp:49] Initializing net from parameters: 
state {
  phase: TRAIN
}
layer {
  name: "img"
  type: "Python"
  top: "img"
  top: "label"
  python_param {
    module: "data_layers.model_net_layer"
    layer: "ModelNetDataLayer"
    param_str: "{\'img_size\': (250, 250), \'split\': \'train\', \'dataset_dir\': \'/home/kevin/dataset/processed_data\', \'variable\': \'depth_map\', \'dtype\': \'frame\', \'seed\': 1337, \'batch_size\': 128, \'mean\': 2}"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "img"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    name: "conv3_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv3_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  param {
    name: "conv4_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv4_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    name: "conv5_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv5_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 40
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1108 12:00:43.183308 10107 layer_factory.hpp:77] Creating layer img
I1108 12:00:43.204823 10107 net.cpp:91] Creating Layer img
I1108 12:00:43.204849 10107 net.cpp:399] img -> img
I1108 12:00:43.204864 10107 net.cpp:399] img -> label
{'img_size': (250, 250), 'split': 'train', 'dataset_dir': '/home/kevin/dataset/processed_data', 'variable': 'depth_map', 'dtype': 'frame', 'seed': 1337, 'batch_size': 128, 'mean': 2}
I1108 12:00:54.160889 10107 net.cpp:141] Setting up img
I1108 12:00:54.160934 10107 net.cpp:148] Top shape: 128 1 250 250 (8000000)
I1108 12:00:54.160958 10107 net.cpp:148] Top shape: 128 1 (128)
I1108 12:00:54.160964 10107 net.cpp:156] Memory required for data: 32000512
I1108 12:00:54.160993 10107 layer_factory.hpp:77] Creating layer label_img_1_split
I1108 12:00:54.161011 10107 net.cpp:91] Creating Layer label_img_1_split
I1108 12:00:54.161017 10107 net.cpp:425] label_img_1_split <- label
I1108 12:00:54.161033 10107 net.cpp:399] label_img_1_split -> label_img_1_split_0
I1108 12:00:54.161052 10107 net.cpp:399] label_img_1_split -> label_img_1_split_1
I1108 12:00:54.161113 10107 net.cpp:141] Setting up label_img_1_split
I1108 12:00:54.161129 10107 net.cpp:148] Top shape: 128 1 (128)
I1108 12:00:54.161134 10107 net.cpp:148] Top shape: 128 1 (128)
I1108 12:00:54.161147 10107 net.cpp:156] Memory required for data: 32001536
I1108 12:00:54.161151 10107 layer_factory.hpp:77] Creating layer conv1
I1108 12:00:54.161176 10107 net.cpp:91] Creating Layer conv1
I1108 12:00:54.161181 10107 net.cpp:425] conv1 <- img
I1108 12:00:54.161188 10107 net.cpp:399] conv1 -> conv1
I1108 12:00:54.162164 10107 net.cpp:141] Setting up conv1
I1108 12:00:54.162189 10107 net.cpp:148] Top shape: 128 128 125 125 (256000000)
I1108 12:00:54.162194 10107 net.cpp:156] Memory required for data: 1056001536
I1108 12:00:54.162204 10107 layer_factory.hpp:77] Creating layer relu1
I1108 12:00:54.162214 10107 net.cpp:91] Creating Layer relu1
I1108 12:00:54.162219 10107 net.cpp:425] relu1 <- conv1
I1108 12:00:54.162225 10107 net.cpp:386] relu1 -> conv1 (in-place)
I1108 12:00:54.162232 10107 net.cpp:141] Setting up relu1
I1108 12:00:54.162238 10107 net.cpp:148] Top shape: 128 128 125 125 (256000000)
I1108 12:00:54.162242 10107 net.cpp:156] Memory required for data: 2080001536
I1108 12:00:54.162247 10107 layer_factory.hpp:77] Creating layer pool1
I1108 12:00:54.162263 10107 net.cpp:91] Creating Layer pool1
I1108 12:00:54.162268 10107 net.cpp:425] pool1 <- conv1
I1108 12:00:54.162273 10107 net.cpp:399] pool1 -> pool1
I1108 12:00:54.162314 10107 net.cpp:141] Setting up pool1
I1108 12:00:54.162330 10107 net.cpp:148] Top shape: 128 128 62 62 (62980096)
I1108 12:00:54.162345 10107 net.cpp:156] Memory required for data: 2331921920
I1108 12:00:54.162350 10107 layer_factory.hpp:77] Creating layer norm1
I1108 12:00:54.162358 10107 net.cpp:91] Creating Layer norm1
I1108 12:00:54.162362 10107 net.cpp:425] norm1 <- pool1
I1108 12:00:54.162367 10107 net.cpp:399] norm1 -> norm1
I1108 12:00:54.162411 10107 net.cpp:141] Setting up norm1
I1108 12:00:54.162417 10107 net.cpp:148] Top shape: 128 128 62 62 (62980096)
I1108 12:00:54.162422 10107 net.cpp:156] Memory required for data: 2583842304
I1108 12:00:54.162427 10107 layer_factory.hpp:77] Creating layer conv2
I1108 12:00:54.162436 10107 net.cpp:91] Creating Layer conv2
I1108 12:00:54.162441 10107 net.cpp:425] conv2 <- norm1
I1108 12:00:54.162446 10107 net.cpp:399] conv2 -> conv2
I1108 12:00:54.166590 10107 net.cpp:141] Setting up conv2
I1108 12:00:54.166615 10107 net.cpp:148] Top shape: 128 256 62 62 (125960192)
I1108 12:00:54.166620 10107 net.cpp:156] Memory required for data: 3087683072
I1108 12:00:54.166640 10107 layer_factory.hpp:77] Creating layer relu2
I1108 12:00:54.166646 10107 net.cpp:91] Creating Layer relu2
I1108 12:00:54.166661 10107 net.cpp:425] relu2 <- conv2
I1108 12:00:54.166676 10107 net.cpp:386] relu2 -> conv2 (in-place)
I1108 12:00:54.166682 10107 net.cpp:141] Setting up relu2
I1108 12:00:54.166688 10107 net.cpp:148] Top shape: 128 256 62 62 (125960192)
I1108 12:00:54.166692 10107 net.cpp:156] Memory required for data: 3591523840
I1108 12:00:54.166707 10107 layer_factory.hpp:77] Creating layer pool2
I1108 12:00:54.166723 10107 net.cpp:91] Creating Layer pool2
I1108 12:00:54.166728 10107 net.cpp:425] pool2 <- conv2
I1108 12:00:54.166748 10107 net.cpp:399] pool2 -> pool2
I1108 12:00:54.166795 10107 net.cpp:141] Setting up pool2
I1108 12:00:54.166801 10107 net.cpp:148] Top shape: 128 256 31 31 (31490048)
I1108 12:00:54.166817 10107 net.cpp:156] Memory required for data: 3717484032
I1108 12:00:54.166822 10107 layer_factory.hpp:77] Creating layer norm2
I1108 12:00:54.166839 10107 net.cpp:91] Creating Layer norm2
I1108 12:00:54.166856 10107 net.cpp:425] norm2 <- pool2
I1108 12:00:54.166862 10107 net.cpp:399] norm2 -> norm2
I1108 12:00:54.166901 10107 net.cpp:141] Setting up norm2
I1108 12:00:54.166908 10107 net.cpp:148] Top shape: 128 256 31 31 (31490048)
I1108 12:00:54.166913 10107 net.cpp:156] Memory required for data: 3843444224
I1108 12:00:54.166916 10107 layer_factory.hpp:77] Creating layer conv3
I1108 12:00:54.166925 10107 net.cpp:91] Creating Layer conv3
I1108 12:00:54.166930 10107 net.cpp:425] conv3 <- norm2
I1108 12:00:54.166946 10107 net.cpp:399] conv3 -> conv3
I1108 12:00:54.168814 10107 net.cpp:141] Setting up conv3
I1108 12:00:54.168836 10107 net.cpp:148] Top shape: 128 256 31 31 (31490048)
I1108 12:00:54.168843 10107 net.cpp:156] Memory required for data: 3969404416
I1108 12:00:54.168859 10107 layer_factory.hpp:77] Creating layer relu3
I1108 12:00:54.168869 10107 net.cpp:91] Creating Layer relu3
I1108 12:00:54.168874 10107 net.cpp:425] relu3 <- conv3
I1108 12:00:54.168879 10107 net.cpp:386] relu3 -> conv3 (in-place)
I1108 12:00:54.168886 10107 net.cpp:141] Setting up relu3
I1108 12:00:54.168892 10107 net.cpp:148] Top shape: 128 256 31 31 (31490048)
I1108 12:00:54.168896 10107 net.cpp:156] Memory required for data: 4095364608
I1108 12:00:54.168901 10107 layer_factory.hpp:77] Creating layer pool3
I1108 12:00:54.168908 10107 net.cpp:91] Creating Layer pool3
I1108 12:00:54.168913 10107 net.cpp:425] pool3 <- conv3
I1108 12:00:54.168918 10107 net.cpp:399] pool3 -> pool3
I1108 12:00:54.168943 10107 net.cpp:141] Setting up pool3
I1108 12:00:54.168951 10107 net.cpp:148] Top shape: 128 256 15 15 (7372800)
I1108 12:00:54.168954 10107 net.cpp:156] Memory required for data: 4124855808
I1108 12:00:54.168958 10107 layer_factory.hpp:77] Creating layer conv4
I1108 12:00:54.168967 10107 net.cpp:91] Creating Layer conv4
I1108 12:00:54.168972 10107 net.cpp:425] conv4 <- pool3
I1108 12:00:54.168977 10107 net.cpp:399] conv4 -> conv4
I1108 12:00:54.172019 10107 net.cpp:141] Setting up conv4
I1108 12:00:54.172042 10107 net.cpp:148] Top shape: 128 512 15 15 (14745600)
I1108 12:00:54.172046 10107 net.cpp:156] Memory required for data: 4183838208
I1108 12:00:54.172065 10107 layer_factory.hpp:77] Creating layer relu4
I1108 12:00:54.172072 10107 net.cpp:91] Creating Layer relu4
I1108 12:00:54.172078 10107 net.cpp:425] relu4 <- conv4
I1108 12:00:54.172086 10107 net.cpp:386] relu4 -> conv4 (in-place)
I1108 12:00:54.172092 10107 net.cpp:141] Setting up relu4
I1108 12:00:54.172098 10107 net.cpp:148] Top shape: 128 512 15 15 (14745600)
I1108 12:00:54.172102 10107 net.cpp:156] Memory required for data: 4242820608
I1108 12:00:54.172107 10107 layer_factory.hpp:77] Creating layer conv5
I1108 12:00:54.172116 10107 net.cpp:91] Creating Layer conv5
I1108 12:00:54.172121 10107 net.cpp:425] conv5 <- conv4
I1108 12:00:54.172127 10107 net.cpp:399] conv5 -> conv5
I1108 12:00:54.178102 10107 net.cpp:141] Setting up conv5
I1108 12:00:54.178127 10107 net.cpp:148] Top shape: 128 512 15 15 (14745600)
I1108 12:00:54.178131 10107 net.cpp:156] Memory required for data: 4301803008
I1108 12:00:54.178143 10107 layer_factory.hpp:77] Creating layer relu5
I1108 12:00:54.178153 10107 net.cpp:91] Creating Layer relu5
I1108 12:00:54.178159 10107 net.cpp:425] relu5 <- conv5
I1108 12:00:54.178176 10107 net.cpp:386] relu5 -> conv5 (in-place)
I1108 12:00:54.178195 10107 net.cpp:141] Setting up relu5
I1108 12:00:54.178210 10107 net.cpp:148] Top shape: 128 512 15 15 (14745600)
I1108 12:00:54.178215 10107 net.cpp:156] Memory required for data: 4360785408
I1108 12:00:54.178220 10107 layer_factory.hpp:77] Creating layer pool5
I1108 12:00:54.178236 10107 net.cpp:91] Creating Layer pool5
I1108 12:00:54.178239 10107 net.cpp:425] pool5 <- conv5
I1108 12:00:54.178254 10107 net.cpp:399] pool5 -> pool5
I1108 12:00:54.178313 10107 net.cpp:141] Setting up pool5
I1108 12:00:54.178319 10107 net.cpp:148] Top shape: 128 512 7 7 (3211264)
I1108 12:00:54.178333 10107 net.cpp:156] Memory required for data: 4373630464
I1108 12:00:54.178338 10107 layer_factory.hpp:77] Creating layer fc6
I1108 12:00:54.178355 10107 net.cpp:91] Creating Layer fc6
I1108 12:00:54.178360 10107 net.cpp:425] fc6 <- pool5
I1108 12:00:54.178377 10107 net.cpp:399] fc6 -> fc6
I1108 12:00:54.691803 10107 net.cpp:141] Setting up fc6
I1108 12:00:54.691835 10107 net.cpp:148] Top shape: 128 4096 (524288)
I1108 12:00:54.691841 10107 net.cpp:156] Memory required for data: 4375727616
I1108 12:00:54.691853 10107 layer_factory.hpp:77] Creating layer relu6
I1108 12:00:54.691874 10107 net.cpp:91] Creating Layer relu6
I1108 12:00:54.691884 10107 net.cpp:425] relu6 <- fc6
I1108 12:00:54.691901 10107 net.cpp:386] relu6 -> fc6 (in-place)
I1108 12:00:54.691920 10107 net.cpp:141] Setting up relu6
I1108 12:00:54.691941 10107 net.cpp:148] Top shape: 128 4096 (524288)
I1108 12:00:54.691944 10107 net.cpp:156] Memory required for data: 4377824768
I1108 12:00:54.691959 10107 layer_factory.hpp:77] Creating layer drop6
I1108 12:00:54.691984 10107 net.cpp:91] Creating Layer drop6
I1108 12:00:54.691999 10107 net.cpp:425] drop6 <- fc6
I1108 12:00:54.692014 10107 net.cpp:386] drop6 -> fc6 (in-place)
I1108 12:00:54.692034 10107 net.cpp:141] Setting up drop6
I1108 12:00:54.692050 10107 net.cpp:148] Top shape: 128 4096 (524288)
I1108 12:00:54.692064 10107 net.cpp:156] Memory required for data: 4379921920
I1108 12:00:54.692067 10107 layer_factory.hpp:77] Creating layer fc7
I1108 12:00:54.692085 10107 net.cpp:91] Creating Layer fc7
I1108 12:00:54.692090 10107 net.cpp:425] fc7 <- fc6
I1108 12:00:54.692108 10107 net.cpp:399] fc7 -> fc7
I1108 12:00:54.791052 10107 net.cpp:141] Setting up fc7
I1108 12:00:54.791085 10107 net.cpp:148] Top shape: 128 4096 (524288)
I1108 12:00:54.791092 10107 net.cpp:156] Memory required for data: 4382019072
I1108 12:00:54.791103 10107 layer_factory.hpp:77] Creating layer relu7
I1108 12:00:54.791115 10107 net.cpp:91] Creating Layer relu7
I1108 12:00:54.791133 10107 net.cpp:425] relu7 <- fc7
I1108 12:00:54.791141 10107 net.cpp:386] relu7 -> fc7 (in-place)
I1108 12:00:54.791162 10107 net.cpp:141] Setting up relu7
I1108 12:00:54.791177 10107 net.cpp:148] Top shape: 128 4096 (524288)
I1108 12:00:54.791182 10107 net.cpp:156] Memory required for data: 4384116224
I1108 12:00:54.791198 10107 layer_factory.hpp:77] Creating layer drop7
I1108 12:00:54.791205 10107 net.cpp:91] Creating Layer drop7
I1108 12:00:54.791210 10107 net.cpp:425] drop7 <- fc7
I1108 12:00:54.791218 10107 net.cpp:386] drop7 -> fc7 (in-place)
I1108 12:00:54.791236 10107 net.cpp:141] Setting up drop7
I1108 12:00:54.791251 10107 net.cpp:148] Top shape: 128 4096 (524288)
I1108 12:00:54.791255 10107 net.cpp:156] Memory required for data: 4386213376
I1108 12:00:54.791270 10107 layer_factory.hpp:77] Creating layer fc8
I1108 12:00:54.791278 10107 net.cpp:91] Creating Layer fc8
I1108 12:00:54.791285 10107 net.cpp:425] fc8 <- fc7
I1108 12:00:54.791290 10107 net.cpp:399] fc8 -> fc8
I1108 12:00:54.792480 10107 net.cpp:141] Setting up fc8
I1108 12:00:54.792495 10107 net.cpp:148] Top shape: 128 40 (5120)
I1108 12:00:54.792500 10107 net.cpp:156] Memory required for data: 4386233856
I1108 12:00:54.792505 10107 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I1108 12:00:54.792526 10107 net.cpp:91] Creating Layer fc8_fc8_0_split
I1108 12:00:54.792531 10107 net.cpp:425] fc8_fc8_0_split <- fc8
I1108 12:00:54.792538 10107 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1108 12:00:54.792546 10107 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1108 12:00:54.792567 10107 net.cpp:141] Setting up fc8_fc8_0_split
I1108 12:00:54.792583 10107 net.cpp:148] Top shape: 128 40 (5120)
I1108 12:00:54.792588 10107 net.cpp:148] Top shape: 128 40 (5120)
I1108 12:00:54.792601 10107 net.cpp:156] Memory required for data: 4386274816
I1108 12:00:54.792605 10107 layer_factory.hpp:77] Creating layer accuracy
I1108 12:00:54.792613 10107 net.cpp:91] Creating Layer accuracy
I1108 12:00:54.792618 10107 net.cpp:425] accuracy <- fc8_fc8_0_split_0
I1108 12:00:54.792623 10107 net.cpp:425] accuracy <- label_img_1_split_0
I1108 12:00:54.792630 10107 net.cpp:399] accuracy -> accuracy
I1108 12:00:54.792637 10107 net.cpp:141] Setting up accuracy
I1108 12:00:54.792644 10107 net.cpp:148] Top shape: (1)
I1108 12:00:54.792647 10107 net.cpp:156] Memory required for data: 4386274820
I1108 12:00:54.792652 10107 layer_factory.hpp:77] Creating layer loss
I1108 12:00:54.792665 10107 net.cpp:91] Creating Layer loss
I1108 12:00:54.792670 10107 net.cpp:425] loss <- fc8_fc8_0_split_1
I1108 12:00:54.792675 10107 net.cpp:425] loss <- label_img_1_split_1
I1108 12:00:54.792680 10107 net.cpp:399] loss -> loss
I1108 12:00:54.792688 10107 layer_factory.hpp:77] Creating layer loss
I1108 12:00:54.792763 10107 net.cpp:141] Setting up loss
I1108 12:00:54.792769 10107 net.cpp:148] Top shape: (1)
I1108 12:00:54.792774 10107 net.cpp:151]     with loss weight 1
I1108 12:00:54.792794 10107 net.cpp:156] Memory required for data: 4386274824
I1108 12:00:54.792799 10107 net.cpp:217] loss needs backward computation.
I1108 12:00:54.792804 10107 net.cpp:219] accuracy does not need backward computation.
I1108 12:00:54.792809 10107 net.cpp:217] fc8_fc8_0_split needs backward computation.
I1108 12:00:54.792814 10107 net.cpp:217] fc8 needs backward computation.
I1108 12:00:54.792819 10107 net.cpp:217] drop7 needs backward computation.
I1108 12:00:54.792824 10107 net.cpp:217] relu7 needs backward computation.
I1108 12:00:54.792839 10107 net.cpp:217] fc7 needs backward computation.
I1108 12:00:54.792843 10107 net.cpp:217] drop6 needs backward computation.
I1108 12:00:54.792848 10107 net.cpp:217] relu6 needs backward computation.
I1108 12:00:54.792862 10107 net.cpp:217] fc6 needs backward computation.
I1108 12:00:54.792866 10107 net.cpp:217] pool5 needs backward computation.
I1108 12:00:54.792872 10107 net.cpp:217] relu5 needs backward computation.
I1108 12:00:54.792878 10107 net.cpp:217] conv5 needs backward computation.
I1108 12:00:54.792882 10107 net.cpp:217] relu4 needs backward computation.
I1108 12:00:54.792887 10107 net.cpp:217] conv4 needs backward computation.
I1108 12:00:54.792892 10107 net.cpp:217] pool3 needs backward computation.
I1108 12:00:54.792898 10107 net.cpp:217] relu3 needs backward computation.
I1108 12:00:54.792902 10107 net.cpp:217] conv3 needs backward computation.
I1108 12:00:54.792908 10107 net.cpp:217] norm2 needs backward computation.
I1108 12:00:54.792913 10107 net.cpp:217] pool2 needs backward computation.
I1108 12:00:54.792918 10107 net.cpp:217] relu2 needs backward computation.
I1108 12:00:54.792922 10107 net.cpp:217] conv2 needs backward computation.
I1108 12:00:54.792927 10107 net.cpp:217] norm1 needs backward computation.
I1108 12:00:54.792932 10107 net.cpp:217] pool1 needs backward computation.
I1108 12:00:54.792937 10107 net.cpp:217] relu1 needs backward computation.
I1108 12:00:54.792943 10107 net.cpp:217] conv1 needs backward computation.
I1108 12:00:54.792948 10107 net.cpp:219] label_img_1_split does not need backward computation.
I1108 12:00:54.792953 10107 net.cpp:219] img does not need backward computation.
I1108 12:00:54.792958 10107 net.cpp:261] This network produces output accuracy
I1108 12:00:54.792963 10107 net.cpp:261] This network produces output loss
I1108 12:00:54.792975 10107 net.cpp:274] Network initialization done.
I1108 12:00:54.793341 10107 solver.cpp:181] Creating test net (#0) specified by test_net file: test.prototxt
I1108 12:00:54.793467 10107 net.cpp:49] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "img"
  type: "Python"
  top: "img"
  top: "label"
  python_param {
    module: "data_layers.model_net_layer"
    layer: "ModelNetDataLayer"
    param_str: "{\'img_size\': (250, 250), \'split\': \'test\', \'dataset_dir\': \'/home/kevin/dataset/processed_data\', \'variable\': \'depth_map\', \'dtype\': \'object\', \'seed\': 1337, \'batch_size\': 128, \'mean\': 2}"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "img"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    name: "conv3_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv3_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  param {
    name: "conv4_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv4_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    name: "conv5_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv5_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 40
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1108 12:00:54.794237 10107 layer_factory.hpp:77] Creating layer img
I1108 12:00:54.794278 10107 net.cpp:91] Creating Layer img
I1108 12:00:54.794286 10107 net.cpp:399] img -> img
I1108 12:00:54.794293 10107 net.cpp:399] img -> label
{'img_size': (250, 250), 'split': 'test', 'dataset_dir': '/home/kevin/dataset/processed_data', 'variable': 'depth_map', 'dtype': 'object', 'seed': 1337, 'batch_size': 128, 'mean': 2}
I1108 12:00:54.941984 10107 net.cpp:141] Setting up img
I1108 12:00:54.942015 10107 net.cpp:148] Top shape: 24 1 250 250 (1500000)
I1108 12:00:54.942023 10107 net.cpp:148] Top shape: 24 1 (24)
I1108 12:00:54.942026 10107 net.cpp:156] Memory required for data: 6000096
I1108 12:00:54.942047 10107 layer_factory.hpp:77] Creating layer label_img_1_split
I1108 12:00:54.942070 10107 net.cpp:91] Creating Layer label_img_1_split
I1108 12:00:54.942085 10107 net.cpp:425] label_img_1_split <- label
I1108 12:00:54.942092 10107 net.cpp:399] label_img_1_split -> label_img_1_split_0
I1108 12:00:54.942111 10107 net.cpp:399] label_img_1_split -> label_img_1_split_1
I1108 12:00:54.942188 10107 net.cpp:141] Setting up label_img_1_split
I1108 12:00:54.942206 10107 net.cpp:148] Top shape: 24 1 (24)
I1108 12:00:54.942211 10107 net.cpp:148] Top shape: 24 1 (24)
I1108 12:00:54.942216 10107 net.cpp:156] Memory required for data: 6000288
I1108 12:00:54.942232 10107 layer_factory.hpp:77] Creating layer conv1
I1108 12:00:54.942255 10107 net.cpp:91] Creating Layer conv1
I1108 12:00:54.942272 10107 net.cpp:425] conv1 <- img
I1108 12:00:54.942287 10107 net.cpp:399] conv1 -> conv1
I1108 12:00:54.942497 10107 net.cpp:141] Setting up conv1
I1108 12:00:54.942505 10107 net.cpp:148] Top shape: 24 128 125 125 (48000000)
I1108 12:00:54.942509 10107 net.cpp:156] Memory required for data: 198000288
I1108 12:00:54.942517 10107 layer_factory.hpp:77] Creating layer relu1
I1108 12:00:54.942534 10107 net.cpp:91] Creating Layer relu1
I1108 12:00:54.942539 10107 net.cpp:425] relu1 <- conv1
I1108 12:00:54.942553 10107 net.cpp:386] relu1 -> conv1 (in-place)
I1108 12:00:54.942560 10107 net.cpp:141] Setting up relu1
I1108 12:00:54.942574 10107 net.cpp:148] Top shape: 24 128 125 125 (48000000)
I1108 12:00:54.942579 10107 net.cpp:156] Memory required for data: 390000288
I1108 12:00:54.942594 10107 layer_factory.hpp:77] Creating layer pool1
I1108 12:00:54.942601 10107 net.cpp:91] Creating Layer pool1
I1108 12:00:54.942605 10107 net.cpp:425] pool1 <- conv1
I1108 12:00:54.942610 10107 net.cpp:399] pool1 -> pool1
I1108 12:00:54.942646 10107 net.cpp:141] Setting up pool1
I1108 12:00:54.942661 10107 net.cpp:148] Top shape: 24 128 62 62 (11808768)
I1108 12:00:54.942664 10107 net.cpp:156] Memory required for data: 437235360
I1108 12:00:54.942678 10107 layer_factory.hpp:77] Creating layer norm1
I1108 12:00:54.942684 10107 net.cpp:91] Creating Layer norm1
I1108 12:00:54.942689 10107 net.cpp:425] norm1 <- pool1
I1108 12:00:54.942694 10107 net.cpp:399] norm1 -> norm1
I1108 12:00:54.942714 10107 net.cpp:141] Setting up norm1
I1108 12:00:54.942720 10107 net.cpp:148] Top shape: 24 128 62 62 (11808768)
I1108 12:00:54.942725 10107 net.cpp:156] Memory required for data: 484470432
I1108 12:00:54.942729 10107 layer_factory.hpp:77] Creating layer conv2
I1108 12:00:54.942749 10107 net.cpp:91] Creating Layer conv2
I1108 12:00:54.942752 10107 net.cpp:425] conv2 <- norm1
I1108 12:00:54.942759 10107 net.cpp:399] conv2 -> conv2
I1108 12:00:54.947141 10107 net.cpp:141] Setting up conv2
I1108 12:00:54.947157 10107 net.cpp:148] Top shape: 24 256 62 62 (23617536)
I1108 12:00:54.947162 10107 net.cpp:156] Memory required for data: 578940576
I1108 12:00:54.947170 10107 layer_factory.hpp:77] Creating layer relu2
I1108 12:00:54.947188 10107 net.cpp:91] Creating Layer relu2
I1108 12:00:54.947194 10107 net.cpp:425] relu2 <- conv2
I1108 12:00:54.947209 10107 net.cpp:386] relu2 -> conv2 (in-place)
I1108 12:00:54.947216 10107 net.cpp:141] Setting up relu2
I1108 12:00:54.947222 10107 net.cpp:148] Top shape: 24 256 62 62 (23617536)
I1108 12:00:54.947227 10107 net.cpp:156] Memory required for data: 673410720
I1108 12:00:54.947232 10107 layer_factory.hpp:77] Creating layer pool2
I1108 12:00:54.947238 10107 net.cpp:91] Creating Layer pool2
I1108 12:00:54.947243 10107 net.cpp:425] pool2 <- conv2
I1108 12:00:54.947249 10107 net.cpp:399] pool2 -> pool2
I1108 12:00:54.947304 10107 net.cpp:141] Setting up pool2
I1108 12:00:54.947311 10107 net.cpp:148] Top shape: 24 256 31 31 (5904384)
I1108 12:00:54.947325 10107 net.cpp:156] Memory required for data: 697028256
I1108 12:00:54.947329 10107 layer_factory.hpp:77] Creating layer norm2
I1108 12:00:54.947345 10107 net.cpp:91] Creating Layer norm2
I1108 12:00:54.947350 10107 net.cpp:425] norm2 <- pool2
I1108 12:00:54.947355 10107 net.cpp:399] norm2 -> norm2
I1108 12:00:54.947376 10107 net.cpp:141] Setting up norm2
I1108 12:00:54.947391 10107 net.cpp:148] Top shape: 24 256 31 31 (5904384)
I1108 12:00:54.947396 10107 net.cpp:156] Memory required for data: 720645792
I1108 12:00:54.947408 10107 layer_factory.hpp:77] Creating layer conv3
I1108 12:00:54.947417 10107 net.cpp:91] Creating Layer conv3
I1108 12:00:54.947423 10107 net.cpp:425] conv3 <- norm2
I1108 12:00:54.947428 10107 net.cpp:399] conv3 -> conv3
I1108 12:00:54.949199 10107 net.cpp:141] Setting up conv3
I1108 12:00:54.949214 10107 net.cpp:148] Top shape: 24 256 31 31 (5904384)
I1108 12:00:54.949229 10107 net.cpp:156] Memory required for data: 744263328
I1108 12:00:54.949237 10107 layer_factory.hpp:77] Creating layer relu3
I1108 12:00:54.949244 10107 net.cpp:91] Creating Layer relu3
I1108 12:00:54.949249 10107 net.cpp:425] relu3 <- conv3
I1108 12:00:54.949255 10107 net.cpp:386] relu3 -> conv3 (in-place)
I1108 12:00:54.949262 10107 net.cpp:141] Setting up relu3
I1108 12:00:54.949268 10107 net.cpp:148] Top shape: 24 256 31 31 (5904384)
I1108 12:00:54.949272 10107 net.cpp:156] Memory required for data: 767880864
I1108 12:00:54.949277 10107 layer_factory.hpp:77] Creating layer pool3
I1108 12:00:54.949283 10107 net.cpp:91] Creating Layer pool3
I1108 12:00:54.949288 10107 net.cpp:425] pool3 <- conv3
I1108 12:00:54.949295 10107 net.cpp:399] pool3 -> pool3
I1108 12:00:54.949318 10107 net.cpp:141] Setting up pool3
I1108 12:00:54.949324 10107 net.cpp:148] Top shape: 24 256 15 15 (1382400)
I1108 12:00:54.949329 10107 net.cpp:156] Memory required for data: 773410464
I1108 12:00:54.949334 10107 layer_factory.hpp:77] Creating layer conv4
I1108 12:00:54.949342 10107 net.cpp:91] Creating Layer conv4
I1108 12:00:54.949347 10107 net.cpp:425] conv4 <- pool3
I1108 12:00:54.949354 10107 net.cpp:399] conv4 -> conv4
I1108 12:00:54.953446 10107 net.cpp:141] Setting up conv4
I1108 12:00:54.953474 10107 net.cpp:148] Top shape: 24 512 15 15 (2764800)
I1108 12:00:54.953480 10107 net.cpp:156] Memory required for data: 784469664
I1108 12:00:54.953503 10107 layer_factory.hpp:77] Creating layer relu4
I1108 12:00:54.953523 10107 net.cpp:91] Creating Layer relu4
I1108 12:00:54.953541 10107 net.cpp:425] relu4 <- conv4
I1108 12:00:54.953559 10107 net.cpp:386] relu4 -> conv4 (in-place)
I1108 12:00:54.953579 10107 net.cpp:141] Setting up relu4
I1108 12:00:54.953590 10107 net.cpp:148] Top shape: 24 512 15 15 (2764800)
I1108 12:00:54.953608 10107 net.cpp:156] Memory required for data: 795528864
I1108 12:00:54.953615 10107 layer_factory.hpp:77] Creating layer conv5
I1108 12:00:54.953629 10107 net.cpp:91] Creating Layer conv5
I1108 12:00:54.953635 10107 net.cpp:425] conv5 <- conv4
I1108 12:00:54.953645 10107 net.cpp:399] conv5 -> conv5
I1108 12:00:54.961021 10107 net.cpp:141] Setting up conv5
I1108 12:00:54.961062 10107 net.cpp:148] Top shape: 24 512 15 15 (2764800)
I1108 12:00:54.961068 10107 net.cpp:156] Memory required for data: 806588064
I1108 12:00:54.961093 10107 layer_factory.hpp:77] Creating layer relu5
I1108 12:00:54.961127 10107 net.cpp:91] Creating Layer relu5
I1108 12:00:54.961133 10107 net.cpp:425] relu5 <- conv5
I1108 12:00:54.961153 10107 net.cpp:386] relu5 -> conv5 (in-place)
I1108 12:00:54.961179 10107 net.cpp:141] Setting up relu5
I1108 12:00:54.961184 10107 net.cpp:148] Top shape: 24 512 15 15 (2764800)
I1108 12:00:54.961191 10107 net.cpp:156] Memory required for data: 817647264
I1108 12:00:54.961196 10107 layer_factory.hpp:77] Creating layer pool5
I1108 12:00:54.961205 10107 net.cpp:91] Creating Layer pool5
I1108 12:00:54.961210 10107 net.cpp:425] pool5 <- conv5
I1108 12:00:54.961217 10107 net.cpp:399] pool5 -> pool5
I1108 12:00:54.961278 10107 net.cpp:141] Setting up pool5
I1108 12:00:54.961294 10107 net.cpp:148] Top shape: 24 512 7 7 (602112)
I1108 12:00:54.961308 10107 net.cpp:156] Memory required for data: 820055712
I1108 12:00:54.961313 10107 layer_factory.hpp:77] Creating layer fc6
I1108 12:00:54.961320 10107 net.cpp:91] Creating Layer fc6
I1108 12:00:54.961325 10107 net.cpp:425] fc6 <- pool5
I1108 12:00:54.961331 10107 net.cpp:399] fc6 -> fc6
I1108 12:00:55.541849 10107 net.cpp:141] Setting up fc6
I1108 12:00:55.541898 10107 net.cpp:148] Top shape: 24 4096 (98304)
I1108 12:00:55.541903 10107 net.cpp:156] Memory required for data: 820448928
I1108 12:00:55.541927 10107 layer_factory.hpp:77] Creating layer relu6
I1108 12:00:55.541939 10107 net.cpp:91] Creating Layer relu6
I1108 12:00:55.541946 10107 net.cpp:425] relu6 <- fc6
I1108 12:00:55.541954 10107 net.cpp:386] relu6 -> fc6 (in-place)
I1108 12:00:55.541966 10107 net.cpp:141] Setting up relu6
I1108 12:00:55.541981 10107 net.cpp:148] Top shape: 24 4096 (98304)
I1108 12:00:55.541985 10107 net.cpp:156] Memory required for data: 820842144
I1108 12:00:55.541999 10107 layer_factory.hpp:77] Creating layer drop6
I1108 12:00:55.542009 10107 net.cpp:91] Creating Layer drop6
I1108 12:00:55.542014 10107 net.cpp:425] drop6 <- fc6
I1108 12:00:55.542021 10107 net.cpp:386] drop6 -> fc6 (in-place)
I1108 12:00:55.542047 10107 net.cpp:141] Setting up drop6
I1108 12:00:55.542053 10107 net.cpp:148] Top shape: 24 4096 (98304)
I1108 12:00:55.542057 10107 net.cpp:156] Memory required for data: 821235360
I1108 12:00:55.542062 10107 layer_factory.hpp:77] Creating layer fc7
I1108 12:00:55.542080 10107 net.cpp:91] Creating Layer fc7
I1108 12:00:55.542096 10107 net.cpp:425] fc7 <- fc6
I1108 12:00:55.542104 10107 net.cpp:399] fc7 -> fc7
I1108 12:00:55.628345 10107 net.cpp:141] Setting up fc7
I1108 12:00:55.628386 10107 net.cpp:148] Top shape: 24 4096 (98304)
I1108 12:00:55.628391 10107 net.cpp:156] Memory required for data: 821628576
I1108 12:00:55.628401 10107 layer_factory.hpp:77] Creating layer relu7
I1108 12:00:55.628423 10107 net.cpp:91] Creating Layer relu7
I1108 12:00:55.628429 10107 net.cpp:425] relu7 <- fc7
I1108 12:00:55.628438 10107 net.cpp:386] relu7 -> fc7 (in-place)
I1108 12:00:55.628448 10107 net.cpp:141] Setting up relu7
I1108 12:00:55.628454 10107 net.cpp:148] Top shape: 24 4096 (98304)
I1108 12:00:55.628459 10107 net.cpp:156] Memory required for data: 822021792
I1108 12:00:55.628476 10107 layer_factory.hpp:77] Creating layer drop7
I1108 12:00:55.628484 10107 net.cpp:91] Creating Layer drop7
I1108 12:00:55.628487 10107 net.cpp:425] drop7 <- fc7
I1108 12:00:55.628492 10107 net.cpp:386] drop7 -> fc7 (in-place)
I1108 12:00:55.628521 10107 net.cpp:141] Setting up drop7
I1108 12:00:55.628540 10107 net.cpp:148] Top shape: 24 4096 (98304)
I1108 12:00:55.628543 10107 net.cpp:156] Memory required for data: 822415008
I1108 12:00:55.628547 10107 layer_factory.hpp:77] Creating layer fc8
I1108 12:00:55.628554 10107 net.cpp:91] Creating Layer fc8
I1108 12:00:55.628568 10107 net.cpp:425] fc8 <- fc7
I1108 12:00:55.628574 10107 net.cpp:399] fc8 -> fc8
I1108 12:00:55.630010 10107 net.cpp:141] Setting up fc8
I1108 12:00:55.630036 10107 net.cpp:148] Top shape: 24 40 (960)
I1108 12:00:55.630045 10107 net.cpp:156] Memory required for data: 822418848
I1108 12:00:55.630067 10107 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I1108 12:00:55.630087 10107 net.cpp:91] Creating Layer fc8_fc8_0_split
I1108 12:00:55.630095 10107 net.cpp:425] fc8_fc8_0_split <- fc8
I1108 12:00:55.630103 10107 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1108 12:00:55.630115 10107 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1108 12:00:55.630199 10107 net.cpp:141] Setting up fc8_fc8_0_split
I1108 12:00:55.630219 10107 net.cpp:148] Top shape: 24 40 (960)
I1108 12:00:55.630237 10107 net.cpp:148] Top shape: 24 40 (960)
I1108 12:00:55.630244 10107 net.cpp:156] Memory required for data: 822426528
I1108 12:00:55.630252 10107 layer_factory.hpp:77] Creating layer accuracy
I1108 12:00:55.630261 10107 net.cpp:91] Creating Layer accuracy
I1108 12:00:55.630270 10107 net.cpp:425] accuracy <- fc8_fc8_0_split_0
I1108 12:00:55.630278 10107 net.cpp:425] accuracy <- label_img_1_split_0
I1108 12:00:55.630287 10107 net.cpp:399] accuracy -> accuracy
I1108 12:00:55.630316 10107 net.cpp:141] Setting up accuracy
I1108 12:00:55.630342 10107 net.cpp:148] Top shape: (1)
I1108 12:00:55.630354 10107 net.cpp:156] Memory required for data: 822426532
I1108 12:00:55.630369 10107 layer_factory.hpp:77] Creating layer loss
I1108 12:00:55.630380 10107 net.cpp:91] Creating Layer loss
I1108 12:00:55.630388 10107 net.cpp:425] loss <- fc8_fc8_0_split_1
I1108 12:00:55.630396 10107 net.cpp:425] loss <- label_img_1_split_1
I1108 12:00:55.630404 10107 net.cpp:399] loss -> loss
I1108 12:00:55.630415 10107 layer_factory.hpp:77] Creating layer loss
I1108 12:00:55.630501 10107 net.cpp:141] Setting up loss
I1108 12:00:55.630511 10107 net.cpp:148] Top shape: (1)
I1108 12:00:55.630527 10107 net.cpp:151]     with loss weight 1
I1108 12:00:55.630542 10107 net.cpp:156] Memory required for data: 822426536
I1108 12:00:55.630548 10107 net.cpp:217] loss needs backward computation.
I1108 12:00:55.630556 10107 net.cpp:219] accuracy does not need backward computation.
I1108 12:00:55.630563 10107 net.cpp:217] fc8_fc8_0_split needs backward computation.
I1108 12:00:55.630571 10107 net.cpp:217] fc8 needs backward computation.
I1108 12:00:55.630578 10107 net.cpp:217] drop7 needs backward computation.
I1108 12:00:55.630585 10107 net.cpp:217] relu7 needs backward computation.
I1108 12:00:55.630592 10107 net.cpp:217] fc7 needs backward computation.
I1108 12:00:55.630599 10107 net.cpp:217] drop6 needs backward computation.
I1108 12:00:55.630606 10107 net.cpp:217] relu6 needs backward computation.
I1108 12:00:55.630614 10107 net.cpp:217] fc6 needs backward computation.
I1108 12:00:55.630621 10107 net.cpp:217] pool5 needs backward computation.
I1108 12:00:55.630628 10107 net.cpp:217] relu5 needs backward computation.
I1108 12:00:55.630635 10107 net.cpp:217] conv5 needs backward computation.
I1108 12:00:55.630652 10107 net.cpp:217] relu4 needs backward computation.
I1108 12:00:55.630659 10107 net.cpp:217] conv4 needs backward computation.
I1108 12:00:55.630667 10107 net.cpp:217] pool3 needs backward computation.
I1108 12:00:55.630674 10107 net.cpp:217] relu3 needs backward computation.
I1108 12:00:55.630681 10107 net.cpp:217] conv3 needs backward computation.
I1108 12:00:55.630688 10107 net.cpp:217] norm2 needs backward computation.
I1108 12:00:55.630695 10107 net.cpp:217] pool2 needs backward computation.
I1108 12:00:55.630703 10107 net.cpp:217] relu2 needs backward computation.
I1108 12:00:55.630710 10107 net.cpp:217] conv2 needs backward computation.
I1108 12:00:55.630717 10107 net.cpp:217] norm1 needs backward computation.
I1108 12:00:55.630724 10107 net.cpp:217] pool1 needs backward computation.
I1108 12:00:55.630731 10107 net.cpp:217] relu1 needs backward computation.
I1108 12:00:55.630738 10107 net.cpp:217] conv1 needs backward computation.
I1108 12:00:55.630745 10107 net.cpp:219] label_img_1_split does not need backward computation.
I1108 12:00:55.630753 10107 net.cpp:219] img does not need backward computation.
I1108 12:00:55.630760 10107 net.cpp:261] This network produces output accuracy
I1108 12:00:55.630767 10107 net.cpp:261] This network produces output loss
I1108 12:00:55.630785 10107 net.cpp:274] Network initialization done.
I1108 12:00:55.630872 10107 solver.cpp:60] Solver scaffolding done.
I1108 12:00:55.641667 10107 solver.cpp:337] Iteration 0, Testing net (#0)
I1108 12:00:57.711184 10107 solver.cpp:228] Iteration 0, loss = 3.70168
I1108 12:00:57.711226 10107 solver.cpp:244]     Train net output #0: accuracy = 0.0234375
I1108 12:00:57.711241 10107 solver.cpp:244]     Train net output #1: loss = 3.70168 (* 1 = 3.70168 loss)
I1108 12:00:57.711262 10107 sgd_solver.cpp:106] Iteration 0, lr = 0.01
^CTraceback (most recent call last):
  File "./solve.py", line 27, in <module>
    solver.step(1000)
  File "/home/kevin/caffeplus/python_layer/data_layers/model_net_layer.py", line 68, in reshape
    def reshape(self, bottom, top):
KeyboardInterrupt
]0;kevin@kevin-desktop: ~/catkin_ws/src/romans_stack/model_net/heavykevin@kevin-desktop:~/catkin_ws/src/romans_stack/model_net/heavy$ ./solve.pycreate_net_heavy.py
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Net<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Blob<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Solver<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
]0;kevin@kevin-desktop: ~/catkin_ws/src/romans_stack/model_net/heavykevin@kevin-desktop:~/catkin_ws/src/romans_stack/model_net/heavy$ ./create_net_heavy.py[11Psolve.py
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Net<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Blob<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Solver<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1108 12:02:15.848701 10127 solver.cpp:48] Initializing solver from parameters: 
train_net: "train.prototxt"
test_net: "test.prototxt"
test_iter: 0
test_interval: 9999999
base_lr: 0.01
display: 100
max_iter: 450000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 5000
snapshot: 1000
snapshot_prefix: "/home/kevin/snapshot/heavy"
solver_mode: GPU
I1108 12:02:15.848883 10127 solver.cpp:81] Creating training net from train_net file: train.prototxt
I1108 12:02:15.849328 10127 net.cpp:49] Initializing net from parameters: 
state {
  phase: TRAIN
}
layer {
  name: "img"
  type: "Python"
  top: "img"
  top: "label"
  python_param {
    module: "data_layers.model_net_layer"
    layer: "ModelNetDataLayer"
    param_str: "{\'img_size\': (250, 250), \'split\': \'train\', \'dataset_dir\': \'/home/kevin/dataset/processed_data\', \'variable\': \'depth_map\', \'dtype\': \'frame\', \'seed\': 1337, \'batch_size\': 128, \'mean\': 2}"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "img"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    name: "conv3_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv3_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  param {
    name: "conv4_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv4_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    name: "conv5_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv5_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 40
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1108 12:02:15.850414 10127 layer_factory.hpp:77] Creating layer img
I1108 12:02:15.875151 10127 net.cpp:91] Creating Layer img
I1108 12:02:15.875176 10127 net.cpp:399] img -> img
I1108 12:02:15.875205 10127 net.cpp:399] img -> label
{'img_size': (250, 250), 'split': 'train', 'dataset_dir': '/home/kevin/dataset/processed_data', 'variable': 'depth_map', 'dtype': 'frame', 'seed': 1337, 'batch_size': 128, 'mean': 2}
I1108 12:02:34.658720 10127 net.cpp:141] Setting up img
I1108 12:02:34.658751 10127 net.cpp:148] Top shape: 128 1 250 250 (8000000)
I1108 12:02:34.658756 10127 net.cpp:148] Top shape: 128 1 (128)
I1108 12:02:34.658759 10127 net.cpp:156] Memory required for data: 32000512
I1108 12:02:34.658766 10127 layer_factory.hpp:77] Creating layer label_img_1_split
I1108 12:02:34.658792 10127 net.cpp:91] Creating Layer label_img_1_split
I1108 12:02:34.658807 10127 net.cpp:425] label_img_1_split <- label
I1108 12:02:34.658812 10127 net.cpp:399] label_img_1_split -> label_img_1_split_0
I1108 12:02:34.658821 10127 net.cpp:399] label_img_1_split -> label_img_1_split_1
I1108 12:02:34.658869 10127 net.cpp:141] Setting up label_img_1_split
I1108 12:02:34.658886 10127 net.cpp:148] Top shape: 128 1 (128)
I1108 12:02:34.658890 10127 net.cpp:148] Top shape: 128 1 (128)
I1108 12:02:34.658895 10127 net.cpp:156] Memory required for data: 32001536
I1108 12:02:34.658898 10127 layer_factory.hpp:77] Creating layer conv1
I1108 12:02:34.658921 10127 net.cpp:91] Creating Layer conv1
I1108 12:02:34.658934 10127 net.cpp:425] conv1 <- img
I1108 12:02:34.658941 10127 net.cpp:399] conv1 -> conv1
I1108 12:02:34.659881 10127 net.cpp:141] Setting up conv1
I1108 12:02:34.659904 10127 net.cpp:148] Top shape: 128 128 125 125 (256000000)
I1108 12:02:34.659909 10127 net.cpp:156] Memory required for data: 1056001536
I1108 12:02:34.659916 10127 layer_factory.hpp:77] Creating layer relu1
I1108 12:02:34.659924 10127 net.cpp:91] Creating Layer relu1
I1108 12:02:34.659940 10127 net.cpp:425] relu1 <- conv1
I1108 12:02:34.659945 10127 net.cpp:386] relu1 -> conv1 (in-place)
I1108 12:02:34.659951 10127 net.cpp:141] Setting up relu1
I1108 12:02:34.659956 10127 net.cpp:148] Top shape: 128 128 125 125 (256000000)
I1108 12:02:34.659960 10127 net.cpp:156] Memory required for data: 2080001536
I1108 12:02:34.659965 10127 layer_factory.hpp:77] Creating layer pool1
I1108 12:02:34.659972 10127 net.cpp:91] Creating Layer pool1
I1108 12:02:34.659977 10127 net.cpp:425] pool1 <- conv1
I1108 12:02:34.659982 10127 net.cpp:399] pool1 -> pool1
I1108 12:02:34.660010 10127 net.cpp:141] Setting up pool1
I1108 12:02:34.660017 10127 net.cpp:148] Top shape: 128 128 62 62 (62980096)
I1108 12:02:34.660022 10127 net.cpp:156] Memory required for data: 2331921920
I1108 12:02:34.660027 10127 layer_factory.hpp:77] Creating layer norm1
I1108 12:02:34.660035 10127 net.cpp:91] Creating Layer norm1
I1108 12:02:34.660039 10127 net.cpp:425] norm1 <- pool1
I1108 12:02:34.660045 10127 net.cpp:399] norm1 -> norm1
I1108 12:02:34.660068 10127 net.cpp:141] Setting up norm1
I1108 12:02:34.660074 10127 net.cpp:148] Top shape: 128 128 62 62 (62980096)
I1108 12:02:34.660079 10127 net.cpp:156] Memory required for data: 2583842304
I1108 12:02:34.660084 10127 layer_factory.hpp:77] Creating layer conv2
I1108 12:02:34.660094 10127 net.cpp:91] Creating Layer conv2
I1108 12:02:34.660099 10127 net.cpp:425] conv2 <- norm1
I1108 12:02:34.660104 10127 net.cpp:399] conv2 -> conv2
I1108 12:02:34.664731 10127 net.cpp:141] Setting up conv2
I1108 12:02:34.664803 10127 net.cpp:148] Top shape: 128 256 62 62 (125960192)
I1108 12:02:34.664813 10127 net.cpp:156] Memory required for data: 3087683072
I1108 12:02:34.664834 10127 layer_factory.hpp:77] Creating layer relu2
I1108 12:02:34.664849 10127 net.cpp:91] Creating Layer relu2
I1108 12:02:34.664855 10127 net.cpp:425] relu2 <- conv2
I1108 12:02:34.664863 10127 net.cpp:386] relu2 -> conv2 (in-place)
I1108 12:02:34.664875 10127 net.cpp:141] Setting up relu2
I1108 12:02:34.664880 10127 net.cpp:148] Top shape: 128 256 62 62 (125960192)
I1108 12:02:34.664885 10127 net.cpp:156] Memory required for data: 3591523840
I1108 12:02:34.664891 10127 layer_factory.hpp:77] Creating layer pool2
I1108 12:02:34.664899 10127 net.cpp:91] Creating Layer pool2
I1108 12:02:34.664906 10127 net.cpp:425] pool2 <- conv2
I1108 12:02:34.664912 10127 net.cpp:399] pool2 -> pool2
I1108 12:02:34.664942 10127 net.cpp:141] Setting up pool2
I1108 12:02:34.664949 10127 net.cpp:148] Top shape: 128 256 31 31 (31490048)
I1108 12:02:34.664954 10127 net.cpp:156] Memory required for data: 3717484032
I1108 12:02:34.664959 10127 layer_factory.hpp:77] Creating layer norm2
I1108 12:02:34.664968 10127 net.cpp:91] Creating Layer norm2
I1108 12:02:34.664973 10127 net.cpp:425] norm2 <- pool2
I1108 12:02:34.664978 10127 net.cpp:399] norm2 -> norm2
I1108 12:02:34.665001 10127 net.cpp:141] Setting up norm2
I1108 12:02:34.665007 10127 net.cpp:148] Top shape: 128 256 31 31 (31490048)
I1108 12:02:34.665011 10127 net.cpp:156] Memory required for data: 3843444224
I1108 12:02:34.665016 10127 layer_factory.hpp:77] Creating layer conv3
I1108 12:02:34.665027 10127 net.cpp:91] Creating Layer conv3
I1108 12:02:34.665032 10127 net.cpp:425] conv3 <- norm2
I1108 12:02:34.665038 10127 net.cpp:399] conv3 -> conv3
I1108 12:02:34.667726 10127 net.cpp:141] Setting up conv3
I1108 12:02:34.667773 10127 net.cpp:148] Top shape: 128 256 31 31 (31490048)
I1108 12:02:34.667778 10127 net.cpp:156] Memory required for data: 3969404416
I1108 12:02:34.667814 10127 layer_factory.hpp:77] Creating layer relu3
I1108 12:02:34.667829 10127 net.cpp:91] Creating Layer relu3
I1108 12:02:34.667836 10127 net.cpp:425] relu3 <- conv3
I1108 12:02:34.667842 10127 net.cpp:386] relu3 -> conv3 (in-place)
I1108 12:02:34.667851 10127 net.cpp:141] Setting up relu3
I1108 12:02:34.667858 10127 net.cpp:148] Top shape: 128 256 31 31 (31490048)
I1108 12:02:34.667873 10127 net.cpp:156] Memory required for data: 4095364608
I1108 12:02:34.667879 10127 layer_factory.hpp:77] Creating layer pool3
I1108 12:02:34.667899 10127 net.cpp:91] Creating Layer pool3
I1108 12:02:34.667906 10127 net.cpp:425] pool3 <- conv3
I1108 12:02:34.667914 10127 net.cpp:399] pool3 -> pool3
I1108 12:02:34.667969 10127 net.cpp:141] Setting up pool3
I1108 12:02:34.667979 10127 net.cpp:148] Top shape: 128 256 15 15 (7372800)
I1108 12:02:34.667986 10127 net.cpp:156] Memory required for data: 4124855808
I1108 12:02:34.667992 10127 layer_factory.hpp:77] Creating layer conv4
I1108 12:02:34.668016 10127 net.cpp:91] Creating Layer conv4
I1108 12:02:34.668022 10127 net.cpp:425] conv4 <- pool3
I1108 12:02:34.668040 10127 net.cpp:399] conv4 -> conv4
I1108 12:02:34.671437 10127 net.cpp:141] Setting up conv4
I1108 12:02:34.671466 10127 net.cpp:148] Top shape: 128 512 15 15 (14745600)
I1108 12:02:34.671473 10127 net.cpp:156] Memory required for data: 4183838208
I1108 12:02:34.671497 10127 layer_factory.hpp:77] Creating layer relu4
I1108 12:02:34.671509 10127 net.cpp:91] Creating Layer relu4
I1108 12:02:34.671516 10127 net.cpp:425] relu4 <- conv4
I1108 12:02:34.671526 10127 net.cpp:386] relu4 -> conv4 (in-place)
I1108 12:02:34.671540 10127 net.cpp:141] Setting up relu4
I1108 12:02:34.671548 10127 net.cpp:148] Top shape: 128 512 15 15 (14745600)
I1108 12:02:34.671555 10127 net.cpp:156] Memory required for data: 4242820608
I1108 12:02:34.671563 10127 layer_factory.hpp:77] Creating layer conv5
I1108 12:02:34.671578 10127 net.cpp:91] Creating Layer conv5
I1108 12:02:34.671586 10127 net.cpp:425] conv5 <- conv4
I1108 12:02:34.671597 10127 net.cpp:399] conv5 -> conv5
I1108 12:02:34.677578 10127 net.cpp:141] Setting up conv5
I1108 12:02:34.677608 10127 net.cpp:148] Top shape: 128 512 15 15 (14745600)
I1108 12:02:34.677615 10127 net.cpp:156] Memory required for data: 4301803008
I1108 12:02:34.677646 10127 layer_factory.hpp:77] Creating layer relu5
I1108 12:02:34.677659 10127 net.cpp:91] Creating Layer relu5
I1108 12:02:34.677669 10127 net.cpp:425] relu5 <- conv5
I1108 12:02:34.677676 10127 net.cpp:386] relu5 -> conv5 (in-place)
I1108 12:02:34.677687 10127 net.cpp:141] Setting up relu5
I1108 12:02:34.677708 10127 net.cpp:148] Top shape: 128 512 15 15 (14745600)
I1108 12:02:34.677726 10127 net.cpp:156] Memory required for data: 4360785408
I1108 12:02:34.677732 10127 layer_factory.hpp:77] Creating layer pool5
I1108 12:02:34.677752 10127 net.cpp:91] Creating Layer pool5
I1108 12:02:34.677758 10127 net.cpp:425] pool5 <- conv5
I1108 12:02:34.677778 10127 net.cpp:399] pool5 -> pool5
I1108 12:02:34.677846 10127 net.cpp:141] Setting up pool5
I1108 12:02:34.677855 10127 net.cpp:148] Top shape: 128 512 7 7 (3211264)
I1108 12:02:34.677860 10127 net.cpp:156] Memory required for data: 4373630464
I1108 12:02:34.677863 10127 layer_factory.hpp:77] Creating layer fc6
I1108 12:02:34.677872 10127 net.cpp:91] Creating Layer fc6
I1108 12:02:34.677878 10127 net.cpp:425] fc6 <- pool5
I1108 12:02:34.677888 10127 net.cpp:399] fc6 -> fc6
I1108 12:02:35.198487 10127 net.cpp:141] Setting up fc6
I1108 12:02:35.198518 10127 net.cpp:148] Top shape: 128 4096 (524288)
I1108 12:02:35.198523 10127 net.cpp:156] Memory required for data: 4375727616
I1108 12:02:35.198532 10127 layer_factory.hpp:77] Creating layer relu6
I1108 12:02:35.198542 10127 net.cpp:91] Creating Layer relu6
I1108 12:02:35.198547 10127 net.cpp:425] relu6 <- fc6
I1108 12:02:35.198565 10127 net.cpp:386] relu6 -> fc6 (in-place)
I1108 12:02:35.198573 10127 net.cpp:141] Setting up relu6
I1108 12:02:35.198580 10127 net.cpp:148] Top shape: 128 4096 (524288)
I1108 12:02:35.198593 10127 net.cpp:156] Memory required for data: 4377824768
I1108 12:02:35.198596 10127 layer_factory.hpp:77] Creating layer drop6
I1108 12:02:35.198631 10127 net.cpp:91] Creating Layer drop6
I1108 12:02:35.198634 10127 net.cpp:425] drop6 <- fc6
I1108 12:02:35.198652 10127 net.cpp:386] drop6 -> fc6 (in-place)
I1108 12:02:35.198674 10127 net.cpp:141] Setting up drop6
I1108 12:02:35.198680 10127 net.cpp:148] Top shape: 128 4096 (524288)
I1108 12:02:35.198693 10127 net.cpp:156] Memory required for data: 4379921920
I1108 12:02:35.198696 10127 layer_factory.hpp:77] Creating layer fc7
I1108 12:02:35.198714 10127 net.cpp:91] Creating Layer fc7
I1108 12:02:35.198717 10127 net.cpp:425] fc7 <- fc6
I1108 12:02:35.198724 10127 net.cpp:399] fc7 -> fc7
I1108 12:02:35.281272 10127 net.cpp:141] Setting up fc7
I1108 12:02:35.281302 10127 net.cpp:148] Top shape: 128 4096 (524288)
I1108 12:02:35.281307 10127 net.cpp:156] Memory required for data: 4382019072
I1108 12:02:35.281318 10127 layer_factory.hpp:77] Creating layer relu7
I1108 12:02:35.281340 10127 net.cpp:91] Creating Layer relu7
I1108 12:02:35.281357 10127 net.cpp:425] relu7 <- fc7
I1108 12:02:35.281375 10127 net.cpp:386] relu7 -> fc7 (in-place)
I1108 12:02:35.281395 10127 net.cpp:141] Setting up relu7
I1108 12:02:35.281409 10127 net.cpp:148] Top shape: 128 4096 (524288)
I1108 12:02:35.281414 10127 net.cpp:156] Memory required for data: 4384116224
I1108 12:02:35.281427 10127 layer_factory.hpp:77] Creating layer drop7
I1108 12:02:35.281435 10127 net.cpp:91] Creating Layer drop7
I1108 12:02:35.281448 10127 net.cpp:425] drop7 <- fc7
I1108 12:02:35.281455 10127 net.cpp:386] drop7 -> fc7 (in-place)
I1108 12:02:35.281493 10127 net.cpp:141] Setting up drop7
I1108 12:02:35.281499 10127 net.cpp:148] Top shape: 128 4096 (524288)
I1108 12:02:35.281503 10127 net.cpp:156] Memory required for data: 4386213376
I1108 12:02:35.281508 10127 layer_factory.hpp:77] Creating layer fc8
I1108 12:02:35.281525 10127 net.cpp:91] Creating Layer fc8
I1108 12:02:35.281540 10127 net.cpp:425] fc8 <- fc7
I1108 12:02:35.281545 10127 net.cpp:399] fc8 -> fc8
I1108 12:02:35.282635 10127 net.cpp:141] Setting up fc8
I1108 12:02:35.282657 10127 net.cpp:148] Top shape: 128 40 (5120)
I1108 12:02:35.282663 10127 net.cpp:156] Memory required for data: 4386233856
I1108 12:02:35.282680 10127 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I1108 12:02:35.282687 10127 net.cpp:91] Creating Layer fc8_fc8_0_split
I1108 12:02:35.282692 10127 net.cpp:425] fc8_fc8_0_split <- fc8
I1108 12:02:35.282698 10127 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1108 12:02:35.282706 10127 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1108 12:02:35.282727 10127 net.cpp:141] Setting up fc8_fc8_0_split
I1108 12:02:35.282743 10127 net.cpp:148] Top shape: 128 40 (5120)
I1108 12:02:35.282747 10127 net.cpp:148] Top shape: 128 40 (5120)
I1108 12:02:35.282759 10127 net.cpp:156] Memory required for data: 4386274816
I1108 12:02:35.282764 10127 layer_factory.hpp:77] Creating layer accuracy
I1108 12:02:35.282775 10127 net.cpp:91] Creating Layer accuracy
I1108 12:02:35.282779 10127 net.cpp:425] accuracy <- fc8_fc8_0_split_0
I1108 12:02:35.282794 10127 net.cpp:425] accuracy <- label_img_1_split_0
I1108 12:02:35.282799 10127 net.cpp:399] accuracy -> accuracy
I1108 12:02:35.282814 10127 net.cpp:141] Setting up accuracy
I1108 12:02:35.282820 10127 net.cpp:148] Top shape: (1)
I1108 12:02:35.282824 10127 net.cpp:156] Memory required for data: 4386274820
I1108 12:02:35.282829 10127 layer_factory.hpp:77] Creating layer loss
I1108 12:02:35.282835 10127 net.cpp:91] Creating Layer loss
I1108 12:02:35.282838 10127 net.cpp:425] loss <- fc8_fc8_0_split_1
I1108 12:02:35.282843 10127 net.cpp:425] loss <- label_img_1_split_1
I1108 12:02:35.282848 10127 net.cpp:399] loss -> loss
I1108 12:02:35.282860 10127 layer_factory.hpp:77] Creating layer loss
I1108 12:02:35.282924 10127 net.cpp:141] Setting up loss
I1108 12:02:35.282930 10127 net.cpp:148] Top shape: (1)
I1108 12:02:35.282935 10127 net.cpp:151]     with loss weight 1
I1108 12:02:35.282946 10127 net.cpp:156] Memory required for data: 4386274824
I1108 12:02:35.282950 10127 net.cpp:217] loss needs backward computation.
I1108 12:02:35.282956 10127 net.cpp:219] accuracy does not need backward computation.
I1108 12:02:35.282960 10127 net.cpp:217] fc8_fc8_0_split needs backward computation.
I1108 12:02:35.282965 10127 net.cpp:217] fc8 needs backward computation.
I1108 12:02:35.282969 10127 net.cpp:217] drop7 needs backward computation.
I1108 12:02:35.282974 10127 net.cpp:217] relu7 needs backward computation.
I1108 12:02:35.282977 10127 net.cpp:217] fc7 needs backward computation.
I1108 12:02:35.282982 10127 net.cpp:217] drop6 needs backward computation.
I1108 12:02:35.282986 10127 net.cpp:217] relu6 needs backward computation.
I1108 12:02:35.282991 10127 net.cpp:217] fc6 needs backward computation.
I1108 12:02:35.282995 10127 net.cpp:217] pool5 needs backward computation.
I1108 12:02:35.283000 10127 net.cpp:217] relu5 needs backward computation.
I1108 12:02:35.283004 10127 net.cpp:217] conv5 needs backward computation.
I1108 12:02:35.283010 10127 net.cpp:217] relu4 needs backward computation.
I1108 12:02:35.283013 10127 net.cpp:217] conv4 needs backward computation.
I1108 12:02:35.283018 10127 net.cpp:217] pool3 needs backward computation.
I1108 12:02:35.283022 10127 net.cpp:217] relu3 needs backward computation.
I1108 12:02:35.283027 10127 net.cpp:217] conv3 needs backward computation.
I1108 12:02:35.283031 10127 net.cpp:217] norm2 needs backward computation.
I1108 12:02:35.283036 10127 net.cpp:217] pool2 needs backward computation.
I1108 12:02:35.283041 10127 net.cpp:217] relu2 needs backward computation.
I1108 12:02:35.283044 10127 net.cpp:217] conv2 needs backward computation.
I1108 12:02:35.283049 10127 net.cpp:217] norm1 needs backward computation.
I1108 12:02:35.283053 10127 net.cpp:217] pool1 needs backward computation.
I1108 12:02:35.283058 10127 net.cpp:217] relu1 needs backward computation.
I1108 12:02:35.283062 10127 net.cpp:217] conv1 needs backward computation.
I1108 12:02:35.283067 10127 net.cpp:219] label_img_1_split does not need backward computation.
I1108 12:02:35.283072 10127 net.cpp:219] img does not need backward computation.
I1108 12:02:35.283077 10127 net.cpp:261] This network produces output accuracy
I1108 12:02:35.283082 10127 net.cpp:261] This network produces output loss
I1108 12:02:35.283093 10127 net.cpp:274] Network initialization done.
I1108 12:02:35.283449 10127 solver.cpp:181] Creating test net (#0) specified by test_net file: test.prototxt
I1108 12:02:35.283579 10127 net.cpp:49] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "img"
  type: "Python"
  top: "img"
  top: "label"
  python_param {
    module: "data_layers.model_net_layer"
    layer: "ModelNetDataLayer"
    param_str: "{\'img_size\': (250, 250), \'split\': \'test\', \'dataset_dir\': \'/home/kevin/dataset/processed_data\', \'variable\': \'depth_map\', \'dtype\': \'object\', \'seed\': 1337, \'batch_size\': 128, \'mean\': 2}"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "img"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    name: "conv3_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv3_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  param {
    name: "conv4_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv4_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    name: "conv5_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv5_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 40
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1108 12:02:35.284354 10127 layer_factory.hpp:77] Creating layer img
I1108 12:02:35.284394 10127 net.cpp:91] Creating Layer img
I1108 12:02:35.284401 10127 net.cpp:399] img -> img
I1108 12:02:35.284409 10127 net.cpp:399] img -> label
{'img_size': (250, 250), 'split': 'test', 'dataset_dir': '/home/kevin/dataset/processed_data', 'variable': 'depth_map', 'dtype': 'object', 'seed': 1337, 'batch_size': 128, 'mean': 2}
I1108 12:02:35.417434 10127 net.cpp:141] Setting up img
I1108 12:02:35.417464 10127 net.cpp:148] Top shape: 24 1 250 250 (1500000)
I1108 12:02:35.417470 10127 net.cpp:148] Top shape: 24 1 (24)
I1108 12:02:35.417474 10127 net.cpp:156] Memory required for data: 6000096
I1108 12:02:35.417481 10127 layer_factory.hpp:77] Creating layer label_img_1_split
I1108 12:02:35.417505 10127 net.cpp:91] Creating Layer label_img_1_split
I1108 12:02:35.417518 10127 net.cpp:425] label_img_1_split <- label
I1108 12:02:35.417526 10127 net.cpp:399] label_img_1_split -> label_img_1_split_0
I1108 12:02:35.417536 10127 net.cpp:399] label_img_1_split -> label_img_1_split_1
I1108 12:02:35.417570 10127 net.cpp:141] Setting up label_img_1_split
I1108 12:02:35.417577 10127 net.cpp:148] Top shape: 24 1 (24)
I1108 12:02:35.417582 10127 net.cpp:148] Top shape: 24 1 (24)
I1108 12:02:35.417585 10127 net.cpp:156] Memory required for data: 6000288
I1108 12:02:35.417606 10127 layer_factory.hpp:77] Creating layer conv1
I1108 12:02:35.417616 10127 net.cpp:91] Creating Layer conv1
I1108 12:02:35.417621 10127 net.cpp:425] conv1 <- img
I1108 12:02:35.417626 10127 net.cpp:399] conv1 -> conv1
I1108 12:02:35.417807 10127 net.cpp:141] Setting up conv1
I1108 12:02:35.417815 10127 net.cpp:148] Top shape: 24 128 125 125 (48000000)
I1108 12:02:35.417819 10127 net.cpp:156] Memory required for data: 198000288
I1108 12:02:35.417827 10127 layer_factory.hpp:77] Creating layer relu1
I1108 12:02:35.417834 10127 net.cpp:91] Creating Layer relu1
I1108 12:02:35.417840 10127 net.cpp:425] relu1 <- conv1
I1108 12:02:35.417845 10127 net.cpp:386] relu1 -> conv1 (in-place)
I1108 12:02:35.417862 10127 net.cpp:141] Setting up relu1
I1108 12:02:35.417876 10127 net.cpp:148] Top shape: 24 128 125 125 (48000000)
I1108 12:02:35.417879 10127 net.cpp:156] Memory required for data: 390000288
I1108 12:02:35.417883 10127 layer_factory.hpp:77] Creating layer pool1
I1108 12:02:35.417898 10127 net.cpp:91] Creating Layer pool1
I1108 12:02:35.417903 10127 net.cpp:425] pool1 <- conv1
I1108 12:02:35.417908 10127 net.cpp:399] pool1 -> pool1
I1108 12:02:35.417932 10127 net.cpp:141] Setting up pool1
I1108 12:02:35.417939 10127 net.cpp:148] Top shape: 24 128 62 62 (11808768)
I1108 12:02:35.417943 10127 net.cpp:156] Memory required for data: 437235360
I1108 12:02:35.417948 10127 layer_factory.hpp:77] Creating layer norm1
I1108 12:02:35.417955 10127 net.cpp:91] Creating Layer norm1
I1108 12:02:35.417959 10127 net.cpp:425] norm1 <- pool1
I1108 12:02:35.417965 10127 net.cpp:399] norm1 -> norm1
I1108 12:02:35.417985 10127 net.cpp:141] Setting up norm1
I1108 12:02:35.417991 10127 net.cpp:148] Top shape: 24 128 62 62 (11808768)
I1108 12:02:35.417995 10127 net.cpp:156] Memory required for data: 484470432
I1108 12:02:35.418000 10127 layer_factory.hpp:77] Creating layer conv2
I1108 12:02:35.418007 10127 net.cpp:91] Creating Layer conv2
I1108 12:02:35.418012 10127 net.cpp:425] conv2 <- norm1
I1108 12:02:35.418018 10127 net.cpp:399] conv2 -> conv2
I1108 12:02:35.422125 10127 net.cpp:141] Setting up conv2
I1108 12:02:35.422139 10127 net.cpp:148] Top shape: 24 256 62 62 (23617536)
I1108 12:02:35.422143 10127 net.cpp:156] Memory required for data: 578940576
I1108 12:02:35.422150 10127 layer_factory.hpp:77] Creating layer relu2
I1108 12:02:35.422171 10127 net.cpp:91] Creating Layer relu2
I1108 12:02:35.422176 10127 net.cpp:425] relu2 <- conv2
I1108 12:02:35.422183 10127 net.cpp:386] relu2 -> conv2 (in-place)
I1108 12:02:35.422204 10127 net.cpp:141] Setting up relu2
I1108 12:02:35.422219 10127 net.cpp:148] Top shape: 24 256 62 62 (23617536)
I1108 12:02:35.422221 10127 net.cpp:156] Memory required for data: 673410720
I1108 12:02:35.422226 10127 layer_factory.hpp:77] Creating layer pool2
I1108 12:02:35.422232 10127 net.cpp:91] Creating Layer pool2
I1108 12:02:35.422237 10127 net.cpp:425] pool2 <- conv2
I1108 12:02:35.422241 10127 net.cpp:399] pool2 -> pool2
I1108 12:02:35.422286 10127 net.cpp:141] Setting up pool2
I1108 12:02:35.422292 10127 net.cpp:148] Top shape: 24 256 31 31 (5904384)
I1108 12:02:35.422297 10127 net.cpp:156] Memory required for data: 697028256
I1108 12:02:35.422300 10127 layer_factory.hpp:77] Creating layer norm2
I1108 12:02:35.422307 10127 net.cpp:91] Creating Layer norm2
I1108 12:02:35.422312 10127 net.cpp:425] norm2 <- pool2
I1108 12:02:35.422317 10127 net.cpp:399] norm2 -> norm2
I1108 12:02:35.422338 10127 net.cpp:141] Setting up norm2
I1108 12:02:35.422343 10127 net.cpp:148] Top shape: 24 256 31 31 (5904384)
I1108 12:02:35.422348 10127 net.cpp:156] Memory required for data: 720645792
I1108 12:02:35.422351 10127 layer_factory.hpp:77] Creating layer conv3
I1108 12:02:35.422359 10127 net.cpp:91] Creating Layer conv3
I1108 12:02:35.422364 10127 net.cpp:425] conv3 <- norm2
I1108 12:02:35.422369 10127 net.cpp:399] conv3 -> conv3
I1108 12:02:35.424000 10127 net.cpp:141] Setting up conv3
I1108 12:02:35.424012 10127 net.cpp:148] Top shape: 24 256 31 31 (5904384)
I1108 12:02:35.424016 10127 net.cpp:156] Memory required for data: 744263328
I1108 12:02:35.424026 10127 layer_factory.hpp:77] Creating layer relu3
I1108 12:02:35.424042 10127 net.cpp:91] Creating Layer relu3
I1108 12:02:35.424047 10127 net.cpp:425] relu3 <- conv3
I1108 12:02:35.424062 10127 net.cpp:386] relu3 -> conv3 (in-place)
I1108 12:02:35.424069 10127 net.cpp:141] Setting up relu3
I1108 12:02:35.424074 10127 net.cpp:148] Top shape: 24 256 31 31 (5904384)
I1108 12:02:35.424078 10127 net.cpp:156] Memory required for data: 767880864
I1108 12:02:35.424083 10127 layer_factory.hpp:77] Creating layer pool3
I1108 12:02:35.424099 10127 net.cpp:91] Creating Layer pool3
I1108 12:02:35.424104 10127 net.cpp:425] pool3 <- conv3
I1108 12:02:35.424119 10127 net.cpp:399] pool3 -> pool3
I1108 12:02:35.424151 10127 net.cpp:141] Setting up pool3
I1108 12:02:35.424157 10127 net.cpp:148] Top shape: 24 256 15 15 (1382400)
I1108 12:02:35.424171 10127 net.cpp:156] Memory required for data: 773410464
I1108 12:02:35.424175 10127 layer_factory.hpp:77] Creating layer conv4
I1108 12:02:35.424183 10127 net.cpp:91] Creating Layer conv4
I1108 12:02:35.424188 10127 net.cpp:425] conv4 <- pool3
I1108 12:02:35.424193 10127 net.cpp:399] conv4 -> conv4
I1108 12:02:35.427073 10127 net.cpp:141] Setting up conv4
I1108 12:02:35.427083 10127 net.cpp:148] Top shape: 24 512 15 15 (2764800)
I1108 12:02:35.427088 10127 net.cpp:156] Memory required for data: 784469664
I1108 12:02:35.427094 10127 layer_factory.hpp:77] Creating layer relu4
I1108 12:02:35.427100 10127 net.cpp:91] Creating Layer relu4
I1108 12:02:35.427105 10127 net.cpp:425] relu4 <- conv4
I1108 12:02:35.427110 10127 net.cpp:386] relu4 -> conv4 (in-place)
I1108 12:02:35.427117 10127 net.cpp:141] Setting up relu4
I1108 12:02:35.427122 10127 net.cpp:148] Top shape: 24 512 15 15 (2764800)
I1108 12:02:35.427126 10127 net.cpp:156] Memory required for data: 795528864
I1108 12:02:35.427130 10127 layer_factory.hpp:77] Creating layer conv5
I1108 12:02:35.427137 10127 net.cpp:91] Creating Layer conv5
I1108 12:02:35.427142 10127 net.cpp:425] conv5 <- conv4
I1108 12:02:35.427147 10127 net.cpp:399] conv5 -> conv5
I1108 12:02:35.432925 10127 net.cpp:141] Setting up conv5
I1108 12:02:35.432962 10127 net.cpp:148] Top shape: 24 512 15 15 (2764800)
I1108 12:02:35.432966 10127 net.cpp:156] Memory required for data: 806588064
I1108 12:02:35.432978 10127 layer_factory.hpp:77] Creating layer relu5
I1108 12:02:35.432998 10127 net.cpp:91] Creating Layer relu5
I1108 12:02:35.433006 10127 net.cpp:425] relu5 <- conv5
I1108 12:02:35.433012 10127 net.cpp:386] relu5 -> conv5 (in-place)
I1108 12:02:35.433027 10127 net.cpp:141] Setting up relu5
I1108 12:02:35.433032 10127 net.cpp:148] Top shape: 24 512 15 15 (2764800)
I1108 12:02:35.433049 10127 net.cpp:156] Memory required for data: 817647264
I1108 12:02:35.433053 10127 layer_factory.hpp:77] Creating layer pool5
I1108 12:02:35.433069 10127 net.cpp:91] Creating Layer pool5
I1108 12:02:35.433074 10127 net.cpp:425] pool5 <- conv5
I1108 12:02:35.433089 10127 net.cpp:399] pool5 -> pool5
I1108 12:02:35.433125 10127 net.cpp:141] Setting up pool5
I1108 12:02:35.433131 10127 net.cpp:148] Top shape: 24 512 7 7 (602112)
I1108 12:02:35.433145 10127 net.cpp:156] Memory required for data: 820055712
I1108 12:02:35.433149 10127 layer_factory.hpp:77] Creating layer fc6
I1108 12:02:35.433156 10127 net.cpp:91] Creating Layer fc6
I1108 12:02:35.433161 10127 net.cpp:425] fc6 <- pool5
I1108 12:02:35.433167 10127 net.cpp:399] fc6 -> fc6
I1108 12:02:35.942636 10127 net.cpp:141] Setting up fc6
I1108 12:02:35.942667 10127 net.cpp:148] Top shape: 24 4096 (98304)
I1108 12:02:35.942673 10127 net.cpp:156] Memory required for data: 820448928
I1108 12:02:35.942683 10127 layer_factory.hpp:77] Creating layer relu6
I1108 12:02:35.942706 10127 net.cpp:91] Creating Layer relu6
I1108 12:02:35.942723 10127 net.cpp:425] relu6 <- fc6
I1108 12:02:35.942739 10127 net.cpp:386] relu6 -> fc6 (in-place)
I1108 12:02:35.942760 10127 net.cpp:141] Setting up relu6
I1108 12:02:35.942766 10127 net.cpp:148] Top shape: 24 4096 (98304)
I1108 12:02:35.942770 10127 net.cpp:156] Memory required for data: 820842144
I1108 12:02:35.942785 10127 layer_factory.hpp:77] Creating layer drop6
I1108 12:02:35.942802 10127 net.cpp:91] Creating Layer drop6
I1108 12:02:35.942807 10127 net.cpp:425] drop6 <- fc6
I1108 12:02:35.942814 10127 net.cpp:386] drop6 -> fc6 (in-place)
I1108 12:02:35.942845 10127 net.cpp:141] Setting up drop6
I1108 12:02:35.942852 10127 net.cpp:148] Top shape: 24 4096 (98304)
I1108 12:02:35.942864 10127 net.cpp:156] Memory required for data: 821235360
I1108 12:02:35.942868 10127 layer_factory.hpp:77] Creating layer fc7
I1108 12:02:35.942886 10127 net.cpp:91] Creating Layer fc7
I1108 12:02:35.942900 10127 net.cpp:425] fc7 <- fc6
I1108 12:02:35.942906 10127 net.cpp:399] fc7 -> fc7
I1108 12:02:36.026510 10127 net.cpp:141] Setting up fc7
I1108 12:02:36.026541 10127 net.cpp:148] Top shape: 24 4096 (98304)
I1108 12:02:36.026546 10127 net.cpp:156] Memory required for data: 821628576
I1108 12:02:36.026569 10127 layer_factory.hpp:77] Creating layer relu7
I1108 12:02:36.026592 10127 net.cpp:91] Creating Layer relu7
I1108 12:02:36.026599 10127 net.cpp:425] relu7 <- fc7
I1108 12:02:36.026609 10127 net.cpp:386] relu7 -> fc7 (in-place)
I1108 12:02:36.026619 10127 net.cpp:141] Setting up relu7
I1108 12:02:36.026635 10127 net.cpp:148] Top shape: 24 4096 (98304)
I1108 12:02:36.026639 10127 net.cpp:156] Memory required for data: 822021792
I1108 12:02:36.026645 10127 layer_factory.hpp:77] Creating layer drop7
I1108 12:02:36.026662 10127 net.cpp:91] Creating Layer drop7
I1108 12:02:36.026667 10127 net.cpp:425] drop7 <- fc7
I1108 12:02:36.026674 10127 net.cpp:386] drop7 -> fc7 (in-place)
I1108 12:02:36.026707 10127 net.cpp:141] Setting up drop7
I1108 12:02:36.026715 10127 net.cpp:148] Top shape: 24 4096 (98304)
I1108 12:02:36.026718 10127 net.cpp:156] Memory required for data: 822415008
I1108 12:02:36.026723 10127 layer_factory.hpp:77] Creating layer fc8
I1108 12:02:36.026731 10127 net.cpp:91] Creating Layer fc8
I1108 12:02:36.026736 10127 net.cpp:425] fc8 <- fc7
I1108 12:02:36.026742 10127 net.cpp:399] fc8 -> fc8
I1108 12:02:36.027910 10127 net.cpp:141] Setting up fc8
I1108 12:02:36.027923 10127 net.cpp:148] Top shape: 24 40 (960)
I1108 12:02:36.027928 10127 net.cpp:156] Memory required for data: 822418848
I1108 12:02:36.027945 10127 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I1108 12:02:36.027952 10127 net.cpp:91] Creating Layer fc8_fc8_0_split
I1108 12:02:36.027957 10127 net.cpp:425] fc8_fc8_0_split <- fc8
I1108 12:02:36.027962 10127 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1108 12:02:36.027969 10127 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1108 12:02:36.028004 10127 net.cpp:141] Setting up fc8_fc8_0_split
I1108 12:02:36.028010 10127 net.cpp:148] Top shape: 24 40 (960)
I1108 12:02:36.028024 10127 net.cpp:148] Top shape: 24 40 (960)
I1108 12:02:36.028028 10127 net.cpp:156] Memory required for data: 822426528
I1108 12:02:36.028043 10127 layer_factory.hpp:77] Creating layer accuracy
I1108 12:02:36.028050 10127 net.cpp:91] Creating Layer accuracy
I1108 12:02:36.028064 10127 net.cpp:425] accuracy <- fc8_fc8_0_split_0
I1108 12:02:36.028069 10127 net.cpp:425] accuracy <- label_img_1_split_0
I1108 12:02:36.028087 10127 net.cpp:399] accuracy -> accuracy
I1108 12:02:36.028095 10127 net.cpp:141] Setting up accuracy
I1108 12:02:36.028100 10127 net.cpp:148] Top shape: (1)
I1108 12:02:36.028105 10127 net.cpp:156] Memory required for data: 822426532
I1108 12:02:36.028110 10127 layer_factory.hpp:77] Creating layer loss
I1108 12:02:36.028115 10127 net.cpp:91] Creating Layer loss
I1108 12:02:36.028120 10127 net.cpp:425] loss <- fc8_fc8_0_split_1
I1108 12:02:36.028126 10127 net.cpp:425] loss <- label_img_1_split_1
I1108 12:02:36.028131 10127 net.cpp:399] loss -> loss
I1108 12:02:36.028138 10127 layer_factory.hpp:77] Creating layer loss
I1108 12:02:36.028213 10127 net.cpp:141] Setting up loss
I1108 12:02:36.028220 10127 net.cpp:148] Top shape: (1)
I1108 12:02:36.028234 10127 net.cpp:151]     with loss weight 1
I1108 12:02:36.028244 10127 net.cpp:156] Memory required for data: 822426536
I1108 12:02:36.028249 10127 net.cpp:217] loss needs backward computation.
I1108 12:02:36.028264 10127 net.cpp:219] accuracy does not need backward computation.
I1108 12:02:36.028268 10127 net.cpp:217] fc8_fc8_0_split needs backward computation.
I1108 12:02:36.028273 10127 net.cpp:217] fc8 needs backward computation.
I1108 12:02:36.028278 10127 net.cpp:217] drop7 needs backward computation.
I1108 12:02:36.028282 10127 net.cpp:217] relu7 needs backward computation.
I1108 12:02:36.028287 10127 net.cpp:217] fc7 needs backward computation.
I1108 12:02:36.028292 10127 net.cpp:217] drop6 needs backward computation.
I1108 12:02:36.028297 10127 net.cpp:217] relu6 needs backward computation.
I1108 12:02:36.028301 10127 net.cpp:217] fc6 needs backward computation.
I1108 12:02:36.028306 10127 net.cpp:217] pool5 needs backward computation.
I1108 12:02:36.028321 10127 net.cpp:217] relu5 needs backward computation.
I1108 12:02:36.028334 10127 net.cpp:217] conv5 needs backward computation.
I1108 12:02:36.028339 10127 net.cpp:217] relu4 needs backward computation.
I1108 12:02:36.028353 10127 net.cpp:217] conv4 needs backward computation.
I1108 12:02:36.028357 10127 net.cpp:217] pool3 needs backward computation.
I1108 12:02:36.028373 10127 net.cpp:217] relu3 needs backward computation.
I1108 12:02:36.028378 10127 net.cpp:217] conv3 needs backward computation.
I1108 12:02:36.028381 10127 net.cpp:217] norm2 needs backward computation.
I1108 12:02:36.028388 10127 net.cpp:217] pool2 needs backward computation.
I1108 12:02:36.028393 10127 net.cpp:217] relu2 needs backward computation.
I1108 12:02:36.028396 10127 net.cpp:217] conv2 needs backward computation.
I1108 12:02:36.028400 10127 net.cpp:217] norm1 needs backward computation.
I1108 12:02:36.028405 10127 net.cpp:217] pool1 needs backward computation.
I1108 12:02:36.028409 10127 net.cpp:217] relu1 needs backward computation.
I1108 12:02:36.028424 10127 net.cpp:217] conv1 needs backward computation.
I1108 12:02:36.028429 10127 net.cpp:219] label_img_1_split does not need backward computation.
I1108 12:02:36.028434 10127 net.cpp:219] img does not need backward computation.
I1108 12:02:36.028437 10127 net.cpp:261] This network produces output accuracy
I1108 12:02:36.028441 10127 net.cpp:261] This network produces output loss
I1108 12:02:36.028455 10127 net.cpp:274] Network initialization done.
I1108 12:02:36.028519 10127 solver.cpp:60] Solver scaffolding done.
I1108 12:02:36.038779 10127 solver.cpp:337] Iteration 0, Testing net (#0)
I1108 12:02:38.025944 10127 solver.cpp:228] Iteration 0, loss = 3.66971
I1108 12:02:38.025988 10127 solver.cpp:244]     Train net output #0: accuracy = 0.0546875
I1108 12:02:38.026005 10127 solver.cpp:244]     Train net output #1: loss = 3.66971 (* 1 = 3.66971 loss)
I1108 12:02:38.026037 10127 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1108 12:17:00.123350 10127 solver.cpp:228] Iteration 100, loss = 2.45028
I1108 12:17:00.123409 10127 solver.cpp:244]     Train net output #0: accuracy = 0.320312
I1108 12:17:00.123431 10127 solver.cpp:244]     Train net output #1: loss = 2.45028 (* 1 = 2.45028 loss)
I1108 12:17:00.123453 10127 sgd_solver.cpp:106] Iteration 100, lr = 0.01
I1108 12:32:54.440641 10127 solver.cpp:228] Iteration 200, loss = 1.99053
I1108 12:32:54.440685 10127 solver.cpp:244]     Train net output #0: accuracy = 0.453125
I1108 12:32:54.440701 10127 solver.cpp:244]     Train net output #1: loss = 1.99053 (* 1 = 1.99053 loss)
I1108 12:32:54.440722 10127 sgd_solver.cpp:106] Iteration 200, lr = 0.01
I1108 12:45:52.531765 10127 solver.cpp:228] Iteration 300, loss = 1.59934
I1108 12:45:52.531829 10127 solver.cpp:244]     Train net output #0: accuracy = 0.554688
I1108 12:45:52.531867 10127 solver.cpp:244]     Train net output #1: loss = 1.59934 (* 1 = 1.59934 loss)
I1108 12:45:52.531882 10127 sgd_solver.cpp:106] Iteration 300, lr = 0.01
I1108 12:58:19.832882 10127 solver.cpp:228] Iteration 400, loss = 1.44506
I1108 12:58:19.832916 10127 solver.cpp:244]     Train net output #0: accuracy = 0.546875
I1108 12:58:19.832927 10127 solver.cpp:244]     Train net output #1: loss = 1.44506 (* 1 = 1.44506 loss)
I1108 12:58:19.832936 10127 sgd_solver.cpp:106] Iteration 400, lr = 0.01
I1108 13:10:21.453292 10127 solver.cpp:228] Iteration 500, loss = 1.49285
I1108 13:10:21.453338 10127 solver.cpp:244]     Train net output #0: accuracy = 0.5625
I1108 13:10:21.453356 10127 solver.cpp:244]     Train net output #1: loss = 1.49285 (* 1 = 1.49285 loss)
I1108 13:10:21.453379 10127 sgd_solver.cpp:106] Iteration 500, lr = 0.01
I1108 13:22:35.216137 10127 solver.cpp:228] Iteration 600, loss = 1.153
I1108 13:22:35.216176 10127 solver.cpp:244]     Train net output #0: accuracy = 0.679688
I1108 13:22:35.216187 10127 solver.cpp:244]     Train net output #1: loss = 1.153 (* 1 = 1.153 loss)
I1108 13:22:35.216194 10127 sgd_solver.cpp:106] Iteration 600, lr = 0.01
I1108 13:34:44.296739 10127 solver.cpp:228] Iteration 700, loss = 1.01532
I1108 13:34:44.296775 10127 solver.cpp:244]     Train net output #0: accuracy = 0.695312
I1108 13:34:44.296785 10127 solver.cpp:244]     Train net output #1: loss = 1.01532 (* 1 = 1.01532 loss)
I1108 13:34:44.296792 10127 sgd_solver.cpp:106] Iteration 700, lr = 0.01
I1108 13:46:39.341156 10127 solver.cpp:228] Iteration 800, loss = 1.12888
I1108 13:46:39.341226 10127 solver.cpp:244]     Train net output #0: accuracy = 0.671875
I1108 13:46:39.341279 10127 solver.cpp:244]     Train net output #1: loss = 1.12888 (* 1 = 1.12888 loss)
I1108 13:46:39.341320 10127 sgd_solver.cpp:106] Iteration 800, lr = 0.01
I1108 13:58:27.138973 10127 solver.cpp:228] Iteration 900, loss = 1.13194
I1108 13:58:27.139015 10127 solver.cpp:244]     Train net output #0: accuracy = 0.648438
I1108 13:58:27.139029 10127 solver.cpp:244]     Train net output #1: loss = 1.13194 (* 1 = 1.13194 loss)
I1108 13:58:27.139051 10127 sgd_solver.cpp:106] Iteration 900, lr = 0.01
I1108 14:10:44.717522 10127 solver.cpp:454] Snapshotting to binary proto file /home/kevin/snapshot/heavy_iter_1000.caffemodel
I1108 14:10:55.471446 10127 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/kevin/snapshot/heavy_iter_1000.solverstate
>>> 2016-11-08 14:10:55.892171 Begin model classification tests
F1108 14:10:56.519928 10127 syncedmem.cpp:56] Check failed: error == cudaSuccess (2 vs. 0)  out of memory
*** Check failure stack trace: ***
Aborted (core dumped)
]0;kevin@kevin-desktop: ~/catkin_ws/src/romans_stack/model_net/heavykevin@kevin-desktop:~/catkin_ws/src/romans_stack/model_net/heavy$ ./solve.pycreate_net_heavy.py[11Psolve.pyg./solve.pye./solve.pyd./solve.pyi./solve.pyt./solve.py ./solve.py
]0;kevin@kevin-desktop: ~/catkin_ws/src/romans_stack/model_net/heavykevin@kevin-desktop:~/catkin_ws/src/romans_stack/model_net/heavy$ gedit ./solve.py[6P./solve.pycreate_net_heavy.py[11Psolve.py
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Net<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Blob<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Solver<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1108 14:34:03.157424 12355 solver.cpp:48] Initializing solver from parameters: 
train_net: "train.prototxt"
test_net: "test.prototxt"
test_iter: 0
test_interval: 9999999
base_lr: 0.01
display: 100
max_iter: 450000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 5000
snapshot: 1000
snapshot_prefix: "/home/kevin/snapshot/heavy"
solver_mode: GPU
I1108 14:34:03.157554 12355 solver.cpp:81] Creating training net from train_net file: train.prototxt
I1108 14:34:03.185497 12355 net.cpp:49] Initializing net from parameters: 
state {
  phase: TRAIN
}
layer {
  name: "img"
  type: "Python"
  top: "img"
  top: "label"
  python_param {
    module: "data_layers.model_net_layer"
    layer: "ModelNetDataLayer"
    param_str: "{\'img_size\': (250, 250), \'split\': \'train\', \'dataset_dir\': \'/home/kevin/dataset/processed_data\', \'variable\': \'depth_map\', \'dtype\': \'frame\', \'seed\': 1337, \'batch_size\': 128, \'mean\': 2}"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "img"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    name: "conv3_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv3_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  param {
    name: "conv4_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv4_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    name: "conv5_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv5_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 40
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1108 14:34:03.186254 12355 layer_factory.hpp:77] Creating layer img
I1108 14:34:03.391576 12355 net.cpp:91] Creating Layer img
I1108 14:34:03.391609 12355 net.cpp:399] img -> img
I1108 14:34:03.391620 12355 net.cpp:399] img -> label
{'img_size': (250, 250), 'split': 'train', 'dataset_dir': '/home/kevin/dataset/processed_data', 'variable': 'depth_map', 'dtype': 'frame', 'seed': 1337, 'batch_size': 128, 'mean': 2}
I1108 14:34:20.364392 12355 net.cpp:141] Setting up img
I1108 14:34:20.364431 12355 net.cpp:148] Top shape: 128 1 250 250 (8000000)
I1108 14:34:20.364437 12355 net.cpp:148] Top shape: 128 1 (128)
I1108 14:34:20.364444 12355 net.cpp:156] Memory required for data: 32000512
I1108 14:34:20.364460 12355 layer_factory.hpp:77] Creating layer label_img_1_split
I1108 14:34:20.364485 12355 net.cpp:91] Creating Layer label_img_1_split
I1108 14:34:20.364491 12355 net.cpp:425] label_img_1_split <- label
I1108 14:34:20.364498 12355 net.cpp:399] label_img_1_split -> label_img_1_split_0
I1108 14:34:20.364519 12355 net.cpp:399] label_img_1_split -> label_img_1_split_1
I1108 14:34:20.364545 12355 net.cpp:141] Setting up label_img_1_split
I1108 14:34:20.364552 12355 net.cpp:148] Top shape: 128 1 (128)
I1108 14:34:20.364565 12355 net.cpp:148] Top shape: 128 1 (128)
I1108 14:34:20.364570 12355 net.cpp:156] Memory required for data: 32001536
I1108 14:34:20.364584 12355 layer_factory.hpp:77] Creating layer conv1
I1108 14:34:20.364605 12355 net.cpp:91] Creating Layer conv1
I1108 14:34:20.364609 12355 net.cpp:425] conv1 <- img
I1108 14:34:20.364624 12355 net.cpp:399] conv1 -> conv1
I1108 14:34:20.365569 12355 net.cpp:141] Setting up conv1
I1108 14:34:20.365582 12355 net.cpp:148] Top shape: 128 128 125 125 (256000000)
I1108 14:34:20.365586 12355 net.cpp:156] Memory required for data: 1056001536
I1108 14:34:20.365595 12355 layer_factory.hpp:77] Creating layer relu1
I1108 14:34:20.365613 12355 net.cpp:91] Creating Layer relu1
I1108 14:34:20.365618 12355 net.cpp:425] relu1 <- conv1
I1108 14:34:20.365625 12355 net.cpp:386] relu1 -> conv1 (in-place)
I1108 14:34:20.365633 12355 net.cpp:141] Setting up relu1
I1108 14:34:20.365638 12355 net.cpp:148] Top shape: 128 128 125 125 (256000000)
I1108 14:34:20.365643 12355 net.cpp:156] Memory required for data: 2080001536
I1108 14:34:20.365646 12355 layer_factory.hpp:77] Creating layer pool1
I1108 14:34:20.365664 12355 net.cpp:91] Creating Layer pool1
I1108 14:34:20.365679 12355 net.cpp:425] pool1 <- conv1
I1108 14:34:20.365686 12355 net.cpp:399] pool1 -> pool1
I1108 14:34:20.365725 12355 net.cpp:141] Setting up pool1
I1108 14:34:20.365731 12355 net.cpp:148] Top shape: 128 128 62 62 (62980096)
I1108 14:34:20.365746 12355 net.cpp:156] Memory required for data: 2331921920
I1108 14:34:20.365749 12355 layer_factory.hpp:77] Creating layer norm1
I1108 14:34:20.365756 12355 net.cpp:91] Creating Layer norm1
I1108 14:34:20.365761 12355 net.cpp:425] norm1 <- pool1
I1108 14:34:20.365767 12355 net.cpp:399] norm1 -> norm1
I1108 14:34:20.365790 12355 net.cpp:141] Setting up norm1
I1108 14:34:20.365797 12355 net.cpp:148] Top shape: 128 128 62 62 (62980096)
I1108 14:34:20.365802 12355 net.cpp:156] Memory required for data: 2583842304
I1108 14:34:20.365806 12355 layer_factory.hpp:77] Creating layer conv2
I1108 14:34:20.365814 12355 net.cpp:91] Creating Layer conv2
I1108 14:34:20.365819 12355 net.cpp:425] conv2 <- norm1
I1108 14:34:20.365825 12355 net.cpp:399] conv2 -> conv2
I1108 14:34:20.369760 12355 net.cpp:141] Setting up conv2
I1108 14:34:20.369773 12355 net.cpp:148] Top shape: 128 256 62 62 (125960192)
I1108 14:34:20.369788 12355 net.cpp:156] Memory required for data: 3087683072
I1108 14:34:20.369797 12355 layer_factory.hpp:77] Creating layer relu2
I1108 14:34:20.369804 12355 net.cpp:91] Creating Layer relu2
I1108 14:34:20.369809 12355 net.cpp:425] relu2 <- conv2
I1108 14:34:20.369814 12355 net.cpp:386] relu2 -> conv2 (in-place)
I1108 14:34:20.369822 12355 net.cpp:141] Setting up relu2
I1108 14:34:20.369827 12355 net.cpp:148] Top shape: 128 256 62 62 (125960192)
I1108 14:34:20.369832 12355 net.cpp:156] Memory required for data: 3591523840
I1108 14:34:20.369835 12355 layer_factory.hpp:77] Creating layer pool2
I1108 14:34:20.369843 12355 net.cpp:91] Creating Layer pool2
I1108 14:34:20.369846 12355 net.cpp:425] pool2 <- conv2
I1108 14:34:20.369853 12355 net.cpp:399] pool2 -> pool2
I1108 14:34:20.369876 12355 net.cpp:141] Setting up pool2
I1108 14:34:20.369884 12355 net.cpp:148] Top shape: 128 256 31 31 (31490048)
I1108 14:34:20.369887 12355 net.cpp:156] Memory required for data: 3717484032
I1108 14:34:20.369892 12355 layer_factory.hpp:77] Creating layer norm2
I1108 14:34:20.369899 12355 net.cpp:91] Creating Layer norm2
I1108 14:34:20.369904 12355 net.cpp:425] norm2 <- pool2
I1108 14:34:20.369910 12355 net.cpp:399] norm2 -> norm2
I1108 14:34:20.369930 12355 net.cpp:141] Setting up norm2
I1108 14:34:20.369935 12355 net.cpp:148] Top shape: 128 256 31 31 (31490048)
I1108 14:34:20.369940 12355 net.cpp:156] Memory required for data: 3843444224
I1108 14:34:20.369945 12355 layer_factory.hpp:77] Creating layer conv3
I1108 14:34:20.369951 12355 net.cpp:91] Creating Layer conv3
I1108 14:34:20.369956 12355 net.cpp:425] conv3 <- norm2
I1108 14:34:20.369962 12355 net.cpp:399] conv3 -> conv3
I1108 14:34:20.371805 12355 net.cpp:141] Setting up conv3
I1108 14:34:20.371827 12355 net.cpp:148] Top shape: 128 256 31 31 (31490048)
I1108 14:34:20.371832 12355 net.cpp:156] Memory required for data: 3969404416
I1108 14:34:20.371840 12355 layer_factory.hpp:77] Creating layer relu3
I1108 14:34:20.371847 12355 net.cpp:91] Creating Layer relu3
I1108 14:34:20.371853 12355 net.cpp:425] relu3 <- conv3
I1108 14:34:20.371858 12355 net.cpp:386] relu3 -> conv3 (in-place)
I1108 14:34:20.371865 12355 net.cpp:141] Setting up relu3
I1108 14:34:20.371871 12355 net.cpp:148] Top shape: 128 256 31 31 (31490048)
I1108 14:34:20.371876 12355 net.cpp:156] Memory required for data: 4095364608
I1108 14:34:20.371881 12355 layer_factory.hpp:77] Creating layer pool3
I1108 14:34:20.371886 12355 net.cpp:91] Creating Layer pool3
I1108 14:34:20.371891 12355 net.cpp:425] pool3 <- conv3
I1108 14:34:20.371896 12355 net.cpp:399] pool3 -> pool3
I1108 14:34:20.371922 12355 net.cpp:141] Setting up pool3
I1108 14:34:20.371927 12355 net.cpp:148] Top shape: 128 256 15 15 (7372800)
I1108 14:34:20.371932 12355 net.cpp:156] Memory required for data: 4124855808
I1108 14:34:20.371937 12355 layer_factory.hpp:77] Creating layer conv4
I1108 14:34:20.371945 12355 net.cpp:91] Creating Layer conv4
I1108 14:34:20.371950 12355 net.cpp:425] conv4 <- pool3
I1108 14:34:20.371956 12355 net.cpp:399] conv4 -> conv4
I1108 14:34:20.374878 12355 net.cpp:141] Setting up conv4
I1108 14:34:20.374900 12355 net.cpp:148] Top shape: 128 512 15 15 (14745600)
I1108 14:34:20.374904 12355 net.cpp:156] Memory required for data: 4183838208
I1108 14:34:20.374910 12355 layer_factory.hpp:77] Creating layer relu4
I1108 14:34:20.374927 12355 net.cpp:91] Creating Layer relu4
I1108 14:34:20.374932 12355 net.cpp:425] relu4 <- conv4
I1108 14:34:20.374936 12355 net.cpp:386] relu4 -> conv4 (in-place)
I1108 14:34:20.374943 12355 net.cpp:141] Setting up relu4
I1108 14:34:20.374948 12355 net.cpp:148] Top shape: 128 512 15 15 (14745600)
I1108 14:34:20.374953 12355 net.cpp:156] Memory required for data: 4242820608
I1108 14:34:20.374958 12355 layer_factory.hpp:77] Creating layer conv5
I1108 14:34:20.374965 12355 net.cpp:91] Creating Layer conv5
I1108 14:34:20.374970 12355 net.cpp:425] conv5 <- conv4
I1108 14:34:20.374976 12355 net.cpp:399] conv5 -> conv5
I1108 14:34:20.381224 12355 net.cpp:141] Setting up conv5
I1108 14:34:20.381296 12355 net.cpp:148] Top shape: 128 512 15 15 (14745600)
I1108 14:34:20.381302 12355 net.cpp:156] Memory required for data: 4301803008
I1108 14:34:20.381327 12355 layer_factory.hpp:77] Creating layer relu5
I1108 14:34:20.381351 12355 net.cpp:91] Creating Layer relu5
I1108 14:34:20.381359 12355 net.cpp:425] relu5 <- conv5
I1108 14:34:20.381369 12355 net.cpp:386] relu5 -> conv5 (in-place)
I1108 14:34:20.381389 12355 net.cpp:141] Setting up relu5
I1108 14:34:20.381395 12355 net.cpp:148] Top shape: 128 512 15 15 (14745600)
I1108 14:34:20.381402 12355 net.cpp:156] Memory required for data: 4360785408
I1108 14:34:20.381407 12355 layer_factory.hpp:77] Creating layer pool5
I1108 14:34:20.381419 12355 net.cpp:91] Creating Layer pool5
I1108 14:34:20.381424 12355 net.cpp:425] pool5 <- conv5
I1108 14:34:20.381431 12355 net.cpp:399] pool5 -> pool5
I1108 14:34:20.381474 12355 net.cpp:141] Setting up pool5
I1108 14:34:20.381484 12355 net.cpp:148] Top shape: 128 512 7 7 (3211264)
I1108 14:34:20.381491 12355 net.cpp:156] Memory required for data: 4373630464
I1108 14:34:20.381497 12355 layer_factory.hpp:77] Creating layer fc6
I1108 14:34:20.392272 12355 net.cpp:91] Creating Layer fc6
I1108 14:34:20.392302 12355 net.cpp:425] fc6 <- pool5
I1108 14:34:20.392312 12355 net.cpp:399] fc6 -> fc6
I1108 14:34:20.905656 12355 net.cpp:141] Setting up fc6
I1108 14:34:20.905699 12355 net.cpp:148] Top shape: 128 4096 (524288)
I1108 14:34:20.905705 12355 net.cpp:156] Memory required for data: 4375727616
I1108 14:34:20.905716 12355 layer_factory.hpp:77] Creating layer relu6
I1108 14:34:20.905728 12355 net.cpp:91] Creating Layer relu6
I1108 14:34:20.905745 12355 net.cpp:425] relu6 <- fc6
I1108 14:34:20.905755 12355 net.cpp:386] relu6 -> fc6 (in-place)
I1108 14:34:20.905764 12355 net.cpp:141] Setting up relu6
I1108 14:34:20.905771 12355 net.cpp:148] Top shape: 128 4096 (524288)
I1108 14:34:20.905776 12355 net.cpp:156] Memory required for data: 4377824768
I1108 14:34:20.905781 12355 layer_factory.hpp:77] Creating layer drop6
I1108 14:34:20.905794 12355 net.cpp:91] Creating Layer drop6
I1108 14:34:20.905800 12355 net.cpp:425] drop6 <- fc6
I1108 14:34:20.905807 12355 net.cpp:386] drop6 -> fc6 (in-place)
I1108 14:34:20.905825 12355 net.cpp:141] Setting up drop6
I1108 14:34:20.905841 12355 net.cpp:148] Top shape: 128 4096 (524288)
I1108 14:34:20.905844 12355 net.cpp:156] Memory required for data: 4379921920
I1108 14:34:20.905858 12355 layer_factory.hpp:77] Creating layer fc7
I1108 14:34:20.905866 12355 net.cpp:91] Creating Layer fc7
I1108 14:34:20.905870 12355 net.cpp:425] fc7 <- fc6
I1108 14:34:20.905887 12355 net.cpp:399] fc7 -> fc7
I1108 14:34:20.988060 12355 net.cpp:141] Setting up fc7
I1108 14:34:20.988102 12355 net.cpp:148] Top shape: 128 4096 (524288)
I1108 14:34:20.988107 12355 net.cpp:156] Memory required for data: 4382019072
I1108 14:34:20.988131 12355 layer_factory.hpp:77] Creating layer relu7
I1108 14:34:20.988147 12355 net.cpp:91] Creating Layer relu7
I1108 14:34:20.988155 12355 net.cpp:425] relu7 <- fc7
I1108 14:34:20.988162 12355 net.cpp:386] relu7 -> fc7 (in-place)
I1108 14:34:20.988173 12355 net.cpp:141] Setting up relu7
I1108 14:34:20.988178 12355 net.cpp:148] Top shape: 128 4096 (524288)
I1108 14:34:20.988183 12355 net.cpp:156] Memory required for data: 4384116224
I1108 14:34:20.988189 12355 layer_factory.hpp:77] Creating layer drop7
I1108 14:34:20.988196 12355 net.cpp:91] Creating Layer drop7
I1108 14:34:20.988201 12355 net.cpp:425] drop7 <- fc7
I1108 14:34:20.988209 12355 net.cpp:386] drop7 -> fc7 (in-place)
I1108 14:34:20.988225 12355 net.cpp:141] Setting up drop7
I1108 14:34:20.988243 12355 net.cpp:148] Top shape: 128 4096 (524288)
I1108 14:34:20.988247 12355 net.cpp:156] Memory required for data: 4386213376
I1108 14:34:20.988251 12355 layer_factory.hpp:77] Creating layer fc8
I1108 14:34:20.988258 12355 net.cpp:91] Creating Layer fc8
I1108 14:34:20.988262 12355 net.cpp:425] fc8 <- fc7
I1108 14:34:20.988268 12355 net.cpp:399] fc8 -> fc8
I1108 14:34:20.989393 12355 net.cpp:141] Setting up fc8
I1108 14:34:20.989418 12355 net.cpp:148] Top shape: 128 40 (5120)
I1108 14:34:20.989421 12355 net.cpp:156] Memory required for data: 4386233856
I1108 14:34:20.989428 12355 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I1108 14:34:20.989436 12355 net.cpp:91] Creating Layer fc8_fc8_0_split
I1108 14:34:20.989441 12355 net.cpp:425] fc8_fc8_0_split <- fc8
I1108 14:34:20.989446 12355 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1108 14:34:20.989454 12355 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1108 14:34:20.989475 12355 net.cpp:141] Setting up fc8_fc8_0_split
I1108 14:34:20.989482 12355 net.cpp:148] Top shape: 128 40 (5120)
I1108 14:34:20.989487 12355 net.cpp:148] Top shape: 128 40 (5120)
I1108 14:34:20.989491 12355 net.cpp:156] Memory required for data: 4386274816
I1108 14:34:20.989496 12355 layer_factory.hpp:77] Creating layer accuracy
I1108 14:34:20.989503 12355 net.cpp:91] Creating Layer accuracy
I1108 14:34:20.989507 12355 net.cpp:425] accuracy <- fc8_fc8_0_split_0
I1108 14:34:20.989513 12355 net.cpp:425] accuracy <- label_img_1_split_0
I1108 14:34:20.989519 12355 net.cpp:399] accuracy -> accuracy
I1108 14:34:20.989526 12355 net.cpp:141] Setting up accuracy
I1108 14:34:20.989532 12355 net.cpp:148] Top shape: (1)
I1108 14:34:20.989536 12355 net.cpp:156] Memory required for data: 4386274820
I1108 14:34:20.989542 12355 layer_factory.hpp:77] Creating layer loss
I1108 14:34:20.989553 12355 net.cpp:91] Creating Layer loss
I1108 14:34:20.989559 12355 net.cpp:425] loss <- fc8_fc8_0_split_1
I1108 14:34:20.989564 12355 net.cpp:425] loss <- label_img_1_split_1
I1108 14:34:20.989569 12355 net.cpp:399] loss -> loss
I1108 14:34:20.989578 12355 layer_factory.hpp:77] Creating layer loss
I1108 14:34:20.989645 12355 net.cpp:141] Setting up loss
I1108 14:34:20.989652 12355 net.cpp:148] Top shape: (1)
I1108 14:34:20.989656 12355 net.cpp:151]     with loss weight 1
I1108 14:34:20.989666 12355 net.cpp:156] Memory required for data: 4386274824
I1108 14:34:20.989671 12355 net.cpp:217] loss needs backward computation.
I1108 14:34:20.989677 12355 net.cpp:219] accuracy does not need backward computation.
I1108 14:34:20.989682 12355 net.cpp:217] fc8_fc8_0_split needs backward computation.
I1108 14:34:20.989686 12355 net.cpp:217] fc8 needs backward computation.
I1108 14:34:20.989691 12355 net.cpp:217] drop7 needs backward computation.
I1108 14:34:20.989696 12355 net.cpp:217] relu7 needs backward computation.
I1108 14:34:20.989701 12355 net.cpp:217] fc7 needs backward computation.
I1108 14:34:20.989704 12355 net.cpp:217] drop6 needs backward computation.
I1108 14:34:20.989709 12355 net.cpp:217] relu6 needs backward computation.
I1108 14:34:20.989713 12355 net.cpp:217] fc6 needs backward computation.
I1108 14:34:20.989718 12355 net.cpp:217] pool5 needs backward computation.
I1108 14:34:20.989723 12355 net.cpp:217] relu5 needs backward computation.
I1108 14:34:20.989727 12355 net.cpp:217] conv5 needs backward computation.
I1108 14:34:20.989732 12355 net.cpp:217] relu4 needs backward computation.
I1108 14:34:20.989737 12355 net.cpp:217] conv4 needs backward computation.
I1108 14:34:20.989742 12355 net.cpp:217] pool3 needs backward computation.
I1108 14:34:20.989748 12355 net.cpp:217] relu3 needs backward computation.
I1108 14:34:20.989753 12355 net.cpp:217] conv3 needs backward computation.
I1108 14:34:20.989758 12355 net.cpp:217] norm2 needs backward computation.
I1108 14:34:20.989761 12355 net.cpp:217] pool2 needs backward computation.
I1108 14:34:20.989766 12355 net.cpp:217] relu2 needs backward computation.
I1108 14:34:20.989770 12355 net.cpp:217] conv2 needs backward computation.
I1108 14:34:20.989775 12355 net.cpp:217] norm1 needs backward computation.
I1108 14:34:20.989780 12355 net.cpp:217] pool1 needs backward computation.
I1108 14:34:20.989784 12355 net.cpp:217] relu1 needs backward computation.
I1108 14:34:20.989789 12355 net.cpp:217] conv1 needs backward computation.
I1108 14:34:20.989794 12355 net.cpp:219] label_img_1_split does not need backward computation.
I1108 14:34:20.989799 12355 net.cpp:219] img does not need backward computation.
I1108 14:34:20.989804 12355 net.cpp:261] This network produces output accuracy
I1108 14:34:20.989807 12355 net.cpp:261] This network produces output loss
I1108 14:34:20.989820 12355 net.cpp:274] Network initialization done.
I1108 14:34:21.012770 12355 solver.cpp:181] Creating test net (#0) specified by test_net file: test.prototxt
I1108 14:34:21.013013 12355 net.cpp:49] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "img"
  type: "Python"
  top: "img"
  top: "label"
  python_param {
    module: "data_layers.model_net_layer"
    layer: "ModelNetDataLayer"
    param_str: "{\'img_size\': (250, 250), \'split\': \'test\', \'dataset_dir\': \'/home/kevin/dataset/processed_data\', \'variable\': \'depth_map\', \'dtype\': \'object\', \'seed\': 1337, \'batch_size\': 128, \'mean\': 2}"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "img"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    name: "conv3_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv3_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  param {
    name: "conv4_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv4_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    name: "conv5_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv5_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 40
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1108 14:34:21.013758 12355 layer_factory.hpp:77] Creating layer img
I1108 14:34:21.013803 12355 net.cpp:91] Creating Layer img
I1108 14:34:21.013811 12355 net.cpp:399] img -> img
I1108 14:34:21.013820 12355 net.cpp:399] img -> label
{'img_size': (250, 250), 'split': 'test', 'dataset_dir': '/home/kevin/dataset/processed_data', 'variable': 'depth_map', 'dtype': 'object', 'seed': 1337, 'batch_size': 128, 'mean': 2}
I1108 14:34:21.154734 12355 net.cpp:141] Setting up img
I1108 14:34:21.154778 12355 net.cpp:148] Top shape: 24 1 250 250 (1500000)
I1108 14:34:21.154784 12355 net.cpp:148] Top shape: 24 1 (24)
I1108 14:34:21.154793 12355 net.cpp:156] Memory required for data: 6000096
I1108 14:34:21.154809 12355 layer_factory.hpp:77] Creating layer label_img_1_split
I1108 14:34:21.154821 12355 net.cpp:91] Creating Layer label_img_1_split
I1108 14:34:21.154834 12355 net.cpp:425] label_img_1_split <- label
I1108 14:34:21.154840 12355 net.cpp:399] label_img_1_split -> label_img_1_split_0
I1108 14:34:21.154850 12355 net.cpp:399] label_img_1_split -> label_img_1_split_1
I1108 14:34:21.154897 12355 net.cpp:141] Setting up label_img_1_split
I1108 14:34:21.154904 12355 net.cpp:148] Top shape: 24 1 (24)
I1108 14:34:21.154919 12355 net.cpp:148] Top shape: 24 1 (24)
I1108 14:34:21.154923 12355 net.cpp:156] Memory required for data: 6000288
I1108 14:34:21.154937 12355 layer_factory.hpp:77] Creating layer conv1
I1108 14:34:21.154949 12355 net.cpp:91] Creating Layer conv1
I1108 14:34:21.154954 12355 net.cpp:425] conv1 <- img
I1108 14:34:21.154960 12355 net.cpp:399] conv1 -> conv1
I1108 14:34:21.155122 12355 net.cpp:141] Setting up conv1
I1108 14:34:21.155129 12355 net.cpp:148] Top shape: 24 128 125 125 (48000000)
I1108 14:34:21.155143 12355 net.cpp:156] Memory required for data: 198000288
I1108 14:34:21.155153 12355 layer_factory.hpp:77] Creating layer relu1
I1108 14:34:21.155159 12355 net.cpp:91] Creating Layer relu1
I1108 14:34:21.155164 12355 net.cpp:425] relu1 <- conv1
I1108 14:34:21.155170 12355 net.cpp:386] relu1 -> conv1 (in-place)
I1108 14:34:21.155176 12355 net.cpp:141] Setting up relu1
I1108 14:34:21.155182 12355 net.cpp:148] Top shape: 24 128 125 125 (48000000)
I1108 14:34:21.155186 12355 net.cpp:156] Memory required for data: 390000288
I1108 14:34:21.155191 12355 layer_factory.hpp:77] Creating layer pool1
I1108 14:34:21.155199 12355 net.cpp:91] Creating Layer pool1
I1108 14:34:21.155203 12355 net.cpp:425] pool1 <- conv1
I1108 14:34:21.155208 12355 net.cpp:399] pool1 -> pool1
I1108 14:34:21.155233 12355 net.cpp:141] Setting up pool1
I1108 14:34:21.155239 12355 net.cpp:148] Top shape: 24 128 62 62 (11808768)
I1108 14:34:21.155244 12355 net.cpp:156] Memory required for data: 437235360
I1108 14:34:21.155249 12355 layer_factory.hpp:77] Creating layer norm1
I1108 14:34:21.155256 12355 net.cpp:91] Creating Layer norm1
I1108 14:34:21.155261 12355 net.cpp:425] norm1 <- pool1
I1108 14:34:21.155266 12355 net.cpp:399] norm1 -> norm1
I1108 14:34:21.155287 12355 net.cpp:141] Setting up norm1
I1108 14:34:21.155292 12355 net.cpp:148] Top shape: 24 128 62 62 (11808768)
I1108 14:34:21.155297 12355 net.cpp:156] Memory required for data: 484470432
I1108 14:34:21.155302 12355 layer_factory.hpp:77] Creating layer conv2
I1108 14:34:21.155309 12355 net.cpp:91] Creating Layer conv2
I1108 14:34:21.155313 12355 net.cpp:425] conv2 <- norm1
I1108 14:34:21.155319 12355 net.cpp:399] conv2 -> conv2
I1108 14:34:21.159350 12355 net.cpp:141] Setting up conv2
I1108 14:34:21.159369 12355 net.cpp:148] Top shape: 24 256 62 62 (23617536)
I1108 14:34:21.159374 12355 net.cpp:156] Memory required for data: 578940576
I1108 14:34:21.159384 12355 layer_factory.hpp:77] Creating layer relu2
I1108 14:34:21.159392 12355 net.cpp:91] Creating Layer relu2
I1108 14:34:21.159397 12355 net.cpp:425] relu2 <- conv2
I1108 14:34:21.159404 12355 net.cpp:386] relu2 -> conv2 (in-place)
I1108 14:34:21.159409 12355 net.cpp:141] Setting up relu2
I1108 14:34:21.159413 12355 net.cpp:148] Top shape: 24 256 62 62 (23617536)
I1108 14:34:21.159417 12355 net.cpp:156] Memory required for data: 673410720
I1108 14:34:21.159422 12355 layer_factory.hpp:77] Creating layer pool2
I1108 14:34:21.159430 12355 net.cpp:91] Creating Layer pool2
I1108 14:34:21.159435 12355 net.cpp:425] pool2 <- conv2
I1108 14:34:21.159440 12355 net.cpp:399] pool2 -> pool2
I1108 14:34:21.159466 12355 net.cpp:141] Setting up pool2
I1108 14:34:21.159472 12355 net.cpp:148] Top shape: 24 256 31 31 (5904384)
I1108 14:34:21.159476 12355 net.cpp:156] Memory required for data: 697028256
I1108 14:34:21.159487 12355 layer_factory.hpp:77] Creating layer norm2
I1108 14:34:21.159500 12355 net.cpp:91] Creating Layer norm2
I1108 14:34:21.159507 12355 net.cpp:425] norm2 <- pool2
I1108 14:34:21.159512 12355 net.cpp:399] norm2 -> norm2
I1108 14:34:21.159535 12355 net.cpp:141] Setting up norm2
I1108 14:34:21.159541 12355 net.cpp:148] Top shape: 24 256 31 31 (5904384)
I1108 14:34:21.159546 12355 net.cpp:156] Memory required for data: 720645792
I1108 14:34:21.159550 12355 layer_factory.hpp:77] Creating layer conv3
I1108 14:34:21.159559 12355 net.cpp:91] Creating Layer conv3
I1108 14:34:21.159564 12355 net.cpp:425] conv3 <- norm2
I1108 14:34:21.159569 12355 net.cpp:399] conv3 -> conv3
I1108 14:34:21.161156 12355 net.cpp:141] Setting up conv3
I1108 14:34:21.161170 12355 net.cpp:148] Top shape: 24 256 31 31 (5904384)
I1108 14:34:21.161175 12355 net.cpp:156] Memory required for data: 744263328
I1108 14:34:21.161181 12355 layer_factory.hpp:77] Creating layer relu3
I1108 14:34:21.161188 12355 net.cpp:91] Creating Layer relu3
I1108 14:34:21.161193 12355 net.cpp:425] relu3 <- conv3
I1108 14:34:21.161198 12355 net.cpp:386] relu3 -> conv3 (in-place)
I1108 14:34:21.161206 12355 net.cpp:141] Setting up relu3
I1108 14:34:21.161211 12355 net.cpp:148] Top shape: 24 256 31 31 (5904384)
I1108 14:34:21.161214 12355 net.cpp:156] Memory required for data: 767880864
I1108 14:34:21.161219 12355 layer_factory.hpp:77] Creating layer pool3
I1108 14:34:21.161226 12355 net.cpp:91] Creating Layer pool3
I1108 14:34:21.161231 12355 net.cpp:425] pool3 <- conv3
I1108 14:34:21.161236 12355 net.cpp:399] pool3 -> pool3
I1108 14:34:21.161259 12355 net.cpp:141] Setting up pool3
I1108 14:34:21.161265 12355 net.cpp:148] Top shape: 24 256 15 15 (1382400)
I1108 14:34:21.161269 12355 net.cpp:156] Memory required for data: 773410464
I1108 14:34:21.161274 12355 layer_factory.hpp:77] Creating layer conv4
I1108 14:34:21.161283 12355 net.cpp:91] Creating Layer conv4
I1108 14:34:21.161288 12355 net.cpp:425] conv4 <- pool3
I1108 14:34:21.161293 12355 net.cpp:399] conv4 -> conv4
I1108 14:34:21.164193 12355 net.cpp:141] Setting up conv4
I1108 14:34:21.164207 12355 net.cpp:148] Top shape: 24 512 15 15 (2764800)
I1108 14:34:21.164212 12355 net.cpp:156] Memory required for data: 784469664
I1108 14:34:21.164221 12355 layer_factory.hpp:77] Creating layer relu4
I1108 14:34:21.164227 12355 net.cpp:91] Creating Layer relu4
I1108 14:34:21.164232 12355 net.cpp:425] relu4 <- conv4
I1108 14:34:21.164238 12355 net.cpp:386] relu4 -> conv4 (in-place)
I1108 14:34:21.164245 12355 net.cpp:141] Setting up relu4
I1108 14:34:21.164250 12355 net.cpp:148] Top shape: 24 512 15 15 (2764800)
I1108 14:34:21.164255 12355 net.cpp:156] Memory required for data: 795528864
I1108 14:34:21.164259 12355 layer_factory.hpp:77] Creating layer conv5
I1108 14:34:21.164268 12355 net.cpp:91] Creating Layer conv5
I1108 14:34:21.164273 12355 net.cpp:425] conv5 <- conv4
I1108 14:34:21.164278 12355 net.cpp:399] conv5 -> conv5
I1108 14:34:21.170091 12355 net.cpp:141] Setting up conv5
I1108 14:34:21.170130 12355 net.cpp:148] Top shape: 24 512 15 15 (2764800)
I1108 14:34:21.170133 12355 net.cpp:156] Memory required for data: 806588064
I1108 14:34:21.170145 12355 layer_factory.hpp:77] Creating layer relu5
I1108 14:34:21.170168 12355 net.cpp:91] Creating Layer relu5
I1108 14:34:21.170173 12355 net.cpp:425] relu5 <- conv5
I1108 14:34:21.170181 12355 net.cpp:386] relu5 -> conv5 (in-place)
I1108 14:34:21.170188 12355 net.cpp:141] Setting up relu5
I1108 14:34:21.170192 12355 net.cpp:148] Top shape: 24 512 15 15 (2764800)
I1108 14:34:21.170197 12355 net.cpp:156] Memory required for data: 817647264
I1108 14:34:21.170203 12355 layer_factory.hpp:77] Creating layer pool5
I1108 14:34:21.170209 12355 net.cpp:91] Creating Layer pool5
I1108 14:34:21.170214 12355 net.cpp:425] pool5 <- conv5
I1108 14:34:21.170220 12355 net.cpp:399] pool5 -> pool5
I1108 14:34:21.170258 12355 net.cpp:141] Setting up pool5
I1108 14:34:21.170264 12355 net.cpp:148] Top shape: 24 512 7 7 (602112)
I1108 14:34:21.170279 12355 net.cpp:156] Memory required for data: 820055712
I1108 14:34:21.170284 12355 layer_factory.hpp:77] Creating layer fc6
I1108 14:34:21.170290 12355 net.cpp:91] Creating Layer fc6
I1108 14:34:21.170303 12355 net.cpp:425] fc6 <- pool5
I1108 14:34:21.170320 12355 net.cpp:399] fc6 -> fc6
I1108 14:34:21.669353 12355 net.cpp:141] Setting up fc6
I1108 14:34:21.669404 12355 net.cpp:148] Top shape: 24 4096 (98304)
I1108 14:34:21.669410 12355 net.cpp:156] Memory required for data: 820448928
I1108 14:34:21.669436 12355 layer_factory.hpp:77] Creating layer relu6
I1108 14:34:21.669450 12355 net.cpp:91] Creating Layer relu6
I1108 14:34:21.669461 12355 net.cpp:425] relu6 <- fc6
I1108 14:34:21.669471 12355 net.cpp:386] relu6 -> fc6 (in-place)
I1108 14:34:21.669483 12355 net.cpp:141] Setting up relu6
I1108 14:34:21.669492 12355 net.cpp:148] Top shape: 24 4096 (98304)
I1108 14:34:21.669497 12355 net.cpp:156] Memory required for data: 820842144
I1108 14:34:21.669502 12355 layer_factory.hpp:77] Creating layer drop6
I1108 14:34:21.669512 12355 net.cpp:91] Creating Layer drop6
I1108 14:34:21.669517 12355 net.cpp:425] drop6 <- fc6
I1108 14:34:21.669523 12355 net.cpp:386] drop6 -> fc6 (in-place)
I1108 14:34:21.669564 12355 net.cpp:141] Setting up drop6
I1108 14:34:21.669579 12355 net.cpp:148] Top shape: 24 4096 (98304)
I1108 14:34:21.669584 12355 net.cpp:156] Memory required for data: 821235360
I1108 14:34:21.669597 12355 layer_factory.hpp:77] Creating layer fc7
I1108 14:34:21.669615 12355 net.cpp:91] Creating Layer fc7
I1108 14:34:21.669620 12355 net.cpp:425] fc7 <- fc6
I1108 14:34:21.669627 12355 net.cpp:399] fc7 -> fc7
I1108 14:34:21.755007 12355 net.cpp:141] Setting up fc7
I1108 14:34:21.755079 12355 net.cpp:148] Top shape: 24 4096 (98304)
I1108 14:34:21.755089 12355 net.cpp:156] Memory required for data: 821628576
I1108 14:34:21.755118 12355 layer_factory.hpp:77] Creating layer relu7
I1108 14:34:21.755137 12355 net.cpp:91] Creating Layer relu7
I1108 14:34:21.755149 12355 net.cpp:425] relu7 <- fc7
I1108 14:34:21.755159 12355 net.cpp:386] relu7 -> fc7 (in-place)
I1108 14:34:21.755174 12355 net.cpp:141] Setting up relu7
I1108 14:34:21.755184 12355 net.cpp:148] Top shape: 24 4096 (98304)
I1108 14:34:21.755192 12355 net.cpp:156] Memory required for data: 822021792
I1108 14:34:21.755198 12355 layer_factory.hpp:77] Creating layer drop7
I1108 14:34:21.755208 12355 net.cpp:91] Creating Layer drop7
I1108 14:34:21.755215 12355 net.cpp:425] drop7 <- fc7
I1108 14:34:21.755224 12355 net.cpp:386] drop7 -> fc7 (in-place)
I1108 14:34:21.755267 12355 net.cpp:141] Setting up drop7
I1108 14:34:21.755286 12355 net.cpp:148] Top shape: 24 4096 (98304)
I1108 14:34:21.755290 12355 net.cpp:156] Memory required for data: 822415008
I1108 14:34:21.755303 12355 layer_factory.hpp:77] Creating layer fc8
I1108 14:34:21.755324 12355 net.cpp:91] Creating Layer fc8
I1108 14:34:21.755329 12355 net.cpp:425] fc8 <- fc7
I1108 14:34:21.755336 12355 net.cpp:399] fc8 -> fc8
I1108 14:34:21.756770 12355 net.cpp:141] Setting up fc8
I1108 14:34:21.756830 12355 net.cpp:148] Top shape: 24 40 (960)
I1108 14:34:21.756840 12355 net.cpp:156] Memory required for data: 822418848
I1108 14:34:21.756861 12355 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I1108 14:34:21.756880 12355 net.cpp:91] Creating Layer fc8_fc8_0_split
I1108 14:34:21.756891 12355 net.cpp:425] fc8_fc8_0_split <- fc8
I1108 14:34:21.756902 12355 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1108 14:34:21.756916 12355 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1108 14:34:21.756971 12355 net.cpp:141] Setting up fc8_fc8_0_split
I1108 14:34:21.756983 12355 net.cpp:148] Top shape: 24 40 (960)
I1108 14:34:21.756999 12355 net.cpp:148] Top shape: 24 40 (960)
I1108 14:34:21.757004 12355 net.cpp:156] Memory required for data: 822426528
I1108 14:34:21.757019 12355 layer_factory.hpp:77] Creating layer accuracy
I1108 14:34:21.757027 12355 net.cpp:91] Creating Layer accuracy
I1108 14:34:21.757032 12355 net.cpp:425] accuracy <- fc8_fc8_0_split_0
I1108 14:34:21.757040 12355 net.cpp:425] accuracy <- label_img_1_split_0
I1108 14:34:21.757046 12355 net.cpp:399] accuracy -> accuracy
I1108 14:34:21.757056 12355 net.cpp:141] Setting up accuracy
I1108 14:34:21.757062 12355 net.cpp:148] Top shape: (1)
I1108 14:34:21.757068 12355 net.cpp:156] Memory required for data: 822426532
I1108 14:34:21.757073 12355 layer_factory.hpp:77] Creating layer loss
I1108 14:34:21.757081 12355 net.cpp:91] Creating Layer loss
I1108 14:34:21.757087 12355 net.cpp:425] loss <- fc8_fc8_0_split_1
I1108 14:34:21.757093 12355 net.cpp:425] loss <- label_img_1_split_1
I1108 14:34:21.757100 12355 net.cpp:399] loss -> loss
I1108 14:34:21.757109 12355 layer_factory.hpp:77] Creating layer loss
I1108 14:34:21.757212 12355 net.cpp:141] Setting up loss
I1108 14:34:21.757221 12355 net.cpp:148] Top shape: (1)
I1108 14:34:21.757226 12355 net.cpp:151]     with loss weight 1
I1108 14:34:21.757241 12355 net.cpp:156] Memory required for data: 822426536
I1108 14:34:21.757246 12355 net.cpp:217] loss needs backward computation.
I1108 14:34:21.757252 12355 net.cpp:219] accuracy does not need backward computation.
I1108 14:34:21.757258 12355 net.cpp:217] fc8_fc8_0_split needs backward computation.
I1108 14:34:21.757263 12355 net.cpp:217] fc8 needs backward computation.
I1108 14:34:21.757268 12355 net.cpp:217] drop7 needs backward computation.
I1108 14:34:21.757273 12355 net.cpp:217] relu7 needs backward computation.
I1108 14:34:21.757278 12355 net.cpp:217] fc7 needs backward computation.
I1108 14:34:21.757283 12355 net.cpp:217] drop6 needs backward computation.
I1108 14:34:21.757287 12355 net.cpp:217] relu6 needs backward computation.
I1108 14:34:21.757292 12355 net.cpp:217] fc6 needs backward computation.
I1108 14:34:21.757297 12355 net.cpp:217] pool5 needs backward computation.
I1108 14:34:21.757302 12355 net.cpp:217] relu5 needs backward computation.
I1108 14:34:21.757308 12355 net.cpp:217] conv5 needs backward computation.
I1108 14:34:21.757313 12355 net.cpp:217] relu4 needs backward computation.
I1108 14:34:21.757318 12355 net.cpp:217] conv4 needs backward computation.
I1108 14:34:21.757323 12355 net.cpp:217] pool3 needs backward computation.
I1108 14:34:21.757328 12355 net.cpp:217] relu3 needs backward computation.
I1108 14:34:21.757333 12355 net.cpp:217] conv3 needs backward computation.
I1108 14:34:21.757338 12355 net.cpp:217] norm2 needs backward computation.
I1108 14:34:21.757344 12355 net.cpp:217] pool2 needs backward computation.
I1108 14:34:21.757349 12355 net.cpp:217] relu2 needs backward computation.
I1108 14:34:21.757352 12355 net.cpp:217] conv2 needs backward computation.
I1108 14:34:21.757359 12355 net.cpp:217] norm1 needs backward computation.
I1108 14:34:21.757362 12355 net.cpp:217] pool1 needs backward computation.
I1108 14:34:21.757369 12355 net.cpp:217] relu1 needs backward computation.
I1108 14:34:21.757374 12355 net.cpp:217] conv1 needs backward computation.
I1108 14:34:21.757380 12355 net.cpp:219] label_img_1_split does not need backward computation.
I1108 14:34:21.757385 12355 net.cpp:219] img does not need backward computation.
I1108 14:34:21.757390 12355 net.cpp:261] This network produces output accuracy
I1108 14:34:21.757395 12355 net.cpp:261] This network produces output loss
I1108 14:34:21.757411 12355 net.cpp:274] Network initialization done.
I1108 14:34:21.757534 12355 solver.cpp:60] Solver scaffolding done.
I1108 14:34:27.078526 12355 solver.cpp:337] Iteration 0, Testing net (#0)
I1108 14:34:28.977944 12355 solver.cpp:228] Iteration 0, loss = 0.982136
I1108 14:34:28.977985 12355 solver.cpp:244]     Train net output #0: accuracy = 0.710938
I1108 14:34:28.978013 12355 solver.cpp:244]     Train net output #1: loss = 0.982136 (* 1 = 0.982136 loss)
I1108 14:34:28.978035 12355 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1108 14:40:48.099650 12355 solver.cpp:228] Iteration 100, loss = 0.891388
I1108 14:40:48.099694 12355 solver.cpp:244]     Train net output #0: accuracy = 0.78125
I1108 14:40:48.099704 12355 solver.cpp:244]     Train net output #1: loss = 0.891388 (* 1 = 0.891388 loss)
I1108 14:40:48.099723 12355 sgd_solver.cpp:106] Iteration 100, lr = 0.01
I1108 14:46:43.511875 12355 solver.cpp:228] Iteration 200, loss = 0.828166
I1108 14:46:43.511958 12355 solver.cpp:244]     Train net output #0: accuracy = 0.78125
I1108 14:46:43.511984 12355 solver.cpp:244]     Train net output #1: loss = 0.828166 (* 1 = 0.828166 loss)
I1108 14:46:43.512002 12355 sgd_solver.cpp:106] Iteration 200, lr = 0.01
I1108 14:52:50.777876 12355 solver.cpp:228] Iteration 300, loss = 0.779092
I1108 14:52:50.777921 12355 solver.cpp:244]     Train net output #0: accuracy = 0.773438
I1108 14:52:50.777932 12355 solver.cpp:244]     Train net output #1: loss = 0.779092 (* 1 = 0.779092 loss)
I1108 14:52:50.777951 12355 sgd_solver.cpp:106] Iteration 300, lr = 0.01
I1108 15:00:17.865485 12355 solver.cpp:228] Iteration 400, loss = 0.810223
I1108 15:00:17.865531 12355 solver.cpp:244]     Train net output #0: accuracy = 0.78125
I1108 15:00:17.865541 12355 solver.cpp:244]     Train net output #1: loss = 0.810223 (* 1 = 0.810223 loss)
I1108 15:00:17.865552 12355 sgd_solver.cpp:106] Iteration 400, lr = 0.01
I1108 15:06:14.827924 12355 solver.cpp:228] Iteration 500, loss = 0.799903
I1108 15:06:14.827982 12355 solver.cpp:244]     Train net output #0: accuracy = 0.75
I1108 15:06:14.828014 12355 solver.cpp:244]     Train net output #1: loss = 0.799903 (* 1 = 0.799903 loss)
I1108 15:06:14.828030 12355 sgd_solver.cpp:106] Iteration 500, lr = 0.01
I1108 15:12:41.794953 12355 solver.cpp:228] Iteration 600, loss = 0.735337
I1108 15:12:41.794987 12355 solver.cpp:244]     Train net output #0: accuracy = 0.765625
I1108 15:12:41.794997 12355 solver.cpp:244]     Train net output #1: loss = 0.735337 (* 1 = 0.735337 loss)
I1108 15:12:41.795016 12355 sgd_solver.cpp:106] Iteration 600, lr = 0.01
I1108 15:19:06.899296 12355 solver.cpp:228] Iteration 700, loss = 0.591273
I1108 15:19:06.899343 12355 solver.cpp:244]     Train net output #0: accuracy = 0.796875
I1108 15:19:06.899353 12355 solver.cpp:244]     Train net output #1: loss = 0.591273 (* 1 = 0.591273 loss)
I1108 15:19:06.899371 12355 sgd_solver.cpp:106] Iteration 700, lr = 0.01
I1108 15:25:02.143223 12355 solver.cpp:228] Iteration 800, loss = 0.785873
I1108 15:25:02.143270 12355 solver.cpp:244]     Train net output #0: accuracy = 0.765625
I1108 15:25:02.143281 12355 solver.cpp:244]     Train net output #1: loss = 0.785873 (* 1 = 0.785873 loss)
I1108 15:25:02.143299 12355 sgd_solver.cpp:106] Iteration 800, lr = 0.01
I1108 15:30:35.145170 12355 solver.cpp:228] Iteration 900, loss = 0.766038
I1108 15:30:35.145212 12355 solver.cpp:244]     Train net output #0: accuracy = 0.757812
I1108 15:30:35.145222 12355 solver.cpp:244]     Train net output #1: loss = 0.766038 (* 1 = 0.766038 loss)
I1108 15:30:35.145232 12355 sgd_solver.cpp:106] Iteration 900, lr = 0.01
I1108 15:39:13.835846 12355 solver.cpp:454] Snapshotting to binary proto file /home/kevin/snapshot/heavy_iter_1000.caffemodel
I1108 15:39:20.249354 12355 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/kevin/snapshot/heavy_iter_1000.solverstate
>>> 2016-11-08 15:39:23.236721 Begin model classification tests
>>> 2016-11-08 15:56:48.035684 Iteration 1000 mean cl141] Setting up pool5
I1108 12:02:34.677855 10127 net.cpp:148] Top shape: 128 512 7 7 (3211264)
I1108 12:02:34.677860 10127 net.cpp:156] Memory required for data: 4373630464
I1108 12:02:34.677863 10127 layer_factory.hpp:77] Creating layer fc6
I1108 12:02:34.677872 10127 net.cpp:91] Creating Layer fc6
I1108 12:02:34.677878 10127 net.cpp:425] fc6 <- pool5
I1108 12:02:34.677888 10127 net.cpp:399] fc6 -> fc6
I1108 12:02:35.198487 10127 net.cpp:141] Setting up fc6
I1108 12:02:35.198518 10127 net.cpp:148] Top shape: 128 4096 (524288)
I1108 12:02:35.198523 10127 net.cpp:156] Memory required for data: 4375727616
I1108 12:02:35.198532 10127 layer_factory.hpp:77] Creating layer relu6
I1108 12:02:35.198542 10127 net.cpp:91] Creating Layer relu6
I1108 12:02:35.198547 10127 net.cpp:425] relu6 <- fc6
I1108 12:02:35.198565 10127 net.cpp:386] relu6 -> fc6 (in-place)
I1108 12:02:35.198573 10127 net.cpp:141] Setting up relu6
I1108 12:02:35.198580 10127 net.cpp:148] Top shape: 128 4096 (524288)
I1108 12:02:35.198593 10127 net.cpp:156] Memory required for data: 4377824768
I1108 12:02:35.198596 10127 layer_factory.hpp:77] Creating layer drop6
I1108 12:02:35.198631 10127 net.cpp:91] Creating Layer drop6
I1108 12:02:35.198634 10127 net.cpp:425] drop6 <- fc6
I1108 12:02:35.198652 10127 net.cpp:386] drop6 -> fc6 (in-place)
I1108 12:02:35.198674 10127 net.cpp:141] Setting up drop6
I1108 12:02:35.198680 10127 net.cpp:148] Top shape: 128 4096 (524288)
I1108 12:02:35.198693 10127 net.cpp:156] Memory required for data: 4379921920
I1108 12:02:35.198696 10127 layer_factory.hpp:77] Creating layer fc7
I1108 12:02:35.198714 10127 net.cpp:91] Creating Layer fc7
I1108 12:02:35.198717 10127 net.cpp:425] fc7 <- fc6
I1108 12:02:35.198724 10127 net.cpp:399] fc7 -> fc7
I1108 12:02:35.281272 10127 net.cpp:141] Setting up fc7
I1108 12:02:35.281302 10127 net.cpp:148] Top shape: 128 4096 (524288)
I1108 12:02:35.281307 10127 net.cpp:156] Memory required for data: 4382019072
I1108 12:02:35.281318 10127 layer_factory.hpp:77] Creating layer relu7
I1108 12:02:35.281340 10127 net.cpp:91] Creating Layer relu7
I1108 12:02:35.281357 10127 net.cpp:425] relu7 <- fc7
I1108 12:02:35.281375 10127 net.cpp:386] relu7 -> fc7 (in-place)
I1108 12:02:35.281395 10127 net.cpp:141] Setting up relu7
I1108 12:02:35.281409 10127 net.cpp:148] Top shape: 128 4096 (524288)
I1108 12:02:35.281414 10127 net.cpp:156] Memory required for data: 4384116224
I1108 12:02:35.281427 10127 layer_factory.hpp:77] Creating layer drop7
I1108 12:02:35.281435 10127 net.cpp:91] Creating Layer drop7
I1108 12:02:35.281448 10127 net.cpp:425] drop7 <- fc7
I1108 12:02:35.281455 10127 net.cpp:386] drop7 -> fc7 (in-place)
I1108 12:02:35.281493 10127 net.cpp:141] Setting up drop7
I1108 12:02:35.281499 10127 net.cpp:148] Top shape: 128 4096 (524288)
I1108 12:02:35.281503 10127 net.cpp:156] Memory required for data: 4386213376
I1108 12:02:35.281508 10127 layer_factory.hpp:77] Creating layer fc8
I1108 12:02:35.281525 10127 net.cpp:91] Creating Layer fc8
I1108 12:02:35.281540 10127 net.cpp:425] fc8 <- fc7
I1108 12:02:35.281545 10127 net.cpp:399] fc8 -> fc8
I1108 12:02:35.282635 10127 net.cpp:141] Setting up fc8
I1108 12:02:35.282657 10127 net.cpp:148] Top shape: 128 40 (5120)
I1108 12:02:35.282663 10127 net.cpp:156] Memory required for data: 4386233856
I1108 12:02:35.282680 10127 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I1108 12:02:35.282687 10127 net.cpp:91] Creating Layer fc8_fc8_0_split
I1108 12:02:35.282692 10127 net.cpp:425] fc8_fc8_0_split <- fc8
I1108 12:02:35.282698 10127 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1108 12:02:35.282706 10127 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1108 12:02:35.282727 10127 net.cpp:141] Setting up fc8_fc8_0_split
I1108 12:02:35.282743 10127 net.cpp:148] Top shape: 128 40 (5120)
I1108 12:02:35.282747 10127 net.cpp:148] Top shape: 128 40 (5120)
I1108 12:02:35.282759 10127 net.cpp:156] Memory required for data: 4386274816
I1108 12:02:35.282764 10127 layer_factory.hpp:77] Creating layer accuracy
I1108 12:02:35.282775 10127 net.cpp:91] Creating Layer accuracy
I1108 12:02:35.282779 10127 net.cpp:425] accuracy <- fc8_fc8_0_split_0
I1108 12:02:35.282794 10127 net.cpp:425] accuracy <- label_img_1_split_0
I1108 12:02:35.282799 10127 net.cpp:399] accuracy -> accuracy
I1108 12:02:35.282814 10127 net.cpp:141] Setting up accuracy
I1108 12:02:35.282820 10127 net.cpp:148] Top shape: (1)
I1108 12:02:35.282824 10127 net.cpp:156] Memory required for data: 4386274820
I1108 12:02:35.282829 10127 layer_factory.hpp:77] Creating layer loss
I1108 12:02:35.282835 10127 net.cpp:91] Creating Layer loss
I1108 12:02:35.282838 10127 net.cpp:425] loss <- fc8_fc8_0_split_1
I1108 12:02:35.282843 10127 net.cpp:425] loss <- label_img_1_split_1
I1108 12:02:35.282848 10127 net.cpp:399] loss -> loss
I1108 12:02:35.282860 10127 layer_factory.hpp:77] Creating layer loss
I1108 12:02:35.282924 10127 net.cpp:141] Setting up loss
I1108 12:02:35.282930 10127 net.cpp:148] Top shape: (1)
I1108 12:02:35.282935 10127 net.cpp:151]     with loss weight 1
I1108 12:02:35.282946 10127 net.cpp:156] Memory required for data: 4386274824
I1108 12:02:35.282950 10127 net.cpp:217] loss needs backward computation.
I1108 12:02:35.282956 10127 net.cpp:219] accuracy does not need backward computation.
I1108 12:02:35.282960 10127 net.cpp:217] fc8_fc8_0_split needs backward computation.
I1108 12:02:35.282965 10127 net.cpp:217] fc8 needs backward computation.
I1108 12:02:35.282969 10127 net.cpp:217] drop7 needs backward computation.
I1108 12:02:35.282974 10127 net.cpp:217] relu7 needs backward computation.
I1108 12:02:35.282977 10127 net.cpp:217] fc7 needs backward computation.
I1108 12:02:35.282982 10127 net.cpp:217] drop6 needs backward computation.
I1108 12:02:35.282986 10127 net.cpp:217] relu6 needs backward computation.
I1108 12:02:35.282991 10127 net.cpp:217] fc6 needs backward computation.
I1108 12:02:35.282995 10127 net.cpp:217] pool5 needs backward computation.
I1108 12:02:35.283000 10127 net.cpp:217] relu5 needs backward computation.
I1108 12:02:35.283004 10127 net.cpp:217] conv5 needs backward computation.
I1108 12:02:35.283010 10127 net.cpp:217] relu4 needs backward computation.
I1108 12:02:35.283013 10127 net.cpp:217] conv4 needs backward computation.
I1108 12:02:35.283018 10127 net.cpp:217] pool3 needs backward computation.
I1108 12:02:35.283022 10127 net.cpp:217] relu3 needs backward computation.
I1108 12:02:35.283027 10127 net.cpp:217] conv3 needs backward computation.
I1108 12:02:35.283031 10127 net.cpp:217] norm2 needs backward computation.
I1108 12:02:35.283036 10127 net.cpp:217] pool2 needs backward computation.
I1108 12:02:35.283041 10127 net.cpp:217] relu2 needs backward computation.
I1108 12:02:35.283044 10127 net.cpp:217] conv2 needs backward computation.
I1108 12:02:35.283049 10127 net.cpp:217] norm1 needs backward computation.
I1108 12:02:35.283053 10127 net.cpp:217] pool1 needs backward computation.
I1108 12:02:35.283058 10127 net.cpp:217] relu1 needs backward computation.
I1108 12:02:35.283062 10127 net.cpp:217] conv1 needs backward computation.
I1108 12:02:35.283067 10127 net.cpp:219] label_img_1_split does not need backward computation.
I1108 12:02:35.283072 10127 net.cpp:219] img does not need backward computation.
I1108 12:02:35.283077 10127 net.cpp:261] This network produces output accuracy
I1108 12:02:35.283082 10127 net.cpp:261] This network produces output loss
I1108 12:02:35.283093 10127 net.cpp:274] Network initialization done.
I1108 12:02:35.283449 10127 solver.cpp:181] Creating test net (#0) specified by test_net file: test.prototxt
I1108 12:02:35.283579 10127 net.cpp:49] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "img"
  type: "Python"
  top: "img"
  top: "label"
  python_param {
    module: "data_layers.model_net_layer"
    layer: "ModelNetDataLayer"
    param_str: "{\'img_size\': (250, 250), \'split\': \'test\', \'dataset_dir\': \'/home/kevin/dataset/processed_data\', \'variable\': \'depth_map\', \'dtype\': \'object\', \'seed\': 1337, \'batch_size\': 128, \'mean\': 2}"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "img"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    name: "conv3_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv3_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  param {
    name: "conv4_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv4_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    name: "conv5_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv5_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 40
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1108 12:02:35.284354 10127 layer_factory.hpp:77] Creating layer img
I1108 12:02:35.284394 10127 net.cpp:91] Creating Layer img
I1108 12:02:35.284401 10127 net.cpp:399] img -> img
I1108 12:02:35.284409 10127 net.cpp:399] img -> label
{'img_size': (250, 250), 'split': 'test', 'dataset_dir': '/home/kevin/dataset/processed_data', 'variable': 'depth_map', 'dtype': 'object', 'seed': 1337, 'batch_size': 128, 'mean': 2}
I1108 12:02:35.417434 10127 net.cpp:141] Setting up img
I1108 12:02:35.417464 10127 net.cpp:148] Top shape: 24 1 250 250 (1500000)
I1108 12:02:35.417470 10127 net.cpp:148] Top shape: 24 1 (24)
I1108 12:02:35.417474 10127 net.cpp:156] Memory required for data: 6000096
I1108 12:02:35.417481 10127 layer_factory.hpp:77] Creating layer label_img_1_split
I1108 12:02:35.417505 10127 net.cpp:91] Creating Layer label_img_1_split
I1108 12:02:35.417518 10127 net.cpp:425] label_img_1_split <- label
I1108 12:02:35.417526 10127 net.cpp:399] label_img_1_split -> label_img_1_split_0
I1108 12:02:35.417536 10127 net.cpp:399] label_img_1_split -> label_img_1_split_1
I1108 12:02:35.417570 10127 net.cpp:141] Setting up label_img_1_split
I1108 12:02:35.417577 10127 net.cpp:148] Top shape: 24 1 (24)
I1108 12:02:35.417582 10127 net.cpp:148] Top shape: 24 1 (24)
I1108 12:02:35.417585 10127 net.cpp:156] Memory required for data: 6000288
I1108 12:02:35.417606 10127 layer_factory.hpp:77] Creating layer conv1
I1108 12:02:35.417616 10127 net.cpp:91] Creating Layer conv1
I1108 12:02:35.417621 10127 net.cpp:425] conv1 <- img
I1108 12:02:35.417626 10127 net.cpp:399] conv1 -> conv1
I1108 12:02:35.417807 10127 net.cpp:141] Setting up conv1
I1108 12:02:35.417815 10127 net.cpp:148] Top shape: 24 128 125 125 (48000000)
I1108 12:02:35.417819 10127 net.cpp:156] Memory required for data: 198000288
I1108 12:02:35.417827 10127 layer_factory.hpp:77] Creating layer relu1
I1108 12:02:35.417834 10127 net.cpp:91] Creating Layer relu1
I1108 12:02:35.417840 10127 net.cpp:425] relu1 <- conv1
I1108 12:02:35.417845 10127 net.cpp:386] relu1 -> conv1 (in-place)
I1108 12:02:35.417862 10127 net.cpp:141] Setting up relu1
I1108 12:02:35.417876 10127 net.cpp:148] Top shape: 24 128 125 125 (48000000)
I1108 12:02:35.417879 10127 net.cpp:156] Memory required for data: 390000288
I1108 12:02:35.417883 10127 layer_factory.hpp:77] Creating layer pool1
I1108 12:02:35.417898 10127 net.cpp:91] Creating Layer pool1
I1108 12:02:35.417903 10127 net.cpp:425] pool1 <- conv1
I1108 12:02:35.417908 10127 net.cpp:399] pool1 -> pool1
I1108 12:02:35.417932 10127 net.cpp:141] Setting up pool1
I1108 12:02:35.417939 10127 net.cpp:148] Top shape: 24 128 62 62 (11808768)
I1108 12:02:35.417943 10127 net.cpp:156] Memory required for data: 437235360
I1108 12:02:35.417948 10127 layer_factory.hpp:77] Creating layer norm1
I1108 12:02:35.417955 10127 net.cpp:91] Creating Layer norm1
I1108 12:02:35.417959 10127 net.cpp:425] norm1 <- pool1
I1108 12:02:35.417965 10127 net.cpp:399] norm1 -> norm1
I1108 12:02:35.417985 10127 net.cpp:141] Setting up norm1
I1108 12:02:35.417991 10127 net.cpp:148] Top shape: 24 128 62 62 (11808768)
I1108 12:02:35.417995 10127 net.cpp:156] Memory required for data: 484470432
I1108 12:02:35.418000 10127 layer_factory.hpp:77] Creating layer conv2
I1108 12:02:35.418007 10127 net.cpp:91] Creating Layer conv2
I1108 12:02:35.418012 10127 net.cpp:425] conv2 <- norm1
I1108 12:02:35.418018 10127 net.cpp:399] conv2 -> conv2
I1108 12:02:35.422125 10127 net.cpp:141] Setting up conv2
I1108 12:02:35.422139 10127 net.cpp:148] Top shape: 24 256 62 62 (23617536)
I1108 12:02:35.422143 10127 net.cpp:156] Memory required for data: 578940576
I1108 12:02:35.422150 10127 layer_factory.hpp:77] Creating layer relu2
I1108 12:02:35.422171 10127 net.cpp:91] Creating Layer relu2
I1108 12:02:35.422176 10127 net.cpp:425] relu2 <- conv2
I1108 12:02:35.422183 10127 net.cpp:386] relu2 -> conv2 (in-place)
I1108 12:02:35.422204 10127 net.cpp:141] Setting up relu2
I1108 12:02:35.422219 10127 net.cpp:148] Top shape: 24 256 62 62 (23617536)
I1108 12:02:35.422221 10127 net.cpp:156] Memory required for data: 673410720
I1108 12:02:35.422226 10127 layer_factory.hpp:77] Creating layer pool2
I1108 12:02:35.422232 10127 net.cpp:91] Creating Layer pool2
I1108 12:02:35.422237 10127 net.cpp:425] pool2 <- conv2
I1108 12:02:35.422241 10127 net.cpp:399] pool2 -> pool2
I1108 12:02:35.422286 10127 net.cpp:141] Setting up pool2
I1108 12:02:35.422292 10127 net.cpp:148] Top shape: 24 256 31 31 (5904384)
I1108 12:02:35.422297 10127 net.cpp:156] Memory required for data: 697028256
I1108 12:02:35.422300 10127 layer_factory.hpp:77] Creating layer norm2
I1108 12:02:35.422307 10127 net.cpp:91] Creating Layer norm2
I1108 12:02:35.422312 10127 net.cpp:425] norm2 <- pool2
I1108 12:02:35.422317 10127 net.cpp:399] norm2 -> norm2
I1108 12:02:35.422338 10127 net.cpp:141] Setting up norm2
I1108 12:02:35.422343 10127 net.cpp:148] Top shape: 24 256 31 31 (5904384)
I1108 12:02:35.422348 10127 net.cpp:156] Memory required for data: 720645792
I1108 12:02:35.422351 10127 layer_factory.hpp:77] Creating layer conv3
I1108 12:02:35.422359 10127 net.cpp:91] Creating Layer conv3
I1108 12:02:35.422364 10127 net.cpp:425] conv3 <- norm2
I1108 12:02:35.422369 10127 net.cpp:399] conv3 -> conv3
I1108 12:02:35.424000 10127 net.cpp:141] Setting up conv3
I1108 12:02:35.424012 10127 net.cpp:148] Top shape: 24 256 31 31 (5904384)
I1108 12:02:35.424016 10127 net.cpp:156] Memory required for data: 744263328
I1108 12:02:35.424026 10127 layer_factory.hpp:77] Creating layer relu3
I1108 12:02:35.424042 10127 net.cpp:91] Creating Layer relu3
I1108 12:02:35.424047 10127 net.cpp:425] relu3 <- conv3
I1108 12:02:35.424062 10127 net.cpp:386] relu3 -> conv3 (in-place)
I1108 12:02:35.424069 10127 net.cpp:141] Setting up relu3
I1108 12:02:35.424074 10127 net.cpp:148] Top shape: 24 256 31 31 (5904384)
I1108 12:02:35.424078 10127 net.cpp:156] Memory required for data: 767880864
I1108 12:02:35.424083 10127 layer_factory.hpp:77] Creating layer pool3
I1108 12:02:35.424099 10127 net.cpp:91] Creating Layer pool3
I1108 12:02:35.424104 10127 net.cpp:425] pool3 <- conv3
I1108 12:02:35.424119 10127 net.cpp:399] pool3 -> pool3
I1108 12:02:35.424151 10127 net.cpp:141] Setting up pool3
I1108 12:02:35.424157 10127 net.cpp:148] Top shape: 24 256 15 15 (1382400)
I1108 12:02:35.424171 10127 net.cpp:156] Memory required for data: 773410464
I1108 12:02:35.424175 10127 layer_factory.hpp:77] Creating layer conv4
I1108 12:02:35.424183 10127 net.cpp:91] Creating Layer conv4
I1108 12:02:35.424188 10127 net.cpp:425] conv4 <- pool3
I1108 12:02:35.424193 10127 net.cpp:399] conv4 -> conv4
I1108 12:02:35.427073 10127 net.cpp:141] Setting up conv4
I1108 12:02:35.427083 10127 net.cpp:148] Top shape: 24 512 15 15 (2764800)
I1108 12:02:35.427088 10127 net.cpp:156] Memory required for data: 784469664
I1108 12:02:35.427094 10127 layer_factory.hpp:77] Creating layer relu4
I1108 12:02:35.427100 10127 net.cpp:91] Creating Layer relu4
I1108 12:02:35.427105 10127 net.cpp:425] relu4 <- conv4
I1108 12:02:35.427110 10127 net.cpp:386] relu4 -> conv4 (in-place)
I1108 12:02:35.427117 10127 net.cpp:141] Setting up relu4
I1108 12:02:35.427122 10127 net.cpp:148] Top shape: 24 512 15 15 (2764800)
I1108 12:02:35.427126 10127 net.cpp:156] Memory required for data: 795528864
I1108 12:02:35.427130 10127 layer_factory.hpp:77] Creating layer conv5
I1108 12:02:35.427137 10127 net.cpp:91] Creating Layer conv5
I1108 12:02:35.427142 10127 net.cpp:425] conv5 <- conv4
I1108 12:02:35.427147 10127 net.cpp:399] conv5 -> conv5
I1108 12:02:35.432925 10127 net.cpp:141] Setting up conv5
I1108 12:02:35.432962 10127 net.cpp:148] Top shape: 24 512 15 15 (2764800)
I1108 12:02:35.432966 10127 net.cpp:156] Memory required for data: 806588064
I1108 12:02:35.432978 10127 layer_factory.hpp:77] Creating layer relu5
I1108 12:02:35.432998 10127 net.cpp:91] Creating Layer relu5
I1108 12:02:35.433006 10127 net.cpp:425] relu5 <- conv5
I1108 12:02:35.433012 10127 net.cpp:386] relu5 -> conv5 (in-place)
I1108 12:02:35.433027 10127 net.cpp:141] Setting up relu5
I1108 12:02:35.433032 10127 net.cpp:148] Top shape: 24 512 15 15 (2764800)
I1108 12:02:35.433049 10127 net.cpp:156] Memory required for data: 817647264
I1108 12:02:35.433053 10127 layer_factory.hpp:77] Creating layer pool5
I1108 12:02:35.433069 10127 net.cpp:91] Creating Layer pool5
I1108 12:02:35.433074 10127 net.cpp:425] pool5 <- conv5
I1108 12:02:35.433089 10127 net.cpp:399] pool5 -> pool5
I1108 12:02:35.433125 10127 net.cpp:141] Setting up pool5
I1108 12:02:35.433131 10127 net.cpp:148] Top shape: 24 512 7 7 (602112)
I1108 12:02:35.433145 10127 net.cpp:156] Memory required for data: 820055712
I1108 12:02:35.433149 10127 layer_factory.hpp:77] Creating layer fc6
I1108 12:02:35.433156 10127 net.cpp:91] Creating Layer fc6
I1108 12:02:35.433161 10127 net.cpp:425] fc6 <- pool5
I1108 12:02:35.433167 10127 net.cpp:399] fc6 -> fc6
I1108 12:02:35.942636 10127 net.cpp:141] Setting up fc6
I1108 12:02:35.942667 10127 net.cpp:148] Top shape: 24 4096 (98304)
I1108 12:02:35.942673 10127 net.cpp:156] Memory required for data: 820448928
I1108 12:02:35.942683 10127 layer_factory.hpp:77] Creating layer relu6
I1108 12:02:35.942706 10127 net.cpp:91] Creating Layer relu6
I1108 12:02:35.942723 10127 net.cpp:425] relu6 <- fc6
I1108 12:02:35.942739 10127 net.cpp:386] relu6 -> fc6 (in-place)
I1108 12:02:35.942760 10127 net.cpp:141] Setting up relu6
I1108 12:02:35.942766 10127 net.cpp:148] Top shape: 24 4096 (98304)
I1108 12:02:35.942770 10127 net.cpp:156] Memory required for data: 820842144
I1108 12:02:35.942785 10127 layer_factory.hpp:77] Creating layer drop6
I1108 12:02:35.942802 10127 net.cpp:91] Creating Layer drop6
I1108 12:02:35.942807 10127 net.cpp:425] drop6 <- fc6
I1108 12:02:35.942814 10127 net.cpp:386] drop6 -> fc6 (in-place)
I1108 12:02:35.942845 10127 net.cpp:141] Setting up drop6
I1108 12:02:35.942852 10127 net.cpp:148] Top shape: 24 4096 (98304)
I1108 12:02:35.942864 10127 net.cpp:156] Memory required for data: 821235360
I1108 12:02:35.942868 10127 layer_factory.hpp:77] Creating layer fc7
I1108 12:02:35.942886 10127 net.cpp:91] Creating Layer fc7
I1108 12:02:35.942900 10127 net.cpp:425] fc7 <- fc6
I1108 12:02:35.942906 10127 net.cpp:399] fc7 -> fc7
I1108 12:02:36.026510 10127 net.cpp:141] Setting up fc7
I1108 12:02:36.026541 10127 net.cpp:148] Top shape: 24 4096 (98304)
I1108 12:02:36.026546 10127 net.cpp:156] Memory required for data: 821628576
I1108 12:02:36.026569 10127 layer_factory.hpp:77] Creating layer relu7
I1108 12:02:36.026592 10127 net.cpp:91] Creating Layer relu7
I1108 12:02:36.026599 10127 net.cpp:425] relu7 <- fc7
I1108 12:02:36.026609 10127 net.cpp:386] relu7 -> fc7 (in-place)
I1108 12:02:36.026619 10127 net.cpp:141] Setting up relu7
I1108 12:02:36.026635 10127 net.cpp:148] Top shape: 24 4096 (98304)
I1108 12:02:36.026639 10127 net.cpp:156] Memory required for data: 822021792
I1108 12:02:36.026645 10127 layer_factory.hpp:77] Creating layer drop7
I1108 12:02:36.026662 10127 net.cpp:91] Creating Layer drop7
I1108 12:02:36.026667 10127 net.cpp:425] drop7 <- fc7
I1108 12:02:36.026674 10127 net.cpp:386] drop7 -> fc7 (in-place)
I1108 12:02:36.026707 10127 net.cpp:141] Setting up drop7
I1108 12:02:36.026715 10127 net.cpp:148] Top shape: 24 4096 (98304)
I1108 12:02:36.026718 10127 net.cpp:156] Memory required for data: 822415008
I1108 12:02:36.026723 10127 layer_factory.hpp:77] Creating layer fc8
I1108 12:02:36.026731 10127 net.cpp:91] Creating Layer fc8
I1108 12:02:36.026736 10127 net.cpp:425] fc8 <- fc7
I1108 12:02:36.026742 10127 net.cpp:399] fc8 -> fc8
I1108 12:02:36.027910 10127 net.cpp:141] Setting up fc8
I1108 12:02:36.027923 10127 net.cpp:148] Top shape: 24 40 (960)
I1108 12:02:36.027928 10127 net.cpp:156] Memory required for data: 822418848
I1108 12:02:36.027945 10127 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I1108 12:02:36.027952 10127 net.cpp:91] Creating Layer fc8_fc8_0_split
I1108 12:02:36.027957 10127 net.cpp:425] fc8_fc8_0_split <- fc8
I1108 12:02:36.027962 10127 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1108 12:02:36.027969 10127 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1108 12:02:36.028004 10127 net.cpp:141] Setting up fc8_fc8_0_split
I1108 12:02:36.028010 10127 net.cpp:148] Top shape: 24 40 (960)
I1108 12:02:36.028024 10127 net.cpp:148] Top shape: 24 40 (960)
I1108 12:02:36.028028 10127 net.cpp:156] Memory required for data: 822426528
I1108 12:02:36.028043 10127 layer_factory.hpp:77] Creating layer accuracy
I1108 12:02:36.028050 10127 net.cpp:91] Creating Layer accuracy
I1108 12:02:36.028064 10127 net.cpp:425] accuracy <- fc8_fc8_0_split_0
I1108 12:02:36.028069 10127 net.cpp:425] accuracy <- label_img_1_split_0
I1108 12:02:36.028087 10127 net.cpp:399] accuracy -> accuracy
I1108 12:02:36.028095 10127 net.cpp:141] Setting up accuracy
I1108 12:02:36.028100 10127 net.cpp:148] Top shape: (1)
I1108 12:02:36.028105 10127 net.cpp:156] Memory required for data: 822426532
I1108 12:02:36.028110 10127 layer_factory.hpp:77] Creating layer loss
I1108 12:02:36.028115 10127 net.cpp:91] Creating Layer loss
I1108 12:02:36.028120 10127 net.cpp:425] loss <- fc8_fc8_0_split_1
I1108 12:02:36.028126 10127 net.cpp:425] loss <- label_img_1_split_1
I1108 12:02:36.028131 10127 net.cpp:399] loss -> loss
I1108 12:02:36.028138 10127 layer_factory.hpp:77] Creating layer loss
I1108 12:02:36.028213 10127 net.cpp:141] Setting up loss
I1108 12:02:36.028220 10127 net.cpp:148] Top shape: (1)
I1108 12:02:36.028234 10127 net.cpp:151]     with loss weight 1
I1108 12:02:36.028244 10127 net.cpp:156] Memory required for data: 822426536
I1108 12:02:36.028249 10127 net.cpp:217] loss needs backward computation.
I1108 12:02:36.028264 10127 net.cpp:219] accuracy does not need backward computation.
I1108 12:02:36.028268 10127 net.cpp:217] fc8_fc8_0_split needs backward computation.
I1108 12:02:36.028273 10127 net.cpp:217] fc8 needs backward computation.
I1108 12:02:36.028278 10127 net.cpp:217] drop7 needs backward computation.
I1108 12:02:36.028282 10127 net.cpp:217] relu7 needs backward computation.
I1108 12:02:36.028287 10127 net.cpp:217] fc7 needs backward computation.
I1108 12:02:36.028292 10127 net.cpp:217] drop6 needs backward computation.
I1108 12:02:36.028297 10127 net.cpp:217] relu6 needs backward computation.
I1108 12:02:36.028301 10127 net.cpp:217] fc6 needs backward computation.
I1108 12:02:36.028306 10127 net.cpp:217] pool5 needs backward computation.
I1108 12:02:36.028321 10127 net.cpp:217] relu5 needs backward computation.
I1108 12:02:36.028334 10127 net.cpp:217] conv5 needs backward computation.
I1108 12:02:36.028339 10127 net.cpp:217] relu4 needs backward computation.
I1108 12:02:36.028353 10127 net.cpp:217] conv4 needs backward computation.
I1108 12:02:36.028357 10127 net.cpp:217] pool3 needs backward computation.
I1108 12:02:36.028373 10127 net.cpp:217] relu3 needs backward computation.
I1108 12:02:36.028378 10127 net.cpp:217] conv3 needs backward computation.
I1108 12:02:36.028381 10127 net.cpp:217] norm2 needs backward computation.
I1108 12:02:36.028388 10127 net.cpp:217] pool2 needs backward computation.
I1108 12:02:36.028393 10127 net.cpp:217] relu2 needs backward computation.
I1108 12:02:36.028396 10127 net.cpp:217] conv2 needs backward computation.
I1108 12:02:36.028400 10127 net.cpp:217] norm1 needs backward computation.
I1108 12:02:36.028405 10127 net.cpp:217] pool1 needs backward computation.
I1108 12:02:36.028409 10127 net.cpp:217] relu1 needs backward computation.
I1108 12:02:36.028424 10127 net.cpp:217] conv1 needs backward computation.
I1108 12:02:36.028429 10127 net.cpp:219] label_img_1_split does not need backward computation.
I1108 12:02:36.028434 10127 net.cpp:219] img does not need backward computation.
I1108 12:02:36.028437 10127 net.cpp:261] This network produces output accuracy
I1108 12:02:36.028441 10127 net.cpp:261] This network produces output loss
I1108 12:02:36.028455 10127 net.cpp:274] Network initialization done.
I1108 12:02:36.028519 10127 solver.cpp:60] Solver scaffolding done.
I1108 12:02:36.038779 10127 solver.cpp:337] Iteration 0, Testing net (#0)
I1108 12:02:38.025944 10127 solver.cpp:228] Iteration 0, loss = 3.66971
I1108 12:02:38.025988 10127 solver.cpp:244]     Train net output #0: accuracy = 0.0546875
I1108 12:02:38.026005 10127 solver.cpp:244]     Train net output #1: loss = 3.66971 (* 1 = 3.66971 loss)
I1108 12:02:38.026037 10127 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1108 12:17:00.123350 10127 solver.cpp:228] Iteration 100, loss = 2.45028
I1108 12:17:00.123409 10127 solver.cpp:244]     Train net output #0: accuracy = 0.320312
I1108 12:17:00.123431 10127 solver.cpp:244]     Train net output #1: loss = 2.45028 (* 1 = 2.45028 loss)
I1108 12:17:00.123453 10127 sgd_solver.cpp:106] Iteration 100, lr = 0.01
I1108 12:32:54.440641 10127 solver.cpp:228] Iteration 200, loss = 1.99053
I1108 12:32:54.440685 10127 solver.cpp:244]     Train net output #0: accuracy = 0.453125
I1108 12:32:54.440701 10127 solver.cpp:244]     Train net output #1: loss = 1.99053 (* 1 = 1.99053 loss)
I1108 12:32:54.440722 10127 sgd_solver.cpp:106] Iteration 200, lr = 0.01
I1108 12:45:52.531765 10127 solver.cpp:228] Iteration 300, loss = 1.59934
I1108 12:45:52.531829 10127 solver.cpp:244]     Train net output #0: accuracy = 0.554688
I1108 12:45:52.531867 10127 solver.cpp:244]     Train net output #1: loss = 1.59934 (* 1 = 1.59934 loss)
I1108 12:45:52.531882 10127 sgd_solver.cpp:106] Iteration 300, lr = 0.01
I1108 12:58:19.832882 10127 solver.cpp:228] Iteration 400, loss = 1.44506
I1108 12:58:19.832916 10127 solver.cpp:244]     Train net output #0: accuracy = 0.546875
I1108 12:58:19.832927 10127 solver.cpp:244]     Train net output #1: loss = 1.44506 (* 1 = 1.44506 loss)
I1108 12:58:19.832936 10127 sgd_solver.cpp:106] Iteration 400, lr = 0.01
I1108 13:10:21.453292 10127 solver.cpp:228] Iteration 500, loss = 1.49285
I1108 13:10:21.453338 10127 solver.cpp:244]     Train net output #0: accuracy = 0.5625
I1108 13:10:21.453356 10127 solver.cpp:244]     Train net output #1: loss = 1.49285 (* 1 = 1.49285 loss)
I1108 13:10:21.453379 10127 sgd_solver.cpp:106] Iteration 500, lr = 0.01
I1108 13:22:35.216137 10127 solver.cpp:228] Iteration 600, loss = 1.153
I1108 13:22:35.216176 10127 solver.cpp:244]     Train net output #0: accuracy = 0.679688
I1108 13:22:35.216187 10127 solver.cpp:244]     Train net output #1: loss = 1.153 (* 1 = 1.153 loss)
I1108 13:22:35.216194 10127 sgd_solver.cpp:106] Iteration 600, lr = 0.01
I1108 13:34:44.296739 10127 solver.cpp:228] Iteration 700, loss = 1.01532
I1108 13:34:44.296775 10127 solver.cpp:244]     Train net output #0: accuracy = 0.695312
I1108 13:34:44.296785 10127 solver.cpp:244]     Train net output #1: loss = 1.01532 (* 1 = 1.01532 loss)
I1108 13:34:44.296792 10127 sgd_solver.cpp:106] Iteration 700, lr = 0.01
I1108 13:46:39.341156 10127 solver.cpp:228] Iteration 800, loss = 1.12888
I1108 13:46:39.341226 10127 solver.cpp:244]     Train net output #0: accuracy = 0.671875
I1108 13:46:39.341279 10127 solver.cpp:244]     Train net output #1: loss = 1.12888 (* 1 = 1.12888 loss)
I1108 13:46:39.341320 10127 sgd_solver.cpp:106] Iteration 800, lr = 0.01
I1108 13:58:27.138973 10127 solver.cpp:228] Iteration 900, loss = 1.13194
I1108 13:58:27.139015 10127 solver.cpp:244]     Train net output #0: accuracy = 0.648438
I1108 13:58:27.139029 10127 solver.cpp:244]     Train net output #1: loss = 1.13194 (* 1 = 1.13194 loss)
I1108 13:58:27.139051 10127 sgd_solver.cpp:106] Iteration 900, lr = 0.01
I1108 14:10:44.717522 10127 solver.cpp:454] Snapshotting to binary proto file /home/kevin/snapshot/heavy_iter_1000.caffemodel
I1108 14:10:55.471446 10127 sgd_solver.cpp:273] Snapshotting solver state to binary proto file /home/kevin/snapshot/heavy_iter_1000.solverstate
>>> 2016-11-08 14:10:55.892171 Begin model classification tests
F1108 14:10:56.519928 10127 syncedmem.cpp:56] Check failed: error == cudaSuccess (2 vs. 0)  out of memory
*** Check failure stack trace: ***
Aborted (core dumped)
]0;kevin@kevin-desktop: ~/catkin_ws/src/romans_stack/model_net/heavykevin@kevin-desktop:~/catkin_ws/src/romans_stack/model_net/heavy$ ./solve.pycreate_net_heavy.py[11Psolve.pyg./solve.pye./solve.pyd./solve.pyi./solve.pyt./solve.py ./solve.py
]0;kevin@kevin-desktop: ~/catkin_ws/src/romans_stack/model_net/heavykevin@kevin-desktop:~/catkin_ws/src/romans_stack/model_net/heavy$ gedit ./solve.py[6P./solve.pycreate_net_heavy.py[11Psolve.py
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Net<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Blob<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
/home/kevin/caffeplus/python/caffe/pycaffe.py:13: RuntimeWarning: to-Python converter for boost::shared_ptr<caffe::Solver<float> > already registered; second conversion method ignored.
  from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \
WARNING: Logging before InitGoogleLogging() is written to STDERR
I1108 14:34:03.157424 12355 solver.cpp:48] Initializing solver from parameters: 
train_net: "train.prototxt"
test_net: "test.prototxt"
test_iter: 0
test_interval: 9999999
base_lr: 0.01
display: 100
max_iter: 450000
lr_policy: "step"
gamma: 0.1
momentum: 0.9
weight_decay: 0.0005
stepsize: 5000
snapshot: 1000
snapshot_prefix: "/home/kevin/snapshot/heavy"
solver_mode: GPU
I1108 14:34:03.157554 12355 solver.cpp:81] Creating training net from train_net file: train.prototxt
I1108 14:34:03.185497 12355 net.cpp:49] Initializing net from parameters: 
state {
  phase: TRAIN
}
layer {
  name: "img"
  type: "Python"
  top: "img"
  top: "label"
  python_param {
    module: "data_layers.model_net_layer"
    layer: "ModelNetDataLayer"
    param_str: "{\'img_size\': (250, 250), \'split\': \'train\', \'dataset_dir\': \'/home/kevin/dataset/processed_data\', \'variable\': \'depth_map\', \'dtype\': \'frame\', \'seed\': 1337, \'batch_size\': 128, \'mean\': 2}"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "img"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    name: "conv3_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv3_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  param {
    name: "conv4_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv4_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    name: "conv5_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv5_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 40
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1108 14:34:03.186254 12355 layer_factory.hpp:77] Creating layer img
I1108 14:34:03.391576 12355 net.cpp:91] Creating Layer img
I1108 14:34:03.391609 12355 net.cpp:399] img -> img
I1108 14:34:03.391620 12355 net.cpp:399] img -> label
{'img_size': (250, 250), 'split': 'train', 'dataset_dir': '/home/kevin/dataset/processed_data', 'variable': 'depth_map', 'dtype': 'frame', 'seed': 1337, 'batch_size': 128, 'mean': 2}
I1108 14:34:20.364392 12355 net.cpp:141] Setting up img
I1108 14:34:20.364431 12355 net.cpp:148] Top shape: 128 1 250 250 (8000000)
I1108 14:34:20.364437 12355 net.cpp:148] Top shape: 128 1 (128)
I1108 14:34:20.364444 12355 net.cpp:156] Memory required for data: 32000512
I1108 14:34:20.364460 12355 layer_factory.hpp:77] Creating layer label_img_1_split
I1108 14:34:20.364485 12355 net.cpp:91] Creating Layer label_img_1_split
I1108 14:34:20.364491 12355 net.cpp:425] label_img_1_split <- label
I1108 14:34:20.364498 12355 net.cpp:399] label_img_1_split -> label_img_1_split_0
I1108 14:34:20.364519 12355 net.cpp:399] label_img_1_split -> label_img_1_split_1
I1108 14:34:20.364545 12355 net.cpp:141] Setting up label_img_1_split
I1108 14:34:20.364552 12355 net.cpp:148] Top shape: 128 1 (128)
I1108 14:34:20.364565 12355 net.cpp:148] Top shape: 128 1 (128)
I1108 14:34:20.364570 12355 net.cpp:156] Memory required for data: 32001536
I1108 14:34:20.364584 12355 layer_factory.hpp:77] Creating layer conv1
I1108 14:34:20.364605 12355 net.cpp:91] Creating Layer conv1
I1108 14:34:20.364609 12355 net.cpp:425] conv1 <- img
I1108 14:34:20.364624 12355 net.cpp:399] conv1 -> conv1
I1108 14:34:20.365569 12355 net.cpp:141] Setting up conv1
I1108 14:34:20.365582 12355 net.cpp:148] Top shape: 128 128 125 125 (256000000)
I1108 14:34:20.365586 12355 net.cpp:156] Memory required for data: 1056001536
I1108 14:34:20.365595 12355 layer_factory.hpp:77] Creating layer relu1
I1108 14:34:20.365613 12355 net.cpp:91] Creating Layer relu1
I1108 14:34:20.365618 12355 net.cpp:425] relu1 <- conv1
I1108 14:34:20.365625 12355 net.cpp:386] relu1 -> conv1 (in-place)
I1108 14:34:20.365633 12355 net.cpp:141] Setting up relu1
I1108 14:34:20.365638 12355 net.cpp:148] Top shape: 128 128 125 125 (256000000)
I1108 14:34:20.365643 12355 net.cpp:156] Memory required for data: 2080001536
I1108 14:34:20.365646 12355 layer_factory.hpp:77] Creating layer pool1
I1108 14:34:20.365664 12355 net.cpp:91] Creating Layer pool1
I1108 14:34:20.365679 12355 net.cpp:425] pool1 <- conv1
I1108 14:34:20.365686 12355 net.cpp:399] pool1 -> pool1
I1108 14:34:20.365725 12355 net.cpp:141] Setting up pool1
I1108 14:34:20.365731 12355 net.cpp:148] Top shape: 128 128 62 62 (62980096)
I1108 14:34:20.365746 12355 net.cpp:156] Memory required for data: 2331921920
I1108 14:34:20.365749 12355 layer_factory.hpp:77] Creating layer norm1
I1108 14:34:20.365756 12355 net.cpp:91] Creating Layer norm1
I1108 14:34:20.365761 12355 net.cpp:425] norm1 <- pool1
I1108 14:34:20.365767 12355 net.cpp:399] norm1 -> norm1
I1108 14:34:20.365790 12355 net.cpp:141] Setting up norm1
I1108 14:34:20.365797 12355 net.cpp:148] Top shape: 128 128 62 62 (62980096)
I1108 14:34:20.365802 12355 net.cpp:156] Memory required for data: 2583842304
I1108 14:34:20.365806 12355 layer_factory.hpp:77] Creating layer conv2
I1108 14:34:20.365814 12355 net.cpp:91] Creating Layer conv2
I1108 14:34:20.365819 12355 net.cpp:425] conv2 <- norm1
I1108 14:34:20.365825 12355 net.cpp:399] conv2 -> conv2
I1108 14:34:20.369760 12355 net.cpp:141] Setting up conv2
I1108 14:34:20.369773 12355 net.cpp:148] Top shape: 128 256 62 62 (125960192)
I1108 14:34:20.369788 12355 net.cpp:156] Memory required for data: 3087683072
I1108 14:34:20.369797 12355 layer_factory.hpp:77] Creating layer relu2
I1108 14:34:20.369804 12355 net.cpp:91] Creating Layer relu2
I1108 14:34:20.369809 12355 net.cpp:425] relu2 <- conv2
I1108 14:34:20.369814 12355 net.cpp:386] relu2 -> conv2 (in-place)
I1108 14:34:20.369822 12355 net.cpp:141] Setting up relu2
I1108 14:34:20.369827 12355 net.cpp:148] Top shape: 128 256 62 62 (125960192)
I1108 14:34:20.369832 12355 net.cpp:156] Memory required for data: 3591523840
I1108 14:34:20.369835 12355 layer_factory.hpp:77] Creating layer pool2
I1108 14:34:20.369843 12355 net.cpp:91] Creating Layer pool2
I1108 14:34:20.369846 12355 net.cpp:425] pool2 <- conv2
I1108 14:34:20.369853 12355 net.cpp:399] pool2 -> pool2
I1108 14:34:20.369876 12355 net.cpp:141] Setting up pool2
I1108 14:34:20.369884 12355 net.cpp:148] Top shape: 128 256 31 31 (31490048)
I1108 14:34:20.369887 12355 net.cpp:156] Memory required for data: 3717484032
I1108 14:34:20.369892 12355 layer_factory.hpp:77] Creating layer norm2
I1108 14:34:20.369899 12355 net.cpp:91] Creating Layer norm2
I1108 14:34:20.369904 12355 net.cpp:425] norm2 <- pool2
I1108 14:34:20.369910 12355 net.cpp:399] norm2 -> norm2
I1108 14:34:20.369930 12355 net.cpp:141] Setting up norm2
I1108 14:34:20.369935 12355 net.cpp:148] Top shape: 128 256 31 31 (31490048)
I1108 14:34:20.369940 12355 net.cpp:156] Memory required for data: 3843444224
I1108 14:34:20.369945 12355 layer_factory.hpp:77] Creating layer conv3
I1108 14:34:20.369951 12355 net.cpp:91] Creating Layer conv3
I1108 14:34:20.369956 12355 net.cpp:425] conv3 <- norm2
I1108 14:34:20.369962 12355 net.cpp:399] conv3 -> conv3
I1108 14:34:20.371805 12355 net.cpp:141] Setting up conv3
I1108 14:34:20.371827 12355 net.cpp:148] Top shape: 128 256 31 31 (31490048)
I1108 14:34:20.371832 12355 net.cpp:156] Memory required for data: 3969404416
I1108 14:34:20.371840 12355 layer_factory.hpp:77] Creating layer relu3
I1108 14:34:20.371847 12355 net.cpp:91] Creating Layer relu3
I1108 14:34:20.371853 12355 net.cpp:425] relu3 <- conv3
I1108 14:34:20.371858 12355 net.cpp:386] relu3 -> conv3 (in-place)
I1108 14:34:20.371865 12355 net.cpp:141] Setting up relu3
I1108 14:34:20.371871 12355 net.cpp:148] Top shape: 128 256 31 31 (31490048)
I1108 14:34:20.371876 12355 net.cpp:156] Memory required for data: 4095364608
I1108 14:34:20.371881 12355 layer_factory.hpp:77] Creating layer pool3
I1108 14:34:20.371886 12355 net.cpp:91] Creating Layer pool3
I1108 14:34:20.371891 12355 net.cpp:425] pool3 <- conv3
I1108 14:34:20.371896 12355 net.cpp:399] pool3 -> pool3
I1108 14:34:20.371922 12355 net.cpp:141] Setting up pool3
I1108 14:34:20.371927 12355 net.cpp:148] Top shape: 128 256 15 15 (7372800)
I1108 14:34:20.371932 12355 net.cpp:156] Memory required for data: 4124855808
I1108 14:34:20.371937 12355 layer_factory.hpp:77] Creating layer conv4
I1108 14:34:20.371945 12355 net.cpp:91] Creating Layer conv4
I1108 14:34:20.371950 12355 net.cpp:425] conv4 <- pool3
I1108 14:34:20.371956 12355 net.cpp:399] conv4 -> conv4
I1108 14:34:20.374878 12355 net.cpp:141] Setting up conv4
I1108 14:34:20.374900 12355 net.cpp:148] Top shape: 128 512 15 15 (14745600)
I1108 14:34:20.374904 12355 net.cpp:156] Memory required for data: 4183838208
I1108 14:34:20.374910 12355 layer_factory.hpp:77] Creating layer relu4
I1108 14:34:20.374927 12355 net.cpp:91] Creating Layer relu4
I1108 14:34:20.374932 12355 net.cpp:425] relu4 <- conv4
I1108 14:34:20.374936 12355 net.cpp:386] relu4 -> conv4 (in-place)
I1108 14:34:20.374943 12355 net.cpp:141] Setting up relu4
I1108 14:34:20.374948 12355 net.cpp:148] Top shape: 128 512 15 15 (14745600)
I1108 14:34:20.374953 12355 net.cpp:156] Memory required for data: 4242820608
I1108 14:34:20.374958 12355 layer_factory.hpp:77] Creating layer conv5
I1108 14:34:20.374965 12355 net.cpp:91] Creating Layer conv5
I1108 14:34:20.374970 12355 net.cpp:425] conv5 <- conv4
I1108 14:34:20.374976 12355 net.cpp:399] conv5 -> conv5
I1108 14:34:20.381224 12355 net.cpp:141] Setting up conv5
I1108 14:34:20.381296 12355 net.cpp:148] Top shape: 128 512 15 15 (14745600)
I1108 14:34:20.381302 12355 net.cpp:156] Memory required for data: 4301803008
I1108 14:34:20.381327 12355 layer_factory.hpp:77] Creating layer relu5
I1108 14:34:20.381351 12355 net.cpp:91] Creating Layer relu5
I1108 14:34:20.381359 12355 net.cpp:425] relu5 <- conv5
I1108 14:34:20.381369 12355 net.cpp:386] relu5 -> conv5 (in-place)
I1108 14:34:20.381389 12355 net.cpp:141] Setting up relu5
I1108 14:34:20.381395 12355 net.cpp:148] Top shape: 128 512 15 15 (14745600)
I1108 14:34:20.381402 12355 net.cpp:156] Memory required for data: 4360785408
I1108 14:34:20.381407 12355 layer_factory.hpp:77] Creating layer pool5
I1108 14:34:20.381419 12355 net.cpp:91] Creating Layer pool5
I1108 14:34:20.381424 12355 net.cpp:425] pool5 <- conv5
I1108 14:34:20.381431 12355 net.cpp:399] pool5 -> pool5
I1108 14:34:20.381474 12355 net.cpp:141] Setting up pool5
I1108 14:34:20.381484 12355 net.cpp:148] Top shape: 128 512 7 7 (3211264)
I1108 14:34:20.381491 12355 net.cpp:156] Memory required for data: 4373630464
I1108 14:34:20.381497 12355 layer_factory.hpp:77] Creating layer fc6
I1108 14:34:20.392272 12355 net.cpp:91] Creating Layer fc6
I1108 14:34:20.392302 12355 net.cpp:425] fc6 <- pool5
I1108 14:34:20.392312 12355 net.cpp:399] fc6 -> fc6
I1108 14:34:20.905656 12355 net.cpp:141] Setting up fc6
I1108 14:34:20.905699 12355 net.cpp:148] Top shape: 128 4096 (524288)
I1108 14:34:20.905705 12355 net.cpp:156] Memory required for data: 4375727616
I1108 14:34:20.905716 12355 layer_factory.hpp:77] Creating layer relu6
I1108 14:34:20.905728 12355 net.cpp:91] Creating Layer relu6
I1108 14:34:20.905745 12355 net.cpp:425] relu6 <- fc6
I1108 14:34:20.905755 12355 net.cpp:386] relu6 -> fc6 (in-place)
I1108 14:34:20.905764 12355 net.cpp:141] Setting up relu6
I1108 14:34:20.905771 12355 net.cpp:148] Top shape: 128 4096 (524288)
I1108 14:34:20.905776 12355 net.cpp:156] Memory required for data: 4377824768
I1108 14:34:20.905781 12355 layer_factory.hpp:77] Creating layer drop6
I1108 14:34:20.905794 12355 net.cpp:91] Creating Layer drop6
I1108 14:34:20.905800 12355 net.cpp:425] drop6 <- fc6
I1108 14:34:20.905807 12355 net.cpp:386] drop6 -> fc6 (in-place)
I1108 14:34:20.905825 12355 net.cpp:141] Setting up drop6
I1108 14:34:20.905841 12355 net.cpp:148] Top shape: 128 4096 (524288)
I1108 14:34:20.905844 12355 net.cpp:156] Memory required for data: 4379921920
I1108 14:34:20.905858 12355 layer_factory.hpp:77] Creating layer fc7
I1108 14:34:20.905866 12355 net.cpp:91] Creating Layer fc7
I1108 14:34:20.905870 12355 net.cpp:425] fc7 <- fc6
I1108 14:34:20.905887 12355 net.cpp:399] fc7 -> fc7
I1108 14:34:20.988060 12355 net.cpp:141] Setting up fc7
I1108 14:34:20.988102 12355 net.cpp:148] Top shape: 128 4096 (524288)
I1108 14:34:20.988107 12355 net.cpp:156] Memory required for data: 4382019072
I1108 14:34:20.988131 12355 layer_factory.hpp:77] Creating layer relu7
I1108 14:34:20.988147 12355 net.cpp:91] Creating Layer relu7
I1108 14:34:20.988155 12355 net.cpp:425] relu7 <- fc7
I1108 14:34:20.988162 12355 net.cpp:386] relu7 -> fc7 (in-place)
I1108 14:34:20.988173 12355 net.cpp:141] Setting up relu7
I1108 14:34:20.988178 12355 net.cpp:148] Top shape: 128 4096 (524288)
I1108 14:34:20.988183 12355 net.cpp:156] Memory required for data: 4384116224
I1108 14:34:20.988189 12355 layer_factory.hpp:77] Creating layer drop7
I1108 14:34:20.988196 12355 net.cpp:91] Creating Layer drop7
I1108 14:34:20.988201 12355 net.cpp:425] drop7 <- fc7
I1108 14:34:20.988209 12355 net.cpp:386] drop7 -> fc7 (in-place)
I1108 14:34:20.988225 12355 net.cpp:141] Setting up drop7
I1108 14:34:20.988243 12355 net.cpp:148] Top shape: 128 4096 (524288)
I1108 14:34:20.988247 12355 net.cpp:156] Memory required for data: 4386213376
I1108 14:34:20.988251 12355 layer_factory.hpp:77] Creating layer fc8
I1108 14:34:20.988258 12355 net.cpp:91] Creating Layer fc8
I1108 14:34:20.988262 12355 net.cpp:425] fc8 <- fc7
I1108 14:34:20.988268 12355 net.cpp:399] fc8 -> fc8
I1108 14:34:20.989393 12355 net.cpp:141] Setting up fc8
I1108 14:34:20.989418 12355 net.cpp:148] Top shape: 128 40 (5120)
I1108 14:34:20.989421 12355 net.cpp:156] Memory required for data: 4386233856
I1108 14:34:20.989428 12355 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I1108 14:34:20.989436 12355 net.cpp:91] Creating Layer fc8_fc8_0_split
I1108 14:34:20.989441 12355 net.cpp:425] fc8_fc8_0_split <- fc8
I1108 14:34:20.989446 12355 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1108 14:34:20.989454 12355 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1108 14:34:20.989475 12355 net.cpp:141] Setting up fc8_fc8_0_split
I1108 14:34:20.989482 12355 net.cpp:148] Top shape: 128 40 (5120)
I1108 14:34:20.989487 12355 net.cpp:148] Top shape: 128 40 (5120)
I1108 14:34:20.989491 12355 net.cpp:156] Memory required for data: 4386274816
I1108 14:34:20.989496 12355 layer_factory.hpp:77] Creating layer accuracy
I1108 14:34:20.989503 12355 net.cpp:91] Creating Layer accuracy
I1108 14:34:20.989507 12355 net.cpp:425] accuracy <- fc8_fc8_0_split_0
I1108 14:34:20.989513 12355 net.cpp:425] accuracy <- label_img_1_split_0
I1108 14:34:20.989519 12355 net.cpp:399] accuracy -> accuracy
I1108 14:34:20.989526 12355 net.cpp:141] Setting up accuracy
I1108 14:34:20.989532 12355 net.cpp:148] Top shape: (1)
I1108 14:34:20.989536 12355 net.cpp:156] Memory required for data: 4386274820
I1108 14:34:20.989542 12355 layer_factory.hpp:77] Creating layer loss
I1108 14:34:20.989553 12355 net.cpp:91] Creating Layer loss
I1108 14:34:20.989559 12355 net.cpp:425] loss <- fc8_fc8_0_split_1
I1108 14:34:20.989564 12355 net.cpp:425] loss <- label_img_1_split_1
I1108 14:34:20.989569 12355 net.cpp:399] loss -> loss
I1108 14:34:20.989578 12355 layer_factory.hpp:77] Creating layer loss
I1108 14:34:20.989645 12355 net.cpp:141] Setting up loss
I1108 14:34:20.989652 12355 net.cpp:148] Top shape: (1)
I1108 14:34:20.989656 12355 net.cpp:151]     with loss weight 1
I1108 14:34:20.989666 12355 net.cpp:156] Memory required for data: 4386274824
I1108 14:34:20.989671 12355 net.cpp:217] loss needs backward computation.
I1108 14:34:20.989677 12355 net.cpp:219] accuracy does not need backward computation.
I1108 14:34:20.989682 12355 net.cpp:217] fc8_fc8_0_split needs backward computation.
I1108 14:34:20.989686 12355 net.cpp:217] fc8 needs backward computation.
I1108 14:34:20.989691 12355 net.cpp:217] drop7 needs backward computation.
I1108 14:34:20.989696 12355 net.cpp:217] relu7 needs backward computation.
I1108 14:34:20.989701 12355 net.cpp:217] fc7 needs backward computation.
I1108 14:34:20.989704 12355 net.cpp:217] drop6 needs backward computation.
I1108 14:34:20.989709 12355 net.cpp:217] relu6 needs backward computation.
I1108 14:34:20.989713 12355 net.cpp:217] fc6 needs backward computation.
I1108 14:34:20.989718 12355 net.cpp:217] pool5 needs backward computation.
I1108 14:34:20.989723 12355 net.cpp:217] relu5 needs backward computation.
I1108 14:34:20.989727 12355 net.cpp:217] conv5 needs backward computation.
I1108 14:34:20.989732 12355 net.cpp:217] relu4 needs backward computation.
I1108 14:34:20.989737 12355 net.cpp:217] conv4 needs backward computation.
I1108 14:34:20.989742 12355 net.cpp:217] pool3 needs backward computation.
I1108 14:34:20.989748 12355 net.cpp:217] relu3 needs backward computation.
I1108 14:34:20.989753 12355 net.cpp:217] conv3 needs backward computation.
I1108 14:34:20.989758 12355 net.cpp:217] norm2 needs backward computation.
I1108 14:34:20.989761 12355 net.cpp:217] pool2 needs backward computation.
I1108 14:34:20.989766 12355 net.cpp:217] relu2 needs backward computation.
I1108 14:34:20.989770 12355 net.cpp:217] conv2 needs backward computation.
I1108 14:34:20.989775 12355 net.cpp:217] norm1 needs backward computation.
I1108 14:34:20.989780 12355 net.cpp:217] pool1 needs backward computation.
I1108 14:34:20.989784 12355 net.cpp:217] relu1 needs backward computation.
I1108 14:34:20.989789 12355 net.cpp:217] conv1 needs backward computation.
I1108 14:34:20.989794 12355 net.cpp:219] label_img_1_split does not need backward computation.
I1108 14:34:20.989799 12355 net.cpp:219] img does not need backward computation.
I1108 14:34:20.989804 12355 net.cpp:261] This network produces output accuracy
I1108 14:34:20.989807 12355 net.cpp:261] This network produces output loss
I1108 14:34:20.989820 12355 net.cpp:274] Network initialization done.
I1108 14:34:21.012770 12355 solver.cpp:181] Creating test net (#0) specified by test_net file: test.prototxt
I1108 14:34:21.013013 12355 net.cpp:49] Initializing net from parameters: 
state {
  phase: TEST
}
layer {
  name: "img"
  type: "Python"
  top: "img"
  top: "label"
  python_param {
    module: "data_layers.model_net_layer"
    layer: "ModelNetDataLayer"
    param_str: "{\'img_size\': (250, 250), \'split\': \'test\', \'dataset_dir\': \'/home/kevin/dataset/processed_data\', \'variable\': \'depth_map\', \'dtype\': \'object\', \'seed\': 1337, \'batch_size\': 128, \'mean\': 2}"
  }
}
layer {
  name: "conv1"
  type: "Convolution"
  bottom: "img"
  top: "conv1"
  param {
    name: "conv1_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv1_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 128
    pad: 2
    kernel_size: 5
    group: 1
    stride: 2
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "conv1"
  top: "conv1"
}
layer {
  name: "pool1"
  type: "Pooling"
  bottom: "conv1"
  top: "pool1"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm1"
  type: "LRN"
  bottom: "pool1"
  top: "norm1"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv2"
  type: "Convolution"
  bottom: "norm1"
  top: "conv2"
  param {
    name: "conv2_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv2_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 2
    kernel_size: 5
    group: 1
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "conv2"
  top: "conv2"
}
layer {
  name: "pool2"
  type: "Pooling"
  bottom: "conv2"
  top: "pool2"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "norm2"
  type: "LRN"
  bottom: "pool2"
  top: "norm2"
  lrn_param {
    local_size: 5
    alpha: 0.0005
    beta: 0.75
    k: 2
  }
}
layer {
  name: "conv3"
  type: "Convolution"
  bottom: "norm2"
  top: "conv3"
  param {
    name: "conv3_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv3_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 256
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu3"
  type: "ReLU"
  bottom: "conv3"
  top: "conv3"
}
layer {
  name: "pool3"
  type: "Pooling"
  bottom: "conv3"
  top: "pool3"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "conv4"
  type: "Convolution"
  bottom: "pool3"
  top: "conv4"
  param {
    name: "conv4_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv4_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu4"
  type: "ReLU"
  bottom: "conv4"
  top: "conv4"
}
layer {
  name: "conv5"
  type: "Convolution"
  bottom: "conv4"
  top: "conv5"
  param {
    name: "conv5_w"
    lr_mult: 1
    decay_mult: 1
  }
  param {
    name: "conv5_b"
    lr_mult: 2
    decay_mult: 0
  }
  convolution_param {
    num_output: 512
    pad: 1
    kernel_size: 3
    group: 2
    stride: 1
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu5"
  type: "ReLU"
  bottom: "conv5"
  top: "conv5"
}
layer {
  name: "pool5"
  type: "Pooling"
  bottom: "conv5"
  top: "pool5"
  pooling_param {
    pool: MAX
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "fc6"
  type: "InnerProduct"
  bottom: "pool5"
  top: "fc6"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu6"
  type: "ReLU"
  bottom: "fc6"
  top: "fc6"
}
layer {
  name: "drop6"
  type: "Dropout"
  bottom: "fc6"
  top: "fc6"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc7"
  type: "InnerProduct"
  bottom: "fc6"
  top: "fc7"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 4096
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "relu7"
  type: "ReLU"
  bottom: "fc7"
  top: "fc7"
}
layer {
  name: "drop7"
  type: "Dropout"
  bottom: "fc7"
  top: "fc7"
  dropout_param {
    dropout_ratio: 0.5
  }
}
layer {
  name: "fc8"
  type: "InnerProduct"
  bottom: "fc7"
  top: "fc8"
  param {
    lr_mult: 1
    decay_mult: 1
  }
  param {
    lr_mult: 2
    decay_mult: 0
  }
  inner_product_param {
    num_output: 40
    weight_filler {
      type: "xavier"
    }
  }
}
layer {
  name: "accuracy"
  type: "Accuracy"
  bottom: "fc8"
  bottom: "label"
  top: "accuracy"
}
layer {
  name: "loss"
  type: "SoftmaxWithLoss"
  bottom: "fc8"
  bottom: "label"
  top: "loss"
}
I1108 14:34:21.013758 12355 layer_factory.hpp:77] Creating layer img
I1108 14:34:21.013803 12355 net.cpp:91] Creating Layer img
I1108 14:34:21.013811 12355 net.cpp:399] img -> img
I1108 14:34:21.013820 12355 net.cpp:399] img -> label
{'img_size': (250, 250), 'split': 'test', 'dataset_dir': '/home/kevin/dataset/processed_data', 'variable': 'depth_map', 'dtype': 'object', 'seed': 1337, 'batch_size': 128, 'mean': 2}
I1108 14:34:21.154734 12355 net.cpp:141] Setting up img
I1108 14:34:21.154778 12355 net.cpp:148] Top shape: 24 1 250 250 (1500000)
I1108 14:34:21.154784 12355 net.cpp:148] Top shape: 24 1 (24)
I1108 14:34:21.154793 12355 net.cpp:156] Memory required for data: 6000096
I1108 14:34:21.154809 12355 layer_factory.hpp:77] Creating layer label_img_1_split
I1108 14:34:21.154821 12355 net.cpp:91] Creating Layer label_img_1_split
I1108 14:34:21.154834 12355 net.cpp:425] label_img_1_split <- label
I1108 14:34:21.154840 12355 net.cpp:399] label_img_1_split -> label_img_1_split_0
I1108 14:34:21.154850 12355 net.cpp:399] label_img_1_split -> label_img_1_split_1
I1108 14:34:21.154897 12355 net.cpp:141] Setting up label_img_1_split
I1108 14:34:21.154904 12355 net.cpp:148] Top shape: 24 1 (24)
I1108 14:34:21.154919 12355 net.cpp:148] Top shape: 24 1 (24)
I1108 14:34:21.154923 12355 net.cpp:156] Memory required for data: 6000288
I1108 14:34:21.154937 12355 layer_factory.hpp:77] Creating layer conv1
I1108 14:34:21.154949 12355 net.cpp:91] Creating Layer conv1
I1108 14:34:21.154954 12355 net.cpp:425] conv1 <- img
I1108 14:34:21.154960 12355 net.cpp:399] conv1 -> conv1
I1108 14:34:21.155122 12355 net.cpp:141] Setting up conv1
I1108 14:34:21.155129 12355 net.cpp:148] Top shape: 24 128 125 125 (48000000)
I1108 14:34:21.155143 12355 net.cpp:156] Memory required for data: 198000288
I1108 14:34:21.155153 12355 layer_factory.hpp:77] Creating layer relu1
I1108 14:34:21.155159 12355 net.cpp:91] Creating Layer relu1
I1108 14:34:21.155164 12355 net.cpp:425] relu1 <- conv1
I1108 14:34:21.155170 12355 net.cpp:386] relu1 -> conv1 (in-place)
I1108 14:34:21.155176 12355 net.cpp:141] Setting up relu1
I1108 14:34:21.155182 12355 net.cpp:148] Top shape: 24 128 125 125 (48000000)
I1108 14:34:21.155186 12355 net.cpp:156] Memory required for data: 390000288
I1108 14:34:21.155191 12355 layer_factory.hpp:77] Creating layer pool1
I1108 14:34:21.155199 12355 net.cpp:91] Creating Layer pool1
I1108 14:34:21.155203 12355 net.cpp:425] pool1 <- conv1
I1108 14:34:21.155208 12355 net.cpp:399] pool1 -> pool1
I1108 14:34:21.155233 12355 net.cpp:141] Setting up pool1
I1108 14:34:21.155239 12355 net.cpp:148] Top shape: 24 128 62 62 (11808768)
I1108 14:34:21.155244 12355 net.cpp:156] Memory required for data: 437235360
I1108 14:34:21.155249 12355 layer_factory.hpp:77] Creating layer norm1
I1108 14:34:21.155256 12355 net.cpp:91] Creating Layer norm1
I1108 14:34:21.155261 12355 net.cpp:425] norm1 <- pool1
I1108 14:34:21.155266 12355 net.cpp:399] norm1 -> norm1
I1108 14:34:21.155287 12355 net.cpp:141] Setting up norm1
I1108 14:34:21.155292 12355 net.cpp:148] Top shape: 24 128 62 62 (11808768)
I1108 14:34:21.155297 12355 net.cpp:156] Memory required for data: 484470432
I1108 14:34:21.155302 12355 layer_factory.hpp:77] Creating layer conv2
I1108 14:34:21.155309 12355 net.cpp:91] Creating Layer conv2
I1108 14:34:21.155313 12355 net.cpp:425] conv2 <- norm1
I1108 14:34:21.155319 12355 net.cpp:399] conv2 -> conv2
I1108 14:34:21.159350 12355 net.cpp:141] Setting up conv2
I1108 14:34:21.159369 12355 net.cpp:148] Top shape: 24 256 62 62 (23617536)
I1108 14:34:21.159374 12355 net.cpp:156] Memory required for data: 578940576
I1108 14:34:21.159384 12355 layer_factory.hpp:77] Creating layer relu2
I1108 14:34:21.159392 12355 net.cpp:91] Creating Layer relu2
I1108 14:34:21.159397 12355 net.cpp:425] relu2 <- conv2
I1108 14:34:21.159404 12355 net.cpp:386] relu2 -> conv2 (in-place)
I1108 14:34:21.159409 12355 net.cpp:141] Setting up relu2
I1108 14:34:21.159413 12355 net.cpp:148] Top shape: 24 256 62 62 (23617536)
I1108 14:34:21.159417 12355 net.cpp:156] Memory required for data: 673410720
I1108 14:34:21.159422 12355 layer_factory.hpp:77] Creating layer pool2
I1108 14:34:21.159430 12355 net.cpp:91] Creating Layer pool2
I1108 14:34:21.159435 12355 net.cpp:425] pool2 <- conv2
I1108 14:34:21.159440 12355 net.cpp:399] pool2 -> pool2
I1108 14:34:21.159466 12355 net.cpp:141] Setting up pool2
I1108 14:34:21.159472 12355 net.cpp:148] Top shape: 24 256 31 31 (5904384)
I1108 14:34:21.159476 12355 net.cpp:156] Memory required for data: 697028256
I1108 14:34:21.159487 12355 layer_factory.hpp:77] Creating layer norm2
I1108 14:34:21.159500 12355 net.cpp:91] Creating Layer norm2
I1108 14:34:21.159507 12355 net.cpp:425] norm2 <- pool2
I1108 14:34:21.159512 12355 net.cpp:399] norm2 -> norm2
I1108 14:34:21.159535 12355 net.cpp:141] Setting up norm2
I1108 14:34:21.159541 12355 net.cpp:148] Top shape: 24 256 31 31 (5904384)
I1108 14:34:21.159546 12355 net.cpp:156] Memory required for data: 720645792
I1108 14:34:21.159550 12355 layer_factory.hpp:77] Creating layer conv3
I1108 14:34:21.159559 12355 net.cpp:91] Creating Layer conv3
I1108 14:34:21.159564 12355 net.cpp:425] conv3 <- norm2
I1108 14:34:21.159569 12355 net.cpp:399] conv3 -> conv3
I1108 14:34:21.161156 12355 net.cpp:141] Setting up conv3
I1108 14:34:21.161170 12355 net.cpp:148] Top shape: 24 256 31 31 (5904384)
I1108 14:34:21.161175 12355 net.cpp:156] Memory required for data: 744263328
I1108 14:34:21.161181 12355 layer_factory.hpp:77] Creating layer relu3
I1108 14:34:21.161188 12355 net.cpp:91] Creating Layer relu3
I1108 14:34:21.161193 12355 net.cpp:425] relu3 <- conv3
I1108 14:34:21.161198 12355 net.cpp:386] relu3 -> conv3 (in-place)
I1108 14:34:21.161206 12355 net.cpp:141] Setting up relu3
I1108 14:34:21.161211 12355 net.cpp:148] Top shape: 24 256 31 31 (5904384)
I1108 14:34:21.161214 12355 net.cpp:156] Memory required for data: 767880864
I1108 14:34:21.161219 12355 layer_factory.hpp:77] Creating layer pool3
I1108 14:34:21.161226 12355 net.cpp:91] Creating Layer pool3
I1108 14:34:21.161231 12355 net.cpp:425] pool3 <- conv3
I1108 14:34:21.161236 12355 net.cpp:399] pool3 -> pool3
I1108 14:34:21.161259 12355 net.cpp:141] Setting up pool3
I1108 14:34:21.161265 12355 net.cpp:148] Top shape: 24 256 15 15 (1382400)
I1108 14:34:21.161269 12355 net.cpp:156] Memory required for data: 773410464
I1108 14:34:21.161274 12355 layer_factory.hpp:77] Creating layer conv4
I1108 14:34:21.161283 12355 net.cpp:91] Creating Layer conv4
I1108 14:34:21.161288 12355 net.cpp:425] conv4 <- pool3
I1108 14:34:21.161293 12355 net.cpp:399] conv4 -> conv4
I1108 14:34:21.164193 12355 net.cpp:141] Setting up conv4
I1108 14:34:21.164207 12355 net.cpp:148] Top shape: 24 512 15 15 (2764800)
I1108 14:34:21.164212 12355 net.cpp:156] Memory required for data: 784469664
I1108 14:34:21.164221 12355 layer_factory.hpp:77] Creating layer relu4
I1108 14:34:21.164227 12355 net.cpp:91] Creating Layer relu4
I1108 14:34:21.164232 12355 net.cpp:425] relu4 <- conv4
I1108 14:34:21.164238 12355 net.cpp:386] relu4 -> conv4 (in-place)
I1108 14:34:21.164245 12355 net.cpp:141] Setting up relu4
I1108 14:34:21.164250 12355 net.cpp:148] Top shape: 24 512 15 15 (2764800)
I1108 14:34:21.164255 12355 net.cpp:156] Memory required for data: 795528864
I1108 14:34:21.164259 12355 layer_factory.hpp:77] Creating layer conv5
I1108 14:34:21.164268 12355 net.cpp:91] Creating Layer conv5
I1108 14:34:21.164273 12355 net.cpp:425] conv5 <- conv4
I1108 14:34:21.164278 12355 net.cpp:399] conv5 -> conv5
I1108 14:34:21.170091 12355 net.cpp:141] Setting up conv5
I1108 14:34:21.170130 12355 net.cpp:148] Top shape: 24 512 15 15 (2764800)
I1108 14:34:21.170133 12355 net.cpp:156] Memory required for data: 806588064
I1108 14:34:21.170145 12355 layer_factory.hpp:77] Creating layer relu5
I1108 14:34:21.170168 12355 net.cpp:91] Creating Layer relu5
I1108 14:34:21.170173 12355 net.cpp:425] relu5 <- conv5
I1108 14:34:21.170181 12355 net.cpp:386] relu5 -> conv5 (in-place)
I1108 14:34:21.170188 12355 net.cpp:141] Setting up relu5
I1108 14:34:21.170192 12355 net.cpp:148] Top shape: 24 512 15 15 (2764800)
I1108 14:34:21.170197 12355 net.cpp:156] Memory required for data: 817647264
I1108 14:34:21.170203 12355 layer_factory.hpp:77] Creating layer pool5
I1108 14:34:21.170209 12355 net.cpp:91] Creating Layer pool5
I1108 14:34:21.170214 12355 net.cpp:425] pool5 <- conv5
I1108 14:34:21.170220 12355 net.cpp:399] pool5 -> pool5
I1108 14:34:21.170258 12355 net.cpp:141] Setting up pool5
I1108 14:34:21.170264 12355 net.cpp:148] Top shape: 24 512 7 7 (602112)
I1108 14:34:21.170279 12355 net.cpp:156] Memory required for data: 820055712
I1108 14:34:21.170284 12355 layer_factory.hpp:77] Creating layer fc6
I1108 14:34:21.170290 12355 net.cpp:91] Creating Layer fc6
I1108 14:34:21.170303 12355 net.cpp:425] fc6 <- pool5
I1108 14:34:21.170320 12355 net.cpp:399] fc6 -> fc6
I1108 14:34:21.669353 12355 net.cpp:141] Setting up fc6
I1108 14:34:21.669404 12355 net.cpp:148] Top shape: 24 4096 (98304)
I1108 14:34:21.669410 12355 net.cpp:156] Memory required for data: 820448928
I1108 14:34:21.669436 12355 layer_factory.hpp:77] Creating layer relu6
I1108 14:34:21.669450 12355 net.cpp:91] Creating Layer relu6
I1108 14:34:21.669461 12355 net.cpp:425] relu6 <- fc6
I1108 14:34:21.669471 12355 net.cpp:386] relu6 -> fc6 (in-place)
I1108 14:34:21.669483 12355 net.cpp:141] Setting up relu6
I1108 14:34:21.669492 12355 net.cpp:148] Top shape: 24 4096 (98304)
I1108 14:34:21.669497 12355 net.cpp:156] Memory required for data: 820842144
I1108 14:34:21.669502 12355 layer_factory.hpp:77] Creating layer drop6
I1108 14:34:21.669512 12355 net.cpp:91] Creating Layer drop6
I1108 14:34:21.669517 12355 net.cpp:425] drop6 <- fc6
I1108 14:34:21.669523 12355 net.cpp:386] drop6 -> fc6 (in-place)
I1108 14:34:21.669564 12355 net.cpp:141] Setting up drop6
I1108 14:34:21.669579 12355 net.cpp:148] Top shape: 24 4096 (98304)
I1108 14:34:21.669584 12355 net.cpp:156] Memory required for data: 821235360
I1108 14:34:21.669597 12355 layer_factory.hpp:77] Creating layer fc7
I1108 14:34:21.669615 12355 net.cpp:91] Creating Layer fc7
I1108 14:34:21.669620 12355 net.cpp:425] fc7 <- fc6
I1108 14:34:21.669627 12355 net.cpp:399] fc7 -> fc7
I1108 14:34:21.755007 12355 net.cpp:141] Setting up fc7
I1108 14:34:21.755079 12355 net.cpp:148] Top shape: 24 4096 (98304)
I1108 14:34:21.755089 12355 net.cpp:156] Memory required for data: 821628576
I1108 14:34:21.755118 12355 layer_factory.hpp:77] Creating layer relu7
I1108 14:34:21.755137 12355 net.cpp:91] Creating Layer relu7
I1108 14:34:21.755149 12355 net.cpp:425] relu7 <- fc7
I1108 14:34:21.755159 12355 net.cpp:386] relu7 -> fc7 (in-place)
I1108 14:34:21.755174 12355 net.cpp:141] Setting up relu7
I1108 14:34:21.755184 12355 net.cpp:148] Top shape: 24 4096 (98304)
I1108 14:34:21.755192 12355 net.cpp:156] Memory required for data: 822021792
I1108 14:34:21.755198 12355 layer_factory.hpp:77] Creating layer drop7
I1108 14:34:21.755208 12355 net.cpp:91] Creating Layer drop7
I1108 14:34:21.755215 12355 net.cpp:425] drop7 <- fc7
I1108 14:34:21.755224 12355 net.cpp:386] drop7 -> fc7 (in-place)
I1108 14:34:21.755267 12355 net.cpp:141] Setting up drop7
I1108 14:34:21.755286 12355 net.cpp:148] Top shape: 24 4096 (98304)
I1108 14:34:21.755290 12355 net.cpp:156] Memory required for data: 822415008
I1108 14:34:21.755303 12355 layer_factory.hpp:77] Creating layer fc8
I1108 14:34:21.755324 12355 net.cpp:91] Creating Layer fc8
I1108 14:34:21.755329 12355 net.cpp:425] fc8 <- fc7
I1108 14:34:21.755336 12355 net.cpp:399] fc8 -> fc8
I1108 14:34:21.756770 12355 net.cpp:141] Setting up fc8
I1108 14:34:21.756830 12355 net.cpp:148] Top shape: 24 40 (960)
I1108 14:34:21.756840 12355 net.cpp:156] Memory required for data: 822418848
I1108 14:34:21.756861 12355 layer_factory.hpp:77] Creating layer fc8_fc8_0_split
I1108 14:34:21.756880 12355 net.cpp:91] Creating Layer fc8_fc8_0_split
I1108 14:34:21.756891 12355 net.cpp:425] fc8_fc8_0_split <- fc8
I1108 14:34:21.756902 12355 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_0
I1108 14:34:21.756916 12355 net.cpp:399] fc8_fc8_0_split -> fc8_fc8_0_split_1
I1108 14:34:21.756971 12355 net.cpp:141] Setting up fc8_fc8_0_split
I1108 14:34:21.756983 12355 net.cpp:148] Top shape: 24 40 (960)
I1108 14:34:21.756999 12355 net.cpp:148] Top shape: 24 40 (960)
I1108 14:34:21.757004 12355 net.cpp:156] Memory required for data: 822426528
I1108 14:34:21.757019 12355 layer_factory.hpp:77] Creating layer accuracy
I1108 14:34:21.757027 12355 net.cpp:91] Creating Layer accuracy
I1108 14:34:21.757032 12355 net.cpp:425] accuracy <- fc8_fc8_0_split_0
I1108 14:34:21.757040 12355 net.cpp:425] accuracy <- label_img_1_split_0
I1108 14:34:21.757046 12355 net.cpp:399] accuracy -> accuracy
I1108 14:34:21.757056 12355 net.cpp:141] Setting up accuracy
I1108 14:34:21.757062 12355 net.cpp:148] Top shape: (1)
I1108 14:34:21.757068 12355 net.cpp:156] Memory required for data: 822426532
I1108 14:34:21.757073 12355 layer_factory.hpp:77] Creating layer loss
I1108 14:34:21.757081 12355 net.cpp:91] Creating Layer loss
I1108 14:34:21.757087 12355 net.cpp:425] loss <- fc8_fc8_0_split_1
I1108 14:34:21.757093 12355 net.cpp:425] loss <- label_img_1_split_1
I1108 14:34:21.757100 12355 net.cpp:399] loss -> loss
I1108 14:34:21.757109 12355 layer_factory.hpp:77] Creating layer loss
I1108 14:34:21.757212 12355 net.cpp:141] Setting up loss
I1108 14:34:21.757221 12355 net.cpp:148] Top shape: (1)
I1108 14:34:21.757226 12355 net.cpp:151]     with loss weight 1
I1108 14:34:21.757241 12355 net.cpp:156] Memory required for data: 822426536
I1108 14:34:21.757246 12355 net.cpp:217] loss needs backward computation.
I1108 14:34:21.757252 12355 net.cpp:219] accuracy does not need backward computation.
I1108 14:34:21.757258 12355 net.cpp:217] fc8_fc8_0_split needs backward computation.
I1108 14:34:21.757263 12355 net.cpp:217] fc8 needs backward computation.
I1108 14:34:21.757268 12355 net.cpp:217] drop7 needs backward computation.
I1108 14:34:21.757273 12355 net.cpp:217] relu7 needs backward computation.
I1108 14:34:21.757278 12355 net.cpp:217] fc7 needs backward computation.
I1108 14:34:21.757283 12355 net.cpp:217] drop6 needs backward computation.
I1108 14:34:21.757287 12355 net.cpp:217] relu6 needs backward computation.
I1108 14:34:21.757292 12355 net.cpp:217] fc6 needs backward computation.
I1108 14:34:21.757297 12355 net.cpp:217] pool5 needs backward computation.
I1108 14:34:21.757302 12355 net.cpp:217] relu5 needs backward computation.
I1108 14:34:21.757308 12355 net.cpp:217] conv5 needs backward computation.
I1108 14:34:21.757313 12355 net.cpp:217] relu4 needs backward computation.
I1108 14:34:21.757318 12355 net.cpp:217] conv4 needs backward computation.
I1108 14:34:21.757323 12355 net.cpp:217] pool3 needs backward computation.
I1108 14:34:21.757328 12355 net.cpp:217] relu3 needs backward computation.
I1108 14:34:21.757333 12355 net.cpp:217] conv3 needs backward computation.
I1108 14:34:21.757338 12355 net.cpp:217] norm2 needs backward computation.
I1108 14:34:21.757344 12355 net.cpp:217] pool2 needs backward computation.
I1108 14:34:21.757349 12355 net.cpp:217] relu2 needs backward computation.
I1108 14:34:21.757352 12355 net.cpp:217] conv2 needs backward computation.
I1108 14:34:21.757359 12355 net.cpp:217] norm1 needs backward computation.
I1108 14:34:21.757362 12355 net.cpp:217] pool1 needs backward computation.
I1108 14:34:21.757369 12355 net.cpp:217] relu1 needs backward computation.
I1108 14:34:21.757374 12355 net.cpp:217] conv1 needs backward computation.
I1108 14:34:21.757380 12355 net.cpp:219] label_img_1_split does not need backward computation.
I1108 14:34:21.757385 12355 net.cpp:219] img does not need backward computation.
I1108 14:34:21.757390 12355 net.cpp:261] This network produces output accuracy
I1108 14:34:21.757395 12355 net.cpp:261] This network produces output loss
I1108 14:34:21.757411 12355 net.cpp:274] Network initialization done.
I1108 14:34:21.757534 12355 solver.cpp:60] Solver scaffolding done.
I1108 14:34:27.078526 12355 solver.cpp:337] Iteration 0, Testing net (#0)
I1108 14:34:28.977944 12355 solver.cpp:228] Iteration 0, loss = 0.982136
I1108 14:34:28.977985 12355 solver.cpp:244]     Train net output #0: accuracy = 0.710938
I1108 14:34:28.978013 12355 solver.cpp:244]     Train net output #1: loss = 0.982136 (* 1 = 0.982136 loss)
I1108 14:34:28.978035 12355 sgd_solver.cpp:106] Iteration 0, lr = 0.01
I1108 14:40:48.099650 12355 solver.cpp:228] Iteration 100, loss = 0.891388
I1108 14:40:48.099694 12355 solver.cpp:244]     Train net output #0: accuracy = 0.78125
I1108 14:40:48.099704 12355 solver.cpp:244]     Train net output #1: loss = 0.891388 (* 1 = 0.891388 loss)
I1108 14:40:48.099723 12355 sgd_solver.cpp:106] Iteration 100, lr = 0.01
I1108 14:46:43.511875 12355 solver.cpp:228] Iteration 200, loss = 0.828166
I1108 14:46:43.511958 12355 solver.cpp:244]     Train net output #0: accuracy = 0.78125
I1108 14:46:43.511984 12355 solver.cpp:244]     Train net output #1: loss = 0.828166 (* 1 = 0.828166 loss)
I1108 14:46:43.512002 12355 sgd_solver.cpp:106] Iteration 200, lr = 0.01
I1108 14:52:50.777876 12355 solver.cpp:228] Iteration 300, loss = 0.779092
I1108 14:52:50.777921 12355 solver.cpp:244]     Train net output #0: accuracy = 0.773438
I1108 14:52:50.777932 12355 solver.cpp:244]     Train net output #1: loss = 0.779092 (* 1 = 0.779092 loss)
I1108 14:52:50.777951 12355 sgd_solver.cpp:106] Iteration 300, lr = 0.01
I1108 15:00:17.865485 12355 solver.cpp:228] Iteration 400, loss = 0.810223
I1108 15:00:17.865531 12355 solver.cpp:244]     Train net output #0: accuracy = 0.78125
I1108 15:00:17.865541 12355 solver.cpp:244]     Train net output #1: loss = 0.810223 (* 1 = 0.810223 loss)
I1108 15:00:17.865552 12355 sgd_solver.cpp:106] Iteration 400, lr = 0.01
I1108 15:06:14.827924 12355 solver.cpp:228] Iteration 500, loss = 0.799903
I1108 15:06:14.827982 12355 solver.cpp:244]     Train net output #0: accuracy = 0.75
I1108 15:06:14.828014 12355 solver.cpp:244]     Train net output #1: loss = 0.799903 (* 1 = 0.799903 loss)
I1108 15:06:14.828030 12355 sgd_solver.cpp:106] Iteration 500, lr = 0.01
I1108 15:12:41.794953 12355 solver.cpp:228] Iteration 600, loss = 0.735337
I1108 15:12:41.794987 12355 solver.cpp:244]     Train net output #0: accuracy = 0.765625
I1108 15:12:41.794997 12355 solver.cpp:244]     Train net output #1: loss = 0.735337 (* 1 = 0.735337 loss)
I1108 15:12:41.795016 12355 sgd_solver.cpp:106] Iteration 600, lr = 0.01
I1108 15:19:06.899296 12355 solver.cpp:228] Iteration 700, loss = 0.591273
I1108 15:19:06.899343 12355 solver.cpp:244]     Train net output #0: accuracy = 0.796875
I1108 15:19:06.899353 12355 solver.cpp:244]     Train net output #1: loss = 0.591273 (* 1 = 0.591273 loss)
I1108 15:19:06.899371 12355 sgd_solver.cpp:106] Iteration 700, lr = 0.01
I1108 15:25:02.143223 12355 solver.cpp:228] Iteration 800, loss = 0.785873
I1108 15:25:02.143270 12355 solver.cpp:244]     Train net output #0: accuracy = 0.765625
I1108 15:25:02.143281 12355 solver.cpp:244]     Train net output #1: loss = 0.785873 (* 1 = 0.785873 loss)
I1108 15:25:02.143299 12355 sgd_solver.cpp:106] Iteration 800, lr = 0.01
I1108 15:30:35.145170 12355 solver.cpp:228] Iteration 900, loss = 0.766038
I1108 15:30:35.145212 12355 solver.cpp:244]     Train net output #0: accuracy = 0.757812
I1108 15:30:35.145222 12355 solver.cpp:244]     Train net output #1: loss = 0.766038 (* 1 